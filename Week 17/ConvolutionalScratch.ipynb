{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Introduction\r\n",
                "Let's create the class Convolutional Neural Network (CNN) from scratch. We will implement the algorithm using only the minimum library such as NumPy.\r\n",
                "\r\n",
                "\r\n",
                "In this Sprint, we will build a 1D Convolutional layer and try to understand the basics of convolution. The next Sprint completes the CNN commonly used for images by creating a two-dimensional convolutional layer and a pooling layer.\r\n",
                "\r\n",
                "\r\n",
                "Name the class Scratch1dCNNClassifier. Please refer to the ScratchDeepNeuralNetrowkClassifier created in the previous Sprint for the class structure.\r\n",
                "\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 157,
            "source": [
                "#needed\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import seaborn as sns\r\n",
                "import matplotlib.pyplot as plt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Needed Old Code"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 158,
            "source": [
                "#intializer\r\n",
                "class SimpleInitializer:\r\n",
                "  def __init__(self, sigma = 0.1):\r\n",
                "    self.sigma = sigma\r\n",
                "  def W(self, n_nodes1, n_nodes2):\r\n",
                "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\r\n",
                "    return W\r\n",
                "  def B(self, n_nodes2):\r\n",
                "\r\n",
                "    B = self.sigma * np.random.randn(1,n_nodes2)\r\n",
                "    return B\r\n",
                "class SGD:\r\n",
                "    \"\"\"\r\n",
                "    Stochastic gradient descent\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    lr : Learning rate\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, lr=0.01):\r\n",
                "        self.lr = lr\r\n",
                "    def update(self, layer):\r\n",
                "        \"\"\"\r\n",
                "        Update weights and biases for a layer\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        layer : Instance of the layer before update\r\n",
                "        \"\"\"\r\n",
                "        #update\r\n",
                "        layer.B += - self.lr * layer.dB\r\n",
                "        layer.W += - self.lr * layer.dW\r\n",
                "        return layer\r\n",
                "#! mini batch...........................\r\n",
                "class GetMiniBatch:\r\n",
                "    \"\"\"\r\n",
                "Iterator to get a mini-batch\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    X : The following forms of ndarray, shape (n_samples, n_features)\r\n",
                "      Training data\r\n",
                "    y : The following form of ndarray, shape (n_samples, 1)\r\n",
                "      Correct answer value\r\n",
                "    batch_size : int\r\n",
                "      Batch size\r\n",
                "    seed : int\r\n",
                "      NumPy random number seed\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, X, y = None, batch_size = 20, seed=0):\r\n",
                "        self.batch_size = batch_size\r\n",
                "        np.random.seed(seed)\r\n",
                "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
                "        self._X = X[shuffle_index]\r\n",
                "        self._y = y[shuffle_index] if y is not None else None\r\n",
                "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
                "    def __len__(self):\r\n",
                "        return self._stop\r\n",
                "    def __getitem__(self,item):\r\n",
                "        p0 = item*self.batch_size\r\n",
                "        p1 = item*self.batch_size + self.batch_size\r\n",
                "        if self._y is not None:\r\n",
                "          return self._X[p0:p1], self._y[p0:p1] \r\n",
                "        else:\r\n",
                "          return self._X[p0:p1]       \r\n",
                "    def __iter__(self):\r\n",
                "        self._counter = 0\r\n",
                "        return self\r\n",
                "    def __next__(self):\r\n",
                "        if self._counter >= self._stop:\r\n",
                "            raise StopIteration()\r\n",
                "        p0 = self._counter*self.batch_size\r\n",
                "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
                "        self._counter += 1\r\n",
                "        if self._y is not None:\r\n",
                "          return self._X[p0:p1], self._y[p0:p1] \r\n",
                "        else:\r\n",
                "          return self._X[p0:p1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 1\r\n",
                "1 Dim convolutional Layer that limit number of channels to 1"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Forward"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "source": [
                "class SimpleConv1D:\r\n",
                "  def __init__(self, filter_size, initializer, optimizer):\r\n",
                "    # Initialize\r\n",
                "    self.optimizer = optimizer\r\n",
                "    self.filter_size = filter_size\r\n",
                "    # Initialize self.W and self.B using the initializer method\r\n",
                "    self.W = initializer.W(filter_size, 1)\r\n",
                "    self.B = initializer.B(1)\r\n",
                "  def forward(self, X):\r\n",
                "    filter_size = self.filter_size\r\n",
                "    self.X = X\r\n",
                "    return self.convolve(self.W,X, bias = self.B)\r\n",
                "\r\n",
                "  def backward(self, dA):\r\n",
                "    # update\r\n",
                "    self.dA = dA\r\n",
                "    self.dW = self.calc_dW()\r\n",
                "    self.dB = self.calc_dB()\r\n",
                "    self.dX = self.calc_dX()\r\n",
                "    self = self.optimizer.update(self)\r\n",
                "    return self.dW, self.dB, self.dX\r\n",
                "  def calc_dW(self):\r\n",
                "    return self.convolve(self.dA, self.X)\r\n",
                "  def calc_dB(self):\r\n",
                "    return np.array(self.dA.sum()).ravel()\r\n",
                "  def calc_dX(self):\r\n",
                "    padA = self.padA()\r\n",
                "    return self.convolve(np.flip(self.W),padA)\r\n",
                "  def padA(self):\r\n",
                "    padding = (len(self.X) - 1 + len(self.W) - len(self.dA)) //2 # cause out = (in - fil + 2pad) + 1\r\n",
                "    print('Padding size: ', padding)\r\n",
                "    return np.pad(self.dA,(padding, padding), mode = 'constant')\r\n",
                "  def convolve(self,F,X,bias = [0]):\r\n",
                "    A = []\r\n",
                "    filter_size = len(F)\r\n",
                "    for i in range(len(X) - filter_size + 1):\r\n",
                "      A.append(X[i : i + filter_size] @ F + bias[0])\r\n",
                "    return np.array(A)\r\n",
                "\r\n",
                "    \r\n",
                "  "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 2\r\n",
                "Output size calculation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 160,
            "source": [
                "def conv_n_out(n_in,filter_size, padding=0, stride=1):\r\n",
                "    return (n_in + 2*padding - filter_size) //stride + 1\r\n",
                "print('Convolutional n-out of input 30 and filter 3:', conv_n_out(30,3))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Convolutional n-out of input 30 and filter 3: 28\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 3\r\n",
                "Experiment of one-dimensional convolutional layer with small array"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 161,
            "source": [
                "x = np.array([1,2,3,4])\r\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\r\n",
                "b = np.array([1], dtype = np.float64)\r\n",
                "layer = SimpleConv1D(3,SimpleInitializer(),SGD())\r\n",
                "layer.W = w\r\n",
                "layer.B = b\r\n",
                "\r\n",
                "print('n_output: ', conv_n_out(len(x),len(w)))\r\n",
                "\r\n",
                "correct_forward_result = np.array([35, 50])\r\n",
                "a = layer.forward(x)\r\n",
                "print('forward: ', layer.forward(x), 'correct result: ', correct_forward_result)\r\n",
                "\r\n",
                "\r\n",
                "delta_a = np.array([10, 20])\r\n",
                "dW,dB,dX = layer.backward(delta_a)\r\n",
                "\r\n",
                "\r\n",
                "correct_delta_b = np.array([30])\r\n",
                "correct_delta_x = np.array([30, 110, 170, 140])\r\n",
                "correct_delta_w = np.array([50, 80, 110])\r\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\r\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\r\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "n_output:  2\n",
                        "forward:  [35. 50.] correct result:  [35 50]\n",
                        "Padding size:  2\n",
                        "dB:  [30] correct result:  [30]\n",
                        "dW:  [ 50  80 110] correct result:  [ 50  80 110]\n",
                        "dX:  [ 30. 110. 170. 140.] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 4\r\n",
                "Creating a one-dimensional convolutional layer class that does not limit the number of channels"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Updated Prototype (NO STRIDE, NO BATCH)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 162,
            "source": [
                "class Conv1D:\r\n",
                "  def __init__(self, filter_size = 3, n_input = 2, n_output = 3, optimizer = SGD(), padding = 0):\r\n",
                "    # Initialize\r\n",
                "    self.optimizer = optimizer\r\n",
                "    self.filter_size = filter_size\r\n",
                "    # Initialize self.W and self.B using the initializer method\r\n",
                "    self.filter_size = filter_size\r\n",
                "    self.n_input = n_input\r\n",
                "    self.n_output = n_output\r\n",
                "\r\n",
                "    self.W = np.ones((n_output, n_input,filter_size))\r\n",
                "    self.B = np.ones(n_output)\r\n",
                "    #padding and stride\r\n",
                "    self.padding = padding\r\n",
                "  def forward(self, X): \r\n",
                "    X = np.pad(X,[(0,0),(self.padding, self.padding)], mode = 'constant')\r\n",
                "    self.X = X\r\n",
                "    output = []\r\n",
                "    for i in range(self.n_output):\r\n",
                "      filt = self.W[i]\r\n",
                "      bias = self.B[i]\r\n",
                "      conv = self.convolve(filt, X)\r\n",
                "      output.append(conv.sum(axis = 0) + bias)\r\n",
                "    return np.array(output)\r\n",
                "  def backward(self, dA):\r\n",
                "    # update\r\n",
                "    self.dA = dA\r\n",
                "    self.dW = self.calc_dW()\r\n",
                "    self.dB = self.calc_dB()\r\n",
                "    self.dX = self.calc_dX()\r\n",
                "    self = self.optimizer.update(self)\r\n",
                "    # print(f'After backward: x {self.X.shape}, w: {self.W.shape}, b: {self.B.shape}, dX: {self.dX.shape}, dW: {self.dW.shape}, dB: {self.dB.shape}, dA: {self.dA.shape}')\r\n",
                "    # print(f'filterz: {self.filter_size}')\r\n",
                "    return self.dW, self.dB, self.dX\r\n",
                "  def calc_dW(self):\r\n",
                "    dW = []\r\n",
                "    dupped_dA = np.repeat(self.dA[:,np.newaxis, : ], self.n_input, axis=1)\r\n",
                "    for i in range(self.n_output): #convolve each output_channel through X\r\n",
                "      conv = self.convolve(dupped_dA[i], self.X)\r\n",
                "      dW.append(conv) \r\n",
                "    return np.array(dW)\r\n",
                "  def calc_dB(self):\r\n",
                "    return np.array(self.dA.sum(axis = 1))\r\n",
                "  def calc_dX(self):\r\n",
                "    pad_dA = self.pad_dA() #match a with x\r\n",
                "    flipped_W = np.flip(self.W,axis = 2).reshape(self.n_input,self.n_output, -1) # flip each filter and the in_out dim also\r\n",
                "    output = []\r\n",
                "    for i in range(self.n_input):\r\n",
                "      filt = flipped_W[i]\r\n",
                "      conv = self.convolve(filt, pad_dA)\r\n",
                "      output.append(conv.sum(axis = 0))\r\n",
                "    return np.array(output)\r\n",
                "  def pad_dA(self):\r\n",
                "    n_features_in = self.dA.shape[1]\r\n",
                "    n_features_out = self.X.shape[1]\r\n",
                "    filter_size = self.filter_size\r\n",
                "    padding = (n_features_out - 1 + filter_size - n_features_in) // 2\r\n",
                "    return self.pad(self.dA, padding)\r\n",
                "  def pad(self,array,padding):\r\n",
                "    return np.pad(array,[(0,0),(padding, padding)], mode = 'constant')\r\n",
                "  def convolve(self,F,X):\r\n",
                "    A = []\r\n",
                "    filter_size = F.shape[1]\r\n",
                "    feature_count = X.shape[1]\r\n",
                "    n_out_features = (feature_count - filter_size) + 1\r\n",
                "    for i in range(n_out_features):\r\n",
                "      A.append((X[:,i : i + filter_size] * F).sum(axis=1))\r\n",
                "    return np.array(A).T"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Forward"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 163,
            "source": [
                "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) #shape (2, 4), (number of input channels, number of features).\r\n",
                "# x = np.array([[1, 2, 3], [3, 4, 5]]) #shape (2, 4), (number of input channels, number of features).\r\n",
                "w = np.ones((3, 2, 3)) # Set to 1 for simplification of the example. (Number of output channels, number of input channels, filter size).\r\n",
                "b = np.array([1, 2, 3]) # (Number of output channels)\r\n",
                "\r\n",
                "conv = Conv1D(3,2,3)\r\n",
                "conv.B = b\r\n",
                "print(conv.forward(x))\r\n",
                "print('correct: result: ',np.array([[16, 22], [17, 23], [18, 24]]))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[16. 22.]\n",
                        " [17. 23.]\n",
                        " [18. 24.]]\n",
                        "correct: result:  [[16 22]\n",
                        " [17 23]\n",
                        " [18 24]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Let's user problem 3 as a reference to make the multichannel propagation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 164,
            "source": [
                "x = np.array([1,2,3,4])\r\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\r\n",
                "filter_size, n_input,n_output = 3,2,3\r\n",
                "# print('f,nin,nout: ', filter_size, n_input,n_output)\r\n",
                "layer = Conv1D()\r\n",
                "x = np.vstack([x] * n_input)\r\n",
                "w = np.vstack([[np.vstack([w]*n_input)]] * n_output)\r\n",
                "b = np.ones(n_output)\r\n",
                "layer.W = w\r\n",
                "layer.B = b\r\n",
                "\r\n",
                "print('X: ', x)\r\n",
                "print('w: ', w)\r\n",
                "print('b: ', b)\r\n",
                "\r\n",
                "correct_forward_result = np.array([35, 50])\r\n",
                "a = layer.forward(x)\r\n",
                "print('forward: ', layer.forward(x), 'correct result: ', correct_forward_result)\r\n",
                "\r\n",
                "\r\n",
                "delta_a = np.vstack([[10, 20]]* n_output)\r\n",
                "layer.dA = delta_a\r\n",
                "dW = layer.calc_dW()\r\n",
                "dB = layer.calc_dB()\r\n",
                "dX = layer.calc_dX()\r\n",
                "\r\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\r\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\r\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "X:  [[1 2 3 4]\n",
                        " [1 2 3 4]]\n",
                        "w:  [[[3. 5. 7.]\n",
                        "  [3. 5. 7.]]\n",
                        "\n",
                        " [[3. 5. 7.]\n",
                        "  [3. 5. 7.]]\n",
                        "\n",
                        " [[3. 5. 7.]\n",
                        "  [3. 5. 7.]]]\n",
                        "b:  [1. 1. 1.]\n",
                        "forward:  [[69. 99.]\n",
                        " [69. 99.]\n",
                        " [69. 99.]] correct result:  [35 50]\n",
                        "dB:  [30 30 30] correct result:  [30]\n",
                        "dW:  [[[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]] correct result:  [ 50  80 110]\n",
                        "dX:  [[ 90. 330. 510. 420.]\n",
                        " [ 90. 330. 510. 420.]] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 5\r\n",
                "Implement Padding"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Note: padding added in prototype"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 165,
            "source": [
                "#padding example\r\n",
                "def add_padding(X, padding_size):\r\n",
                "    return np.pad(X, ((0,0),(padding_size,padding_size)), mode = 'constant', constant_values = 0)\r\n",
                "\r\n",
                "#we are conv 1d so just pad horizontally\r\n",
                "print(f'x: {x}')\r\n",
                "print(f'padded:\\n {add_padding(x, 3)}')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "x: [[1 2 3 4]\n",
                        " [1 2 3 4]]\n",
                        "padded:\n",
                        " [[0 0 0 1 2 3 4 0 0 0]\n",
                        " [0 0 0 1 2 3 4 0 0 0]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 166,
            "source": [
                "# test run on prototype just to see if it works\r\n",
                "padded_layer = Conv1D(padding = 2)\r\n",
                "a = padded_layer.forward(x)\r\n",
                "dW,dB,dX = padded_layer.backward(a)\r\n",
                "print('shapes: ', dW.shape,dB.shape,dX.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "shapes:  (3, 2, 3) (3,) (2, 8)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 6\r\n",
                "Response to mini batch"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### NOTE\r\n",
                "Currently, our layer only works with one sample with many layers\r\n",
                "\r\n",
                "**Let's add many samples**\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Batch Prototype"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 167,
            "source": [
                "class ConvInitializer:\r\n",
                "  def __init__(self, sigma = 0.1):\r\n",
                "    self.sigma = sigma\r\n",
                "  def W(self, n_output, n_input, filter_size):\r\n",
                "    W = self.sigma * np.random.randn(n_output, n_input, filter_size)\r\n",
                "    return W\r\n",
                "  def B(self, n_output):\r\n",
                "\r\n",
                "    B = self.sigma * np.random.randn(n_output)\r\n",
                "    return B"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 168,
            "source": [
                "class Conv1DBatch:\r\n",
                "  def __init__(self, filter_size = 3, n_input = 2, n_output = 3, optimizer = SGD(), initializer = ConvInitializer(), padding = 0):\r\n",
                "    # Initialize\r\n",
                "    self.optimizer = optimizer\r\n",
                "    self.filter_size = filter_size\r\n",
                "    # Initialize self.W and self.B using the initializer method\r\n",
                "    self.filter_size = filter_size\r\n",
                "    self.n_input = n_input\r\n",
                "    self.n_output = n_output\r\n",
                "\r\n",
                "    if initializer == None:\r\n",
                "      self.W = np.ones((n_output, n_input,filter_size))\r\n",
                "      self.B = np.ones(n_output)\r\n",
                "    else:\r\n",
                "      self.W = initializer.W(n_output, n_input, filter_size)\r\n",
                "      self.B = initializer.B(n_output)\r\n",
                "    #padding and stride\r\n",
                "    self.padding = padding\r\n",
                "        \r\n",
                "  def forward(self,X): #! NOTE: X must be 3 dimensional (batch_size, channel_count, feature_count)\r\n",
                "    self.batch_size = len(X)\r\n",
                "    X = np.pad(X,[(0,0),(0,0),(self.padding, self.padding)], mode = 'constant')\r\n",
                "    self.X = X\r\n",
                "    result = []\r\n",
                "    for x in self.X:\r\n",
                "      result.append(self._forward_sample(x))\r\n",
                "    # print('forward: ', np.array(result).shape)\r\n",
                "    return np.array(result)\r\n",
                "  def _forward_sample(self,X):    \r\n",
                "    output = []\r\n",
                "    for i in range(self.n_output):\r\n",
                "      filt = self.W[i]\r\n",
                "      bias = self.B[i]\r\n",
                "      conv = self.convolve(filt, X)\r\n",
                "      output.append(conv.sum(axis = 0) + bias)\r\n",
                "    return np.array(output)\r\n",
                "\r\n",
                "  def backward(self, dA):\r\n",
                "    # update\r\n",
                "    self.dA = dA\r\n",
                "    self.dX = []\r\n",
                "\r\n",
                "    for i in range(self.batch_size):\r\n",
                "      self.dW = self.calc_dW(i)\r\n",
                "      self.dB = self.calc_dB(i)\r\n",
                "      self.dx = self.calc_dx(i)\r\n",
                "      self = self.optimizer.update(self)\r\n",
                "      self.dX.append(self.dx) #! keep updating while calculating error for prev layer\r\n",
                "      # print('dx: ', i, self.dx.shape,self.dx)\r\n",
                "    self.dX = np.array(self.dX)\r\n",
                "    return self.dW, self.dB , self.dX\r\n",
                "    \r\n",
                "\r\n",
                "  def calc_dW(self,sample_index):\r\n",
                "    dW = []\r\n",
                "    dupped_dA = np.repeat(self.dA[sample_index][:,np.newaxis, : ], self.n_input, axis=1)\r\n",
                "    for i in range(self.n_output): #convolve each output_channel through X\r\n",
                "      conv = self.convolve(dupped_dA[i], self.X[sample_index])\r\n",
                "      dW.append(conv) \r\n",
                "    return np.array(dW)\r\n",
                "  def calc_dB(self,sample_index):\r\n",
                "    return np.array(self.dA[sample_index].sum(axis = 1))\r\n",
                "\r\n",
                "  def calc_dx(self, sample_index): #! careful\r\n",
                "    pad_dA = self.pad_dA(sample_index) #match a with x\r\n",
                "    flipped_W = np.flip(self.W,axis = 2).reshape(self.n_input,self.n_output, -1) # flip each filter and the in_out dim also\r\n",
                "    output = []\r\n",
                "    for i in range(self.n_input):\r\n",
                "      filt = flipped_W[i]\r\n",
                "      conv = self.convolve(filt, pad_dA)\r\n",
                "      output.append(conv.sum(axis = 0))\r\n",
                "    return np.array(output)\r\n",
                "  def pad_dA(self,sample_index):\r\n",
                "    array = self.dA[sample_index]\r\n",
                "    n_features_in = array.shape[1]\r\n",
                "    n_features_out = self.X.shape[-1]\r\n",
                "    filter_size = self.filter_size\r\n",
                "    padding = (n_features_out - 1 + filter_size - n_features_in) // 2\r\n",
                "    return self.pad(array, padding)\r\n",
                "  def pad(self,array,padding):\r\n",
                "    return np.pad(array,[(0,0),(padding, padding)], mode = 'constant')\r\n",
                "  def convolve(self,F,X):\r\n",
                "    A = []\r\n",
                "    filter_size = F.shape[-1]\r\n",
                "    feature_count = X.shape[-1]\r\n",
                "    n_out_features = (feature_count - filter_size) + 1\r\n",
                "    for i in range(n_out_features):\r\n",
                "      A.append((X[...,i : i + filter_size] * F).sum(axis=-1))\r\n",
                "    return np.array(A).T\r\n",
                "\r\n",
                "# layer = Conv1DBatch(padding = 2)\r\n",
                "# X = np.array([[[1,2,3],[4,5,6]], [[2,3,4],[5,6,7]]])\r\n",
                "# layer.B = np.array([1,2,3])\r\n",
                "# fr = layer.forward(X)\r\n",
                "# print(fr.shape, fr)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 170,
            "source": [
                "\r\n",
                "#batch test\r\n",
                "x = np.array([1,2,3,4])\r\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\r\n",
                "filter_size, n_input,n_output = 3,2,3\r\n",
                "# print('f,nin,nout: ', filter_size, n_input,n_output)\r\n",
                "layer = Conv1DBatch(filter_size = filter_size, n_input = n_input, n_output =n_output)\r\n",
                "x = np.vstack([x] * n_input)\r\n",
                "w = np.vstack([[np.vstack([w]*n_input)]] * n_output)\r\n",
                "b = np.ones(n_output)\r\n",
                "layer.W = w\r\n",
                "layer.B = b\r\n",
                "\r\n",
                "#add dim\r\n",
                "batch_size = 2\r\n",
                "x = np.vstack([[x]] * batch_size)\r\n",
                "\r\n",
                "print(x.shape,x)\r\n",
                "fr = layer.forward(x)\r\n",
                "print(fr)\r\n",
                "\r\n",
                "delta_a = np.vstack([[10, 20]]* n_output)\r\n",
                "delta_a = np.vstack([[delta_a]] * batch_size)\r\n",
                "# layer.dA = delta_a\r\n",
                "print('delta_a: ', delta_a.shape, delta_a)\r\n",
                "dW,dB,dX = layer.backward(delta_a)\r\n",
                "\r\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\r\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\r\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(2, 2, 4) [[[1 2 3 4]\n",
                        "  [1 2 3 4]]\n",
                        "\n",
                        " [[1 2 3 4]\n",
                        "  [1 2 3 4]]]\n",
                        "[[[69. 99.]\n",
                        "  [69. 99.]\n",
                        "  [69. 99.]]\n",
                        "\n",
                        " [[69. 99.]\n",
                        "  [69. 99.]\n",
                        "  [69. 99.]]]\n",
                        "delta_a:  (2, 3, 2) [[[10 20]\n",
                        "  [10 20]\n",
                        "  [10 20]]\n",
                        "\n",
                        " [[10 20]\n",
                        "  [10 20]\n",
                        "  [10 20]]]\n",
                        "dB:  [30 30 30] correct result:  [30]\n",
                        "dW:  [[[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]] correct result:  [ 50  80 110]\n",
                        "dX:  [[[ 90. 330. 510. 420.]\n",
                        "  [ 90. 330. 510. 420.]]\n",
                        "\n",
                        " [[ 75. 276. 429. 354.]\n",
                        "  [ 75. 276. 429. 354.]]] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Modified Copy Old Code"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 171,
            "source": [
                "from sklearn.preprocessing import OneHotEncoder\r\n",
                "from sklearn.metrics import accuracy_score\r\n",
                "\r\n",
                "\r\n",
                "#! activations..............\r\n",
                "\r\n",
                "class ActivationFunction():\r\n",
                "  def forward(self,A):\r\n",
                "    pass\r\n",
                "  def backward(self,dZ):\r\n",
                "    pass\r\n",
                "class Tanh(ActivationFunction):\r\n",
                "  def forward(self,A):\r\n",
                "    self.A = A\r\n",
                "    Z = (np.exp(A) - np.exp(-A)) / (np.exp(A) + np.exp(-A))\r\n",
                "    return Z\r\n",
                "  def backward(self,dZ):\r\n",
                "    A = self.A\r\n",
                "    dA = dZ * (1 - np.tanh(A) ** 2)\r\n",
                "    return dA\r\n",
                "  \r\n",
                "class SoftMax(ActivationFunction):\r\n",
                "  def forward(self,A):\r\n",
                "    self.A = A\r\n",
                "    Z = np.exp(A) / np.sum(np.exp(A), axis = 1).reshape(-1,1)\r\n",
                "    return Z\r\n",
                "  def backward(self,Z,Y):\r\n",
                "    A = self.A\r\n",
                "    nb = Z.shape[0]\r\n",
                "    dA = 1/nb * (Z - Y)\r\n",
                "    return dA\r\n",
                "\r\n",
                "#! Full Connected Layer ...........................\r\n",
                "\r\n",
                "class FC:\r\n",
                "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\r\n",
                "        # Initialize\r\n",
                "        self.optimizer = optimizer\r\n",
                "        self.n_nodes1, self.n_nodes2 = n_nodes1, n_nodes2\r\n",
                "        # Initialize self.W and self.B using the initializer method\r\n",
                "        self.W = initializer.W(n_nodes1, n_nodes2)\r\n",
                "        self.B = initializer.B(n_nodes2)\r\n",
                "        pass\r\n",
                "    def forward(self, X):   \r\n",
                "        self.X = X\r\n",
                "        A = X @ self.W + self.B\r\n",
                "        return A\r\n",
                "    def backward(self, dA):\r\n",
                "        # update\r\n",
                "        self.dA = dA\r\n",
                "        self.dW = self.calc_dW()\r\n",
                "        self.dB = self.calc_dB()\r\n",
                "        self.dZ = self.calc_dZ()\r\n",
                "\r\n",
                "        self = self.optimizer.update(self)\r\n",
                "        return self.dW, self.dB, self.dZ\r\n",
                "        # return self.dZ\r\n",
                "    def calc_dB(self):\r\n",
                "        dB = self.dA.sum(axis = 0).reshape(1,-1)  \r\n",
                "        return dB\r\n",
                "    def calc_dW(self):\r\n",
                "        dW = self.X.T @ self.dA\r\n",
                "        return dW\r\n",
                "    def calc_dZ(self):\r\n",
                "        dZ = self.dA @ self.W.T\r\n",
                "        return dZ\r\n",
                "\r\n",
                "class AdaGrad():\r\n",
                "    \"\"\"\r\n",
                "    Stochastic gradient descent\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    lr : Learning rate\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, lr):\r\n",
                "        self.lr = lr\r\n",
                "        self.Hw = 0\r\n",
                "        self.Hb = 0\r\n",
                "    def update(self, layer):\r\n",
                "        \"\"\"\r\n",
                "        Update weights and biases for a layer\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        layer : Instance of the layer before update\r\n",
                "        \"\"\"\r\n",
                "        #ada weight\r\n",
                "        dW = layer.dW\r\n",
                "        dB = layer.dB\r\n",
                "\r\n",
                "        eps = 1e-6\r\n",
                "        self.Hw += dW ** 2\r\n",
                "        self.Hb += dB ** 2\r\n",
                "        layer.W += - self.lr * (1 / np.sqrt(self.Hw + eps)) * dW\r\n",
                "        layer.B += - self.lr * (1 / np.sqrt(self.Hb + eps)) * dB\r\n",
                "        return layer\r\n",
                "\r\n",
                "\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 8\r\n",
                "Learing and Estimation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Deep Neural Net Proto\r\n",
                "**Tweaked from the old implementation to fit with new Conv1D**"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 172,
            "source": [
                "class DeepNeuralNetworkClassifier():\r\n",
                "    def __init__(self,encoder, max_iter = 5, lr = 0.1, batch_size = 20 ,\r\n",
                "               verbose = False, debug = False):\r\n",
                "        self.epoch = max_iter\r\n",
                "        self.verbose = verbose\r\n",
                "        self.debug = debug\r\n",
                "        self.lr = lr\r\n",
                "        #other non-parametric vars:\r\n",
                "        self.encoder = encoder\r\n",
                "        self.sigma = 0.01\r\n",
                "        self.batch_size = batch_size # batch size \r\n",
                "        \r\n",
                "        #layers and activations\r\n",
                "        self.layers = []\r\n",
                "        self.activations = []\r\n",
                "\r\n",
                "    def add(self,layer, activation):\r\n",
                "      self.layers.append(layer)\r\n",
                "      self.activations.append(activation)\r\n",
                "        \r\n",
                "    def enum_layer_act(self, rev = False):\r\n",
                "      zipped = zip(self.layers, self.activations)\r\n",
                "      if rev:\r\n",
                "        return enumerate(reversed(list(zipped)))\r\n",
                "      return enumerate(zipped)\r\n",
                "      \r\n",
                "    def forward_prop(self,X):\r\n",
                "      Z = X\r\n",
                "      for i, (layer, activation) in self.enum_layer_act():\r\n",
                "        A = layer.forward(Z)\r\n",
                "        Z = activation.forward(A)\r\n",
                "        if self.debug:\r\n",
                "          print(f'Z{i+1}: ', Z.shape, A.shape)\r\n",
                "      return Z\r\n",
                "\r\n",
                "    def backward_prop(self,Z,y):\r\n",
                "      dA = self.activations[-1].backward(Z,y)\r\n",
                "      if self.debug:\r\n",
                "        print(f'Backward last Y: ', dA.shape)\r\n",
                "      for i, (layer, activation) in self.enum_layer_act(rev = True):\r\n",
                "        if i == 0: #last layer has different activation backward!\r\n",
                "          dW,dB,dZ = layer.backward(dA)\r\n",
                "          if self.debug:\r\n",
                "            print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
                "          continue\r\n",
                "        dA = activation.backward(dZ)\r\n",
                "        dW,dB,dZ = layer.backward(dA)\r\n",
                "        if self.debug:\r\n",
                "          print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
                "        \r\n",
                "    \r\n",
                "    def cross_entropy_error(self,Z,y):\r\n",
                "      return (np.log(Z) * y).sum() / (- len(Z))\r\n",
                "\r\n",
                "    def predict(self,X):\r\n",
                "      y = np.zeros(X.shape[0])\r\n",
                "      Z  = self.forward_prop(X)\r\n",
                "      return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))\r\n",
                "\r\n",
                "\r\n",
                "    def fit(self,X,y, X_val = None, y_val = None):\r\n",
                "      #prepare\r\n",
                "      self.n_features = X.shape[1]\r\n",
                "      self.lenx = len(X)\r\n",
                "      self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\r\n",
                "\r\n",
                "      if self.verbose:\r\n",
                "          print('X shape: ', X.shape, 'type: ', X.dtype)\r\n",
                "          print('Batch count: ', self.batch_count)\r\n",
                "          # for i, (layer, activation) in self.enum_layer_act():\r\n",
                "          #   print(f'Layer {i+1}: ', layer.n_nodes1, layer.n_nodes2)\r\n",
                "          #   print(f'Activ: {i+1}:', activation.__class__.__name__)\r\n",
                "\r\n",
                "      #train\r\n",
                "      self.loss = np.zeros(self.epoch)\r\n",
                "      self.accuracy = np.zeros(self.epoch)\r\n",
                "      for i in range(self.epoch): #one full data ilteration\r\n",
                "          if self.verbose: print('Epoch: ', i)\r\n",
                "          self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\r\n",
                "          for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\r\n",
                "              if self.debug: print('Current batch: ', idx, mini_X_train.shape, mini_y_train.shape)\r\n",
                "              #train mini_batch\r\n",
                "              Z = self.forward_prop(mini_X_train)\r\n",
                "              self.backward_prop(Z,mini_y_train)\r\n",
                "\r\n",
                "\r\n",
                "          #record loss data\r\n",
                "          if self.debug: print('Predicting and recording loss/acc')\r\n",
                "          Z = self.forward_prop(X)\r\n",
                "          self.loss[i] = self.cross_entropy_error(Z,y)\r\n",
                "          train_pred = self.predict(X)\r\n",
                "          self.accuracy[i]  = accuracy_score(train_pred,y)\r\n",
                "          if self.verbose:\r\n",
                "              print(f'Loss {i}:', self.loss[i])\r\n",
                "              print(f'Acc {i}:', self.accuracy[i])\r\n",
                "              \r\n",
                "      #verbose\r\n",
                "      if self.verbose:\r\n",
                "          print('Final train loss:',self.loss[-1])\r\n",
                "          print('Final train accuracy:',self.accuracy[-1])\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "source": [
                "#data set\r\n",
                "from keras.datasets import mnist\r\n",
                "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
                "# #reshape\r\n",
                "# X_train = X_train.reshape(-1, 784)\r\n",
                "# X_test = X_test.reshape(-1, 784)\r\n",
                "#scaling\r\n",
                "X_train = X_train.astype(np.float)\r\n",
                "X_test = X_test.astype(np.float)\r\n",
                "X_train /= 255\r\n",
                "X_test /= 255\r\n",
                "#one hot encode for multiclass labels!\r\n",
                "from sklearn.preprocessing import OneHotEncoder\r\n",
                "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
                "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\r\n",
                "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\r\n",
                "#validation split\r\n",
                "from sklearn.model_selection import train_test_split\r\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, train_size=0.5)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 174,
            "source": [
                "print(X_train.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(30000, 28, 28)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 175,
            "source": [
                "#flatten layer\r\n",
                "class FlattenLayer():\r\n",
                "    def __init__(self):\r\n",
                "        self.in_shape = None\r\n",
                "    def forward(self,X):\r\n",
                "        if (X.shape != self.in_shape): print('self: ', self.in_shape, 'x: ',X.shape)\r\n",
                "        self.in_shape = X.shape\r\n",
                "        return X.reshape(-1,X.shape[-1]) #flatten the channel to 1\r\n",
                "    def backward(self,dA):\r\n",
                "        return None, None ,dA.reshape(self.in_shape)\r\n",
                "class TransparentFunction():\r\n",
                "    def forward(self,X):\r\n",
                "        return X\r\n",
                "    def backward(self,dA):\r\n",
                "        return dA"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 176,
            "source": [
                "# test run deep neural\r\n",
                "model = DeepNeuralNetworkClassifier(enc, debug = False, verbose = True, max_iter = 5)\r\n",
                "n_in_features = X_train.shape[1]\r\n",
                "batch_size  = 20\r\n",
                "print('train shape: ', X_train.shape)\r\n",
                "print('input features: ', n_in_features)\r\n",
                "print('input channels: ', batch_size)\r\n",
                "\r\n",
                "\r\n",
                "in_channel = X_train.shape[1]\r\n",
                "\r\n",
                "l1 = Conv1DBatch(filter_size = 3, n_input = in_channel, n_output = 1, optimizer = AdaGrad(0.1))\r\n",
                "model.add(l1,Tanh())\r\n",
                "lshape = FlattenLayer()\r\n",
                "model.add(lshape,TransparentFunction())\r\n",
                "l2 = FC(26,100, SimpleInitializer(),SGD()) #cause output of conv is 394\r\n",
                "model.add(l2,Tanh())\r\n",
                "l3 = FC(100,10, SimpleInitializer(),SGD())\r\n",
                "model.add(l3,SoftMax())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "train shape:  (30000, 28, 28)\n",
                        "input features:  28\n",
                        "input channels:  20\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 177,
            "source": [
                "print('xshape: ', X_train.shape, 'yshape: ', y_train.shape)\r\n",
                "model.fit(X_train, y_train)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "xshape:  (30000, 28, 28) yshape:  (30000, 10)\n",
                        "X shape:  (30000, 28, 28) type:  float64\n",
                        "Batch count:  1500\n",
                        "Epoch:  0\n",
                        "self:  None x:  (20, 1, 26)\n",
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 0: 1.2185637287664015\n",
                        "Acc 0: 0.6158666666666667\n",
                        "Epoch:  1\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n",
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 1: 1.0851401636641715\n",
                        "Acc 1: 0.6580333333333334\n",
                        "Epoch:  2\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n",
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 2: 1.0236440081154576\n",
                        "Acc 2: 0.6764666666666667\n",
                        "Epoch:  3\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n",
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 3: 0.9873512058209677\n",
                        "Acc 3: 0.6870333333333334\n",
                        "Epoch:  4\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n",
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 4: 0.9617437135828294\n",
                        "Acc 4: 0.6922666666666667\n",
                        "Final train loss: 0.9617437135828294\n",
                        "Final train accuracy: 0.6922666666666667\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Finally it works, though im not sure why accuracy is so low, could be:\r\n",
                "- Compression from 28 -> 1 channel\r\n",
                "- Wrong code?\r\n",
                "- ...?\r\n",
                "  \r\n",
                "**Anyway, it works :), took real long for this one**"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 178,
            "source": [
                "# loss result\r\n",
                "plt.plot(model.loss, label = 'TrainLoss')\r\n",
                "plt.plot(model.accuracy, label = 'TrainAccuracy')\r\n",
                "plt.title('Training Metrics')\r\n",
                "plt.legend()\r\n",
                "plt.show()"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### NOTE:\r\n",
                "**Stride is not implemented!**"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}