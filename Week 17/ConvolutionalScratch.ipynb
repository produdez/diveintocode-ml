{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Introduction\r\n",
                "Let's create the class Convolutional Neural Network (CNN) from scratch. We will implement the algorithm using only the minimum library such as NumPy.\r\n",
                "\r\n",
                "\r\n",
                "In this Sprint, we will build a 1D Convolutional layer and try to understand the basics of convolution. The next Sprint completes the CNN commonly used for images by creating a two-dimensional convolutional layer and a pooling layer.\r\n",
                "\r\n",
                "\r\n",
                "Name the class Scratch1dCNNClassifier. Please refer to the ScratchDeepNeuralNetrowkClassifier created in the previous Sprint for the class structure.\r\n",
                "\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "#needed\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import seaborn as sns\r\n",
                "import matplotlib.pyplot as plt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Data Set"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "#data set\r\n",
                "from keras.datasets import mnist\r\n",
                "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
                "#reshape\r\n",
                "X_train = X_train.reshape(-1, 784)\r\n",
                "X_test = X_test.reshape(-1, 784)\r\n",
                "#scaling\r\n",
                "X_train = X_train.astype(np.float)\r\n",
                "X_test = X_test.astype(np.float)\r\n",
                "X_train /= 255\r\n",
                "X_test /= 255\r\n",
                "#one hot encode for multiclass labels!\r\n",
                "from sklearn.preprocessing import OneHotEncoder\r\n",
                "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
                "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\r\n",
                "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\r\n",
                "#validation split\r\n",
                "from sklearn.model_selection import train_test_split\r\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "#intializer\r\n",
                "class SimpleInitializer:\r\n",
                "  def __init__(self, sigma = 0.1):\r\n",
                "    self.sigma = sigma\r\n",
                "  def W(self, n_nodes1, n_nodes2):\r\n",
                "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\r\n",
                "    return W\r\n",
                "  def B(self, n_nodes2):\r\n",
                "\r\n",
                "    B = self.sigma * np.random.randn(1,n_nodes2)\r\n",
                "    return B\r\n",
                "class SGD:\r\n",
                "    \"\"\"\r\n",
                "    Stochastic gradient descent\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    lr : Learning rate\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, lr=0.01):\r\n",
                "        self.lr = lr\r\n",
                "    def update(self, layer):\r\n",
                "        \"\"\"\r\n",
                "        Update weights and biases for a layer\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        layer : Instance of the layer before update\r\n",
                "        \"\"\"\r\n",
                "        #update\r\n",
                "        layer.B += - self.lr * layer.dB\r\n",
                "        layer.W += - self.lr * layer.dW\r\n",
                "        return layer\r\n",
                "#! mini batch...........................\r\n",
                "class GetMiniBatch:\r\n",
                "    \"\"\"\r\n",
                "Iterator to get a mini-batch\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    X : The following forms of ndarray, shape (n_samples, n_features)\r\n",
                "      Training data\r\n",
                "    y : The following form of ndarray, shape (n_samples, 1)\r\n",
                "      Correct answer value\r\n",
                "    batch_size : int\r\n",
                "      Batch size\r\n",
                "    seed : int\r\n",
                "      NumPy random number seed\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, X, y = None, batch_size = 20, seed=0):\r\n",
                "        self.batch_size = batch_size\r\n",
                "        np.random.seed(seed)\r\n",
                "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
                "        self._X = X[shuffle_index]\r\n",
                "        self._y = y[shuffle_index] if y is not None else None\r\n",
                "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
                "    def __len__(self):\r\n",
                "        return self._stop\r\n",
                "    def __getitem__(self,item):\r\n",
                "        p0 = item*self.batch_size\r\n",
                "        p1 = item*self.batch_size + self.batch_size\r\n",
                "        if self._y is not None:\r\n",
                "          return self._X[p0:p1], self._y[p0:p1] \r\n",
                "        else:\r\n",
                "          return self._X[p0:p1]       \r\n",
                "    def __iter__(self):\r\n",
                "        self._counter = 0\r\n",
                "        return self\r\n",
                "    def __next__(self):\r\n",
                "        if self._counter >= self._stop:\r\n",
                "            raise StopIteration()\r\n",
                "        p0 = self._counter*self.batch_size\r\n",
                "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
                "        self._counter += 1\r\n",
                "        if self._y is not None:\r\n",
                "          return self._X[p0:p1], self._y[p0:p1] \r\n",
                "        else:\r\n",
                "          return self._X[p0:p1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 1\r\n",
                "1 Dim convolutional Layer that limit number of channels to 1"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Forward"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "class SimpleConv1D:\r\n",
                "  def __init__(self, filter_size, initializer, optimizer):\r\n",
                "    # Initialize\r\n",
                "    self.optimizer = optimizer\r\n",
                "    self.filter_size = filter_size\r\n",
                "    # Initialize self.W and self.B using the initializer method\r\n",
                "    self.W = initializer.W(filter_size, 1)\r\n",
                "    self.B = initializer.B(1)\r\n",
                "  def forward(self, X):\r\n",
                "    filter_size = self.filter_size\r\n",
                "    self.X = X\r\n",
                "    return self.convolve(self.W,X, bias = self.B)\r\n",
                "\r\n",
                "  def backward(self, dA):\r\n",
                "    # update\r\n",
                "    self.dA = dA\r\n",
                "    self.dW = self.calc_dW()\r\n",
                "    self.dB = self.calc_dB()\r\n",
                "    self.dX = self.calc_dX()\r\n",
                "    self = self.optimizer.update(self)\r\n",
                "    return self.dW, self.dB, self.dX\r\n",
                "  def calc_dW(self):\r\n",
                "    return self.convolve(self.dA, self.X)\r\n",
                "  def calc_dB(self):\r\n",
                "    return np.array(self.dA.sum()).ravel()\r\n",
                "  def calc_dX(self):\r\n",
                "    padA = self.padA()\r\n",
                "    return self.convolve(np.flip(self.W),padA)\r\n",
                "  def padA(self):\r\n",
                "    padding = (len(self.X) - 1 + len(self.W) - len(self.dA)) //2 # cause out = (in - fil + 2pad) + 1\r\n",
                "    print('Padding size: ', padding)\r\n",
                "    return np.pad(self.dA,(padding, padding), mode = 'constant')\r\n",
                "  def convolve(self,F,X,bias = [0]):\r\n",
                "    A = []\r\n",
                "    filter_size = len(F)\r\n",
                "    for i in range(len(X) - filter_size + 1):\r\n",
                "      A.append(X[i : i + filter_size] @ F + bias[0])\r\n",
                "    return np.array(A)\r\n",
                "\r\n",
                "    \r\n",
                "  "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 2\r\n",
                "Output size calculation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def conv_n_out(n_in,filter_size, padding=0, stride=1):\r\n",
                "    return (n_in + 2*padding - filter_size) //stride + 1\r\n",
                "print('Convolutional n-out of input 30 and filter 3:', conv_n_out(30,3))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Convolutional n-out of input 30 and filter 3: 28\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 3\r\n",
                "Experiment of one-dimensional convolutional layer with small array"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "x = np.array([1,2,3,4])\r\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\r\n",
                "b = np.array([1], dtype = np.float64)\r\n",
                "layer = SimpleConv1D(3,SimpleInitializer(),SGD())\r\n",
                "layer.W = w\r\n",
                "layer.B = b\r\n",
                "\r\n",
                "print('n_output: ', conv_n_out(len(x),len(w)))\r\n",
                "\r\n",
                "correct_forward_result = np.array([35, 50])\r\n",
                "a = layer.forward(x)\r\n",
                "print('forward: ', layer.forward(x), 'correct result: ', correct_forward_result)\r\n",
                "\r\n",
                "\r\n",
                "delta_a = np.array([10, 20])\r\n",
                "dW,dB,dX = layer.backward(delta_a)\r\n",
                "\r\n",
                "\r\n",
                "correct_delta_b = np.array([30])\r\n",
                "correct_delta_x = np.array([30, 110, 170, 140])\r\n",
                "correct_delta_w = np.array([50, 80, 110])\r\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\r\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\r\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "n_output:  2\n",
                        "forward:  [35. 50.] correct result:  [35 50]\n",
                        "Padding size:  2\n",
                        "dB:  [30] correct result:  [30]\n",
                        "dW:  [ 50  80 110] correct result:  [ 50  80 110]\n",
                        "dX:  [ 30. 110. 170. 140.] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 4\r\n",
                "Creating a one-dimensional convolutional layer class that does not limit the number of channels"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Updated Prototype (NO STRIDE, NO BATCH)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "class Conv1D:\r\n",
                "  def __init__(self, filter_size = 3, n_input = 2, n_output = 3, optimizer = SGD(), padding = 0):\r\n",
                "    # Initialize\r\n",
                "    self.optimizer = optimizer\r\n",
                "    self.filter_size = filter_size\r\n",
                "    # Initialize self.W and self.B using the initializer method\r\n",
                "    self.filter_size = filter_size\r\n",
                "    self.n_input = n_input\r\n",
                "    self.n_output = n_output\r\n",
                "\r\n",
                "    self.W = np.ones((n_output, n_input,filter_size))\r\n",
                "    self.B = np.ones(n_output)\r\n",
                "    #padding and stride\r\n",
                "    self.padding = padding\r\n",
                "    #batch size to respond to mini batch\r\n",
                "    self.batch_size = batch_size\r\n",
                "  def forward(self, X): \r\n",
                "    X = np.pad(X,[(0,0),(self.padding, self.padding)], mode = 'constant')\r\n",
                "    self.X = X\r\n",
                "    output = []\r\n",
                "    for i in range(self.n_output):\r\n",
                "      filt = self.W[i]\r\n",
                "      bias = self.B[i]\r\n",
                "      conv = self.convolve(filt, X)\r\n",
                "      output.append(conv.sum(axis = 0) + bias)\r\n",
                "    return np.array(output)\r\n",
                "  def backward(self, dA):\r\n",
                "    # update\r\n",
                "    self.dA = dA\r\n",
                "    self.dW = self.calc_dW()\r\n",
                "    self.dB = self.calc_dB()\r\n",
                "    self.dX = self.calc_dX()\r\n",
                "    self = self.optimizer.update(self)\r\n",
                "    # print(f'After backward: x {self.X.shape}, w: {self.W.shape}, b: {self.B.shape}, dX: {self.dX.shape}, dW: {self.dW.shape}, dB: {self.dB.shape}, dA: {self.dA.shape}')\r\n",
                "    # print(f'filterz: {self.filter_size}')\r\n",
                "    return self.dW, self.dB, self.dX\r\n",
                "  def calc_dW(self):\r\n",
                "    dW = []\r\n",
                "    dupped_dA = np.repeat(self.dA[:,np.newaxis, : ], self.n_input, axis=1)\r\n",
                "    for i in range(self.n_output): #convolve each output_channel through X\r\n",
                "      conv = self.convolve(dupped_dA[i], self.X)\r\n",
                "      dW.append(conv) \r\n",
                "    return np.array(dW)\r\n",
                "  def calc_dB(self):\r\n",
                "    return np.array(self.dA.sum(axis = 1))\r\n",
                "  def calc_dX(self):\r\n",
                "    pad_dA = self.pad_dA() #match a with x\r\n",
                "    flipped_W = np.flip(self.W,axis = 2).reshape(self.n_input,self.n_output, -1) # flip each filter and the in_out dim also\r\n",
                "    output = []\r\n",
                "    for i in range(self.n_input):\r\n",
                "      filt = flipped_W[i]\r\n",
                "      conv = self.convolve(filt, pad_dA)\r\n",
                "      output.append(conv.sum(axis = 0))\r\n",
                "    return np.array(output)\r\n",
                "  def pad_dA(self):\r\n",
                "    n_features_in = self.dA.shape[1]\r\n",
                "    n_features_out = self.X.shape[1]\r\n",
                "    filter_size = self.filter_size\r\n",
                "    padding = (n_features_out - 1 + filter_size - n_features_in) // 2\r\n",
                "    return self.pad(self.dA, padding)\r\n",
                "  def pad(self,array,padding):\r\n",
                "    return np.pad(array,[(0,0),(padding, padding)], mode = 'constant')\r\n",
                "  def convolve(self,F,X):\r\n",
                "    A = []\r\n",
                "    filter_size = F.shape[1]\r\n",
                "    feature_count = X.shape[1]\r\n",
                "    n_out_features = (feature_count - filter_size) + 1\r\n",
                "    for i in range(n_out_features):\r\n",
                "      A.append((X[:,i : i + filter_size] * F).sum(axis=1))\r\n",
                "    return np.array(A).T"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Forward"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) #shape (2, 4), (number of input channels, number of features).\r\n",
                "# x = np.array([[1, 2, 3], [3, 4, 5]]) #shape (2, 4), (number of input channels, number of features).\r\n",
                "w = np.ones((3, 2, 3)) # Set to 1 for simplification of the example. (Number of output channels, number of input channels, filter size).\r\n",
                "b = np.array([1, 2, 3]) # (Number of output channels)\r\n",
                "\r\n",
                "conv = Conv1D(3,2,3)\r\n",
                "conv.B = b\r\n",
                "print(conv.forward(x))\r\n",
                "print('correct: result: ',np.array([[16, 22], [17, 23], [18, 24]]))"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'batch_size' is not defined",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-8-05034db2fe97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (Number of output channels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-7-155de71d599b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filter_size, n_input, n_output, optimizer, padding)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#batch size to respond to mini batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Let's user problem 3 as a reference to make the multichannel propagation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "x = np.array([1,2,3,4])\r\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\r\n",
                "filter_size, n_input,n_output = 3,2,3\r\n",
                "# print('f,nin,nout: ', filter_size, n_input,n_output)\r\n",
                "layer = Conv1D()\r\n",
                "x = np.vstack([x] * n_input)\r\n",
                "w = np.vstack([[np.vstack([w]*n_input)]] * n_output)\r\n",
                "b = np.ones(n_output)\r\n",
                "layer.W = w\r\n",
                "layer.B = b\r\n",
                "\r\n",
                "print('X: ', x)\r\n",
                "print('w: ', w)\r\n",
                "print('b: ', b)\r\n",
                "\r\n",
                "correct_forward_result = np.array([35, 50])\r\n",
                "a = layer.forward(x)\r\n",
                "print('forward: ', layer.forward(x), 'correct result: ', correct_forward_result)\r\n",
                "\r\n",
                "\r\n",
                "delta_a = np.vstack([[10, 20]]* n_output)\r\n",
                "layer.dA = delta_a\r\n",
                "dW = layer.calc_dW()\r\n",
                "dB = layer.calc_dB()\r\n",
                "dX = layer.calc_dX()\r\n",
                "\r\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\r\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\r\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "X:  [[1 2 3 4]\n",
                        " [1 2 3 4]]\n",
                        "w:  [[[3. 5. 7.]\n",
                        "  [3. 5. 7.]]\n",
                        "\n",
                        " [[3. 5. 7.]\n",
                        "  [3. 5. 7.]]\n",
                        "\n",
                        " [[3. 5. 7.]\n",
                        "  [3. 5. 7.]]]\n",
                        "b:  [1. 1. 1.]\n",
                        "forward:  [[69. 99.]\n",
                        " [69. 99.]\n",
                        " [69. 99.]] correct result:  [35 50]\n",
                        "dB:  [30 30 30] correct result:  [30]\n",
                        "dW:  [[[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]] correct result:  [ 50  80 110]\n",
                        "dX:  [[ 90. 330. 510. 420.]\n",
                        " [ 90. 330. 510. 420.]] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 5\r\n",
                "Implement Padding"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Note: padding added in prototype"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#padding example\r\n",
                "def add_padding(X, padding_size):\r\n",
                "    return np.pad(X, ((0,0),(padding_size,padding_size)), mode = 'constant', constant_values = 0)\r\n",
                "\r\n",
                "#we are conv 1d so just pad horizontally\r\n",
                "print(f'x: {x}')\r\n",
                "print(f'padded:\\n {add_padding(x, 3)}')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "x: [[1 2 3 4]\n",
                        " [1 2 3 4]]\n",
                        "padded:\n",
                        " [[0 0 0 1 2 3 4 0 0 0]\n",
                        " [0 0 0 1 2 3 4 0 0 0]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# test run on prototype just to see if it works\r\n",
                "padded_layer = Conv1D(padding = 2)\r\n",
                "a = padded_layer.forward(x)\r\n",
                "dW,dB,dX = padded_layer.backward(a)\r\n",
                "print('shapes: ', dW.shape,dB.shape,dX.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "shapes:  (3, 2, 3) (3,) (2, 8)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 6\r\n",
                "Response to mini batch"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Mini Batch is used only during prediction** "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#! exapmle?"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Modified Copy Old Code"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.preprocessing import OneHotEncoder\r\n",
                "from sklearn.metrics import accuracy_score\r\n",
                "\r\n",
                "\r\n",
                "#! activations..............\r\n",
                "\r\n",
                "class ActivationFunction():\r\n",
                "  def forward(self,A):\r\n",
                "    pass\r\n",
                "  def backward(self,dZ):\r\n",
                "    pass\r\n",
                "class Tanh(ActivationFunction):\r\n",
                "  def forward(self,A):\r\n",
                "    self.A = A\r\n",
                "    Z = (np.exp(A) - np.exp(-A)) / (np.exp(A) + np.exp(-A))\r\n",
                "    return Z\r\n",
                "  def backward(self,dZ):\r\n",
                "    A = self.A\r\n",
                "    dA = dZ * (1 - np.tanh(A) ** 2)\r\n",
                "    return dA\r\n",
                "  \r\n",
                "class SoftMax(ActivationFunction):\r\n",
                "  def forward(self,A):\r\n",
                "    self.A = A\r\n",
                "    Z = np.exp(A) / np.sum(np.exp(A), axis = 1).reshape(-1,1)\r\n",
                "    return Z\r\n",
                "  def backward(self,Z,Y):\r\n",
                "    A = self.A\r\n",
                "    nb = Z.shape[0]\r\n",
                "    dA = 1/nb * (Z - Y)\r\n",
                "    return dA\r\n",
                "\r\n",
                "#! Full Connected Layer ...........................\r\n",
                "\r\n",
                "class FC:\r\n",
                "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\r\n",
                "        # Initialize\r\n",
                "        self.optimizer = optimizer\r\n",
                "        self.n_nodes1, self.n_nodes2 = n_nodes1, n_nodes2\r\n",
                "        # Initialize self.W and self.B using the initializer method\r\n",
                "        self.W = initializer.W(n_nodes1, n_nodes2)\r\n",
                "        self.B = initializer.B(n_nodes2)\r\n",
                "        pass\r\n",
                "    def forward(self, X):   \r\n",
                "        self.X = X\r\n",
                "        A = X @ self.W + self.B\r\n",
                "        return A\r\n",
                "    def backward(self, dA):\r\n",
                "        # update\r\n",
                "        self.dA = dA\r\n",
                "        self.dW = self.calc_dW()\r\n",
                "        self.dB = self.calc_dB()\r\n",
                "        self.dZ = self.calc_dZ()\r\n",
                "\r\n",
                "        self = self.optimizer.update(self)\r\n",
                "        return self.dW, self.dB, self.dZ\r\n",
                "        # return self.dZ\r\n",
                "    def calc_dB(self):\r\n",
                "        dB = self.dA.sum(axis = 0).reshape(1,-1)  \r\n",
                "        return dB\r\n",
                "    def calc_dW(self):\r\n",
                "        dW = self.X.T @ self.dA\r\n",
                "        return dW\r\n",
                "    def calc_dZ(self):\r\n",
                "        dZ = self.dA @ self.W.T\r\n",
                "        return dZ\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 8\r\n",
                "Learing and Estimation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Deep Neural Net Proto\r\n",
                "**Tweaked from the old implementation to fit with new Conv1D**"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class DeepNeuralNetworkClassifier():\r\n",
                "    def __init__(self,encoder, max_iter = 5, lr = 0.1, batch_size = 20 ,\r\n",
                "               verbose = False, debug = False):\r\n",
                "        self.epoch = max_iter\r\n",
                "        self.verbose = verbose\r\n",
                "        self.debug = debug\r\n",
                "        self.lr = lr\r\n",
                "        #other non-parametric vars:\r\n",
                "        self.encoder = encoder\r\n",
                "        self.sigma = 0.01\r\n",
                "        self.batch_size = batch_size # batch size \r\n",
                "        \r\n",
                "        #layers and activations\r\n",
                "        self.layers = []\r\n",
                "        self.activations = []\r\n",
                "\r\n",
                "    def add(self,layer, activation):\r\n",
                "      self.layers.append(layer)\r\n",
                "      self.activations.append(activation)\r\n",
                "        \r\n",
                "    def enum_layer_act(self, rev = False):\r\n",
                "      zipped = zip(self.layers, self.activations)\r\n",
                "      if rev:\r\n",
                "        return enumerate(reversed(list(zipped)))\r\n",
                "      return enumerate(zipped)\r\n",
                "      \r\n",
                "    def forward_prop(self,X):\r\n",
                "      Z = X\r\n",
                "      for i, (layer, activation) in self.enum_layer_act():\r\n",
                "        A = layer.forward(Z)\r\n",
                "        Z = activation.forward(A)\r\n",
                "        if self.debug:\r\n",
                "          print(f'Z{i+1}: ', Z.shape, A.shape)\r\n",
                "      return Z\r\n",
                "\r\n",
                "    def backward_prop(self,Z,y):\r\n",
                "      dA = self.activations[-1].backward(Z,y)\r\n",
                "      if self.debug:\r\n",
                "        print(f'Backward last Y: ', dA.shape)\r\n",
                "      for i, (layer, activation) in self.enum_layer_act(rev = True):\r\n",
                "        if i == 0: #last layer has different activation backward!\r\n",
                "          dW,dB,dZ = layer.backward(dA)\r\n",
                "          if self.debug:\r\n",
                "            print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
                "          continue\r\n",
                "        dA = activation.backward(dZ)\r\n",
                "        dW,dB,dZ = layer.backward(dA)\r\n",
                "        if self.debug:\r\n",
                "          print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
                "        \r\n",
                "    \r\n",
                "    def cross_entropy_error(self,Z,y):\r\n",
                "      return (np.log(Z) * y).sum() / (- len(Z))\r\n",
                "\r\n",
                "    def predict(self,X):\r\n",
                "      y = np.zeros(X.shape[0])\r\n",
                "      Z  = self.forward_prop(X)\r\n",
                "      return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))\r\n",
                "\r\n",
                "\r\n",
                "    def fit(self,X,y, X_val = None, y_val = None):\r\n",
                "      #prepare\r\n",
                "      self.n_features = X.shape[1]\r\n",
                "      self.lenx = len(X)\r\n",
                "      self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\r\n",
                "\r\n",
                "      if self.verbose:\r\n",
                "          print('X shape: ', X.shape, 'type: ', X.dtype)\r\n",
                "          print('Batch count: ', self.batch_count)\r\n",
                "          # for i, (layer, activation) in self.enum_layer_act():\r\n",
                "          #   print(f'Layer {i+1}: ', layer.n_nodes1, layer.n_nodes2)\r\n",
                "          #   print(f'Activ: {i+1}:', activation.__class__.__name__)\r\n",
                "\r\n",
                "      #train\r\n",
                "      self.loss = np.zeros(self.epoch)\r\n",
                "      self.accuracy = np.zeros(self.epoch)\r\n",
                "      for i in range(self.epoch): #one full data ilteration\r\n",
                "          if self.verbose: print('Epoch: ', i)\r\n",
                "          self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\r\n",
                "          for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\r\n",
                "              if self.debug: print('Current batch: ', idx)\r\n",
                "              #train mini_batch\r\n",
                "              Z = self.forward_prop(mini_X_train)\r\n",
                "              self.backward_prop(Z,mini_y_train)\r\n",
                "\r\n",
                "\r\n",
                "          #record loss data\r\n",
                "          if self.debug: print('Predicting and recording loss/acc')\r\n",
                "          Z = self.forward_prop(X)\r\n",
                "          self.loss[i] = self.cross_entropy_error(Z,y)\r\n",
                "          train_pred = self.predict(X)\r\n",
                "          self.accuracy[i]  = accuracy_score(train_pred,y)\r\n",
                "          if self.verbose:\r\n",
                "              print(f'Loss {i}:', self.loss[i])\r\n",
                "              print(f'Acc {i}:', self.accuracy[i])\r\n",
                "              \r\n",
                "      #verbose\r\n",
                "      if self.verbose:\r\n",
                "          print('Final train loss:',self.loss[-1])\r\n",
                "          print('Final train accuracy:',self.accuracy[-1])\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# test run deep neural\r\n",
                "model = DeepNeuralNetworkClassifier(enc, debug = True, verbose = True, max_iter = 5)\r\n",
                "n_in_features = X_train.shape[1]\r\n",
                "batch_size  = 20\r\n",
                "print('train shape: ', X_train.shape)\r\n",
                "print('input features: ', n_in_features)\r\n",
                "print('input channels: ', batch_size)\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "l1 = Conv1D(filter_size = 20, n_input = batch_size, n_output = 1, batch_size = batch_size)\r\n",
                "model.add(l1,Tanh())\r\n",
                "l2 = FC(765,200, SimpleInitializer(),SGD()) #cause output of conv is 394\r\n",
                "model.add(l2,Tanh())\r\n",
                "l3 = FC(200,10, SimpleInitializer(),SGD())\r\n",
                "model.add(l3,SoftMax())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "train shape:  (48000, 784)\n",
                        "input features:  784\n",
                        "input channels:  20\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print('xshape: ', X_val.shape, 'yshape: ', y_val.shape)\r\n",
                "model.fit(X_val, y_val)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "xshape:  (12000, 784) yshape:  (12000, 10)\n",
                        "X shape:  (12000, 784) type:  float64\n",
                        "Batch count:  600\n",
                        "Epoch:  0\n",
                        "Current batch:  0\n",
                        "Z1:  (1, 765) (1, 765)\n",
                        "Z2:  (1, 200) (1, 200)\n",
                        "Z3:  (1, 10) (1, 10)\n",
                        "Backward last Y:  (20, 10)\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "ValueError",
                    "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 1)",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-714-35daef59a9c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xshape: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yshape: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;32m<ipython-input-701-461da7c118a5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     82\u001b[0m               \u001b[1;31m#train mini_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m               \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_X_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_y_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-701-461da7c118a5>\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(self, Z, y)\u001b[0m\n\u001b[0;32m     40\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menum_layer_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#last layer has different activation backward!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m           \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Backward layer: {len(self.layers) - (i)} '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-700-5775c8679fb5>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dA)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_dW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_dB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_dZ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-700-5775c8679fb5>\u001b[0m in \u001b[0;36mcalc_dW\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalc_dW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalc_dZ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 1)"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Problem 7\r\n",
                "Arbitrary number of strides"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## NOTE: added in prototype"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#stride example\r\n",
                "x = np.array(([1,1,1,1,1,1,1,1,1],[2,2,2,2,2,2,2,2,2]))\r\n",
                "w = np.array([[1,1,1],[1,1,1]], dtype = np.float64)\r\n",
                "b = 0\r\n",
                "#in this example, in channel 2 and out channel 1\r\n",
                "def conv_forward(x,w,b,stride):\r\n",
                "    n_in_feature = x.shape[1]\r\n",
                "    filter_size = w.shape[1]\r\n",
                "    n_out_feature = (n_in_feature - filter_size)//stride + 1\r\n",
                "    output = []\r\n",
                "    for i in range(n_out_feature):\r\n",
                "        idx = i * stride\r\n",
                "        output.append((x[:, i : i + filter_size] * w).sum(axis = 1))\r\n",
                "    print('Output before sum: \\n', np.array(output).T)\r\n",
                "    return np.array(output).T.sum(axis = 0) + b\r\n",
                "print('Conv: ', conv_forward(x,w,b,2))\r\n",
                "print('Conv: ', conv_forward(x,w,b,3))\r\n",
                "print('Conv: ', conv_forward(x,w,b,4))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Output before sum: \n",
                        " [[3. 3. 3. 3.]\n",
                        " [6. 6. 6. 6.]]\n",
                        "Conv:  [9. 9. 9. 9.]\n",
                        "Output before sum: \n",
                        " [[3. 3. 3.]\n",
                        " [6. 6. 6.]]\n",
                        "Conv:  [9. 9. 9.]\n",
                        "Output before sum: \n",
                        " [[3. 3.]\n",
                        " [6. 6.]]\n",
                        "Conv:  [9. 9.]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### NOTE: stride backward propagation will be implemented later!"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}