{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 13 - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous scratches, we:\n",
    "- Had to initialize the weights\n",
    "- Needed an epoch loop\n",
    "- Coded the Activation Functions\n",
    "- Decided the learning rate, sizes, number of nodes and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 28.2772, val_loss : 60.7899, acc : 0.750, val_acc : 0.375\n",
      "Epoch 1, loss : 21.6052, val_loss : 19.3948, acc : 0.250, val_acc : 0.625\n",
      "Epoch 2, loss : 1.7351, val_loss : 3.3591, acc : 0.750, val_acc : 0.625\n",
      "Epoch 3, loss : 0.0810, val_loss : 1.7886, acc : 1.000, val_acc : 0.875\n",
      "Epoch 4, loss : 0.0000, val_loss : 0.0150, acc : 1.000, val_acc : 1.000\n",
      "Epoch 5, loss : 0.0000, val_loss : 0.9217, acc : 1.000, val_acc : 0.875\n",
      "Epoch 6, loss : 0.3560, val_loss : 2.8946, acc : 0.750, val_acc : 0.750\n",
      "Epoch 7, loss : 1.8495, val_loss : 3.8037, acc : 0.750, val_acc : 0.812\n",
      "Epoch 8, loss : 0.0003, val_loss : 1.4182, acc : 1.000, val_acc : 0.750\n",
      "Epoch 9, loss : 0.8965, val_loss : 3.5553, acc : 0.750, val_acc : 0.875\n",
      "test_acc : 0.850\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                               \n",
    "logits = example_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that the Weights initialization is done through tf.Variable(tf.random_normal); Adam is used as an Optimizer and the activation function passes through tf.nn.relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use all three types of objective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 23.4042, val_loss : 42.4266, acc : 0.167, val_acc : 0.458\n",
      "Epoch 1, loss : 6.5729, val_loss : 10.8760, acc : 0.833, val_acc : 0.458\n",
      "Epoch 2, loss : 0.0000, val_loss : 1.5306, acc : 1.000, val_acc : 0.792\n",
      "Epoch 3, loss : 0.0000, val_loss : 1.8406, acc : 1.000, val_acc : 0.792\n",
      "Epoch 4, loss : 0.0000, val_loss : 1.9852, acc : 1.000, val_acc : 0.750\n",
      "Epoch 5, loss : 0.6849, val_loss : 9.5980, acc : 0.833, val_acc : 0.667\n",
      "Epoch 6, loss : 0.0000, val_loss : 2.5414, acc : 1.000, val_acc : 0.708\n",
      "Epoch 7, loss : 0.0000, val_loss : 1.5766, acc : 1.000, val_acc : 0.792\n",
      "Epoch 8, loss : 0.0000, val_loss : 0.6769, acc : 1.000, val_acc : 0.833\n",
      "Epoch 9, loss : 0.0018, val_loss : 8.7320, acc : 1.000, val_acc : 0.667\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    \n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                             \n",
    "logits = example_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a model usiing the House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 814900.4375, val_loss : 654065.1250\n",
      "Epoch 1, loss : 185489.9688, val_loss : 79993.6172\n",
      "Epoch 2, loss : 71697.3203, val_loss : 49004.1797\n",
      "Epoch 3, loss : 34417.7656, val_loss : 13739.6787\n",
      "Epoch 4, loss : 24298.0703, val_loss : 16023.9580\n",
      "Epoch 5, loss : 11933.3633, val_loss : 7824.4097\n",
      "Epoch 6, loss : 7919.8516, val_loss : 6407.2456\n",
      "Epoch 7, loss : 9596.2646, val_loss : 11079.0303\n",
      "Epoch 8, loss : 12568.0439, val_loss : 13525.6230\n",
      "Epoch 9, loss : 17604.6465, val_loss : 18546.1738\n",
      "test_mse : 17604.646\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                               \n",
    "logits = example_net(X)\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "y_pred = logits\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        loss_list.append(loss)\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    print(\"test_mse : {:.3f}\".format(loss))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally create a model that classifies the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 1.5963, val_loss : 1.1176, acc : 0.600, val_acc : 0.755\n",
      "Epoch 1, loss : 0.9744, val_loss : 0.5949, acc : 0.600, val_acc : 0.859\n",
      "Epoch 2, loss : 0.4027, val_loss : 0.3571, acc : 0.800, val_acc : 0.905\n",
      "Epoch 3, loss : 0.6947, val_loss : 0.3437, acc : 0.700, val_acc : 0.913\n",
      "Epoch 4, loss : 0.4339, val_loss : 0.3112, acc : 0.900, val_acc : 0.923\n",
      "Epoch 5, loss : 0.5902, val_loss : 0.3151, acc : 0.800, val_acc : 0.927\n",
      "Epoch 6, loss : 0.4283, val_loss : 0.3180, acc : 0.800, val_acc : 0.929\n",
      "Epoch 7, loss : 0.2984, val_loss : 0.3294, acc : 0.900, val_acc : 0.931\n",
      "Epoch 8, loss : 0.2504, val_loss : 0.3268, acc : 0.900, val_acc : 0.933\n",
      "Epoch 9, loss : 0.1827, val_loss : 0.3475, acc : 0.900, val_acc : 0.931\n",
      "test_acc : 0.937\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                               \n",
    "logits = example_net(X)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "max_Y = (tf.argmax(Y, 1))\n",
    "max_Y_pred = tf.argmax(logits, 1)\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
