{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\r\n",
    "\r\n",
    "We will build from scratch a class of convolutional neural networks (CNNs) for 2D, implementing the algorithms using only minimal libraries such as NumPy.\r\n",
    "\r\n",
    "\r\n",
    "We will also create a pooling layer and so on to complete the basic form of the CNN. The name of the class should be Scratch2dCNNClassifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def println(*str):\n",
    "    for i in str:\n",
    "        print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Forward Propagation (problem 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTE\n",
    "- [Reference](http://d2l.ai/chapter_convolutional-neural-networks/channels.html)\n",
    "- Multiple output just means repeating different kernels many times, each gives one output.\n",
    "- The kernel shape should be ($n_{out},n_{in},k_{height},k_{width}$)\n",
    "- Summing all the input channels give one output result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# creating data, weight and bias\n",
    "np.random.seed(0)\n",
    "n_in = 1\n",
    "n_out = 2\n",
    "dim = (4,4)\n",
    "kernel_size = (3,3)\n",
    "print('n_in', n_in)\n",
    "print('n_out', n_out)\n",
    "print('dim',dim)\n",
    "print('kernel_size', kernel_size)\n",
    "\n",
    "# X = np.random.randint(0,10,(n_in, *dim))\n",
    "X = np.array([[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]])\n",
    "println('X',X.shape,  X)\n",
    "\n",
    "dup_needed = True\n",
    "# W = np.random.randint(0,2,(n_out, n_in, *kernel_size)) #init kernel\n",
    "W = np.array([[[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]]],\n",
    "              [[[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]]])\n",
    "B = np.random.randint(0,1, n_out)\n",
    "println('W', W.shape, W)\n",
    "println('B', B)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_in 1\n",
      "n_out 2\n",
      "dim (4, 4)\n",
      "kernel_size (3, 3)\n",
      "X\n",
      "(1, 4, 4)\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]\n",
      "  [13 14 15 16]]]\n",
      "W\n",
      "(2, 1, 3, 3)\n",
      "[[[[ 0.  0.  0.]\n",
      "   [ 0.  1.  0.]\n",
      "   [ 0. -1.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.]\n",
      "   [ 0. -1.  1.]\n",
      "   [ 0.  0.  0.]]]]\n",
      "B\n",
      "[0 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# duplicate X to match n_out\n",
    "if dup_needed:\n",
    "    X = np.vstack([[X]] * n_out)\n",
    "    println('X', X.shape, X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "(2, 1, 4, 4)\n",
      "[[[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]\n",
      "   [ 9 10 11 12]\n",
      "   [13 14 15 16]]]\n",
      "\n",
      "\n",
      " [[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]\n",
      "   [ 9 10 11 12]\n",
      "   [13 14 15 16]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# forward\n",
    "\n",
    "in_x, in_y = dim\n",
    "ker_x, ker_y = kernel_size\n",
    "conv_size_x = in_x - ker_x + 1\n",
    "conv_size_y = in_y - ker_y + 1\n",
    "output_shape = (conv_size_x, conv_size_y)\n",
    "result = np.ones((n_out, *output_shape))\n",
    "print('expected output shape: ', output_shape)\n",
    "print('result shape: ', result.shape)\n",
    "print('Xshape: ', X.shape)\n",
    "for i in range(conv_size_x):\n",
    "    for j in range(conv_size_y):\n",
    "        print('convolving at: ', i,j)\n",
    "        temp_x = X[:,:, i : i + ker_x, j : j + ker_y]\n",
    "        # println('before: ', temp_x, temp_x.shape)\n",
    "        temp = temp_x * W\n",
    "        # print(1,temp)\n",
    "        temp = np.sum(temp, axis = (2,3))\n",
    "        # print(2, temp)\n",
    "        temp = np.sum(temp, axis = 1) \n",
    "        # print(3,temp)\n",
    "        result[:,i,j] = temp + B\n",
    "        print(4,result[:,i,j] )\n",
    "\n",
    "println('Forward', result.shape, result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expected output shape:  (2, 2)\n",
      "result shape:  (2, 2, 2)\n",
      "Xshape:  (2, 1, 4, 4)\n",
      "convolving at:  0 0\n",
      "4 [-4.  1.]\n",
      "convolving at:  0 1\n",
      "4 [-4.  1.]\n",
      "convolving at:  1 0\n",
      "4 [-4.  1.]\n",
      "convolving at:  1 1\n",
      "4 [-4.  1.]\n",
      "Forward\n",
      "(2, 2, 2)\n",
      "[[[-4. -4.]\n",
      "  [-4. -4.]]\n",
      "\n",
      " [[ 1.  1.]\n",
      "  [ 1.  1.]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test backward prop (problem 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTEs\n",
    "- Basic backward prop [Ref](https://www.youtube.com/watch?v=i94OvYb6noo)\n",
    "- Using this property of chain rule, plus some clever tricks, we can find a great way to back propagate on conv neural network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In ultra simplification form\n",
    "The formula for forward propagation is basically (super simpilifield):\n",
    "\n",
    "Given z is one cell of the output propagation, x are the selected cells from input X for one convolution, W is kernel and b is bias, the forward prop formula is as follows (for one output cell):\n",
    "$$ z = y + b $$\n",
    "$$ y = \\sum_ix_i*w_i $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So in backward propagation, given $\\frac{dL}{dZ}$, with dz is one cell of dZ\n",
    "$$\\frac{dz}{dy} = \\frac{dz}{db} = 1 \\Rightarrow \\frac{dL}{db} = \\frac{dL}{dz}\\frac{dz}{db} = \\frac{dL}{dz}$$\n",
    "\n",
    "Continuing, let's simply denote any sum $\\sum$  as all the values that has the contribution of (or related to) a cell xi, So:\n",
    "$$\\frac{dL}{dx_i} = \\sum \\frac{dL}{dz}\\frac{dz}{dy}\\frac{dy}{dx_i}$$\n",
    "$$ = \\sum (\\frac{dL}{dz} *1* w_i) $$ \n",
    "with wi being the weight that was use to multiply with xi to get corresponding y\n",
    "\n",
    "To put this in words, the gradient of some $x_i$ is gradient of each of the output cell that $x_i$ contributed to multiply with the corresponding weight of $x_i$ used in that output cell.\n",
    "\n",
    "So our formula is just basically some correct W multiply with some correct dZ. Just need to select the correct ones for each x. Here's an example of the gradient dX for (3,3) input with (2,2) kernel\n",
    "\n",
    "- **X:**\n",
    "$$\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$$\n",
    "\n",
    "- **dL/dx**, given the formula i described above, y here is actually dy or dz (i write y for simplicity)\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "y1w1 & y1w2 + y2w3 & y3w3 \\\\ \n",
    "y1w3 + y3w1 & y1w4 + y2w3 + y3w2 + y4w1 & y2w4 + y4w2 \\\\ \n",
    "y3w3 & y3w4 + y4w3 & y4w4 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The way to get this output is to use a clever trick: **convolution + padding + transformed kernel**\n",
    "- We rotate the kernel by 180 degrees\n",
    "- And convove it through the padded gradient dZ (or dY)\n",
    "- In the previoud example, padding is one, after convole gives expected result\n",
    "\n",
    "The result blew my mind"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### So how about gradient of weight?\n",
    "\n",
    "Remember the function?\n",
    "\n",
    "$$ y = \\sum_ix_i*w_i $$\n",
    "\n",
    "Given that, the formula is basically the same, just with different selection of x and z and different output shape\n",
    "\n",
    "We, similar to the above tric, will\n",
    "- Use the gradient of output Z as a kernel\n",
    "- Convolve it through the input X\n",
    "- Result is gradient of kernel W ( or K what ever you call it )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# sample dZ\n",
    "dZ = np.array([[[ -4,  -4],\n",
    "                   [ 10,  11]],\n",
    "                  [[  1,  -7],\n",
    "                   [  1, -11]]])\n",
    "print('Grad dZ shape: ', dZ.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Grad dZ shape:  (2, 2, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# let's quickly define a convolve method, copying from the forward method\n",
    "def convolve(X,K, B = None): #NOTE: INPUT FOR THIS METHOD MUST HAVE 4 dimensions (out, in, height, width)\n",
    "    in_x, in_y = X.shape[-2], X.shape[-1]\n",
    "    c_out, c_in, ker_x, ker_y = K.shape\n",
    "    B = B if not B is None else np.zeros(c_out)\n",
    "    x_out = in_x - ker_x + 1\n",
    "    y_out = in_y - ker_y + 1\n",
    "    output_shape = (x_out, y_out)\n",
    "    result = np.ones((c_out, *output_shape))\n",
    "    # print('result shape: ', result.shape)\n",
    "    # print('in,out: ', c_in, c_out)\n",
    "    # print('kshape: ', K.shape, 'Xshape: ', X.shape)\n",
    "    for i in range(x_out):\n",
    "        for j in range(y_out):\n",
    "            temp_x = X[:,:, i : i + ker_x, j : j + ker_y]\n",
    "            temp = temp_x * K\n",
    "            temp = np.sum(temp, axis = (2,3))\n",
    "            temp = np.sum(temp, axis = 1) \n",
    "            result[:,i,j] = temp + B\n",
    "\n",
    "    # println('Final convolve result: ', result, result.shape)\n",
    "    return result\n",
    "\n",
    "def flip180(arr, axes = (-2,-1)):\n",
    "    new_arr = np.rot90(arr,2, axes = axes)\n",
    "    return new_arr\n",
    "def padded(arr, pad_size = 1):\n",
    "    return np.pad(arr, ((0,0),(0,0),(pad_size, pad_size),(pad_size, pad_size)), 'constant')\n",
    "def swap_in_out_channels(arr, ax1 = 0, ax2 = 1):\n",
    "    return np.swapaxes(arr, ax1, ax2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# back prop\n",
    "\n",
    "# remember to flip in/out channel of weights\n",
    "W = swap_in_out_channels(W)\n",
    "\n",
    "# bias gradient\n",
    "dZ_dB = dZ\n",
    "print('dZ_dB: ', dZ_dB, dZ_dB.shape)\n",
    "\n",
    "#z with added dimension\n",
    "dZ = np.stack([dZ]*n_in, axis = 0)\n",
    "# println('Dz: ', dZ, dZ.shape)\n",
    "# x gradient\n",
    "padded_dZ = padded(dZ, pad_size = 2)\n",
    "dZ_dX = convolve(padded_dZ, flip180(W))\n",
    "println('Dz_dX: ', dZ_dX, dZ_dX.shape)\n",
    "# w gradient\n",
    "dZ_dW = convolve(swap_in_out_channels(X), dZ)\n",
    "println('dZ_dW:', dZ_dW, dZ_dW.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dZ_dB:  [[[ -4  -4]\n",
      "  [ 10  11]]\n",
      "\n",
      " [[  1  -7]\n",
      "  [  1 -11]]] (2, 2, 2)\n",
      "Dz_dX: \n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.  -5.   4.  -7.]\n",
      "  [  0.  13.  27. -11.]\n",
      "  [  0. -10. -11.   0.]]]\n",
      "(1, 4, 4)\n",
      "dZ_dW:\n",
      "[[[30. 27. 24.]\n",
      "  [18. 15. 12.]\n",
      "  [ 6.  3.  0.]]]\n",
      "(1, 3, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Suppporting Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma = 0.1):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, dimension = ()):\n",
    "        return rng.normal(0, self.sigma, dimension)\n",
    "    \n",
    "    def B(self,n=0):\n",
    "        return rng.normal(0, self.sigma, (1,n))\n",
    "\n",
    "\n",
    "# Test\n",
    "# simp_init = SimpleInitializer()\n",
    "# w = simp_init.W(dimension = (3,2))\n",
    "# b = simp_init.B(2)\n",
    "\n",
    "# println('W', w)\n",
    "# println('B', b)\n",
    "\n",
    "# x = np.random.randint(5,size = (2,3))\n",
    "# println('X', x)\n",
    "\n",
    "# println('x*w + b', x@w + b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class SGDOptimizer():\n",
    "    def __init__(self, learning_rate = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    def update(self, layer, dW, dB):\n",
    "        layer.W = layer.W - self.learning_rate * dW\n",
    "        layer.B = layer.B - self.learning_rate * dB\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "2D Convolutional Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Fake supporting classes\n",
    "class SimpleInitializer:\n",
    "  def W(*arg): pass\n",
    "  def B(*arg): pass\n",
    "\n",
    "class SGD:\n",
    "  def update(*arg): pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, \n",
    "        initializer = SimpleInitializer(), \n",
    "        optimizer = SGD(), \n",
    "        kernel_size = (3,3), \n",
    "        n_out_channels = 3, \n",
    "        padding = 0, \n",
    "        stride = 1):\n",
    "        #! let's not consider padding and stride at the moment\n",
    "        \n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.n_out = n_out_channels\n",
    "        self.skip_init_weight = False\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        if X.ndim != 3:\n",
    "            raise 'Input must be 3-dimensional (input_channels, height, width)'\n",
    "        \n",
    "        # init size\n",
    "        self.n_in, self.n_row, self.n_col = X.shape\n",
    "\n",
    "        # init weight and biases\n",
    "        if not self.skip_init_weight:\n",
    "            self.W = self.initializer.W(dimension = (self.n_out, self.n_in, *self.kernel_size))\n",
    "            self.B = self.initializer.B(self.n_out)\n",
    "\n",
    "        # add one duplicated dimension to X\n",
    "        self.X = np.stack([X]*self.n_out)\n",
    "\n",
    "        return self.convolve(self.X, self.W, self.B)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        if dZ.ndim != 3:\n",
    "            raise 'Input Gradient must be 3-dimensional (output_channels, height, width)'\n",
    "\n",
    "        # remember to flip in/out channel of weights and X\n",
    "        W = self.swap_in_out_channels(self.W)\n",
    "        X = self.swap_in_out_channels(self.X)\n",
    "        #z with added dimension\n",
    "        dZ = np.stack([dZ]*n_in, axis = 0)\n",
    "        # bias gradient\n",
    "        dZ_dB = dZ\n",
    "        # w gradient\n",
    "        dZ_dW = self.convolve(X, dZ)\n",
    "\n",
    "        self.optimizer.update(self, dZ_dW, dZ_dB)\n",
    "\n",
    "        \n",
    "        #z with pad\n",
    "        pad_x, pad_y = self.kernel_size[0] - 1, self.kernel_size[1] - 1\n",
    "        padded_dZ = self.padded(dZ, pad_x, pad_y)\n",
    "        # x gradient\n",
    "        dZ_dX = self.convolve(padded_dZ, self.flip180(W))\n",
    "        return dZ_dX\n",
    "\n",
    "\n",
    "    def convolve(self, X, W, B = None):\n",
    "    #NOTE: INPUT FOR THIS METHOD MUST HAVE 4 dimensions (out, in, height, width)\n",
    "        in_x, in_y = X.shape[-2], X.shape[-1]\n",
    "        c_out, c_in, ker_x, ker_y = W.shape\n",
    "        B = B if not B is None else np.zeros(c_out)\n",
    "        x_out = in_x - ker_x + 1\n",
    "        y_out = in_y - ker_y + 1\n",
    "        output_shape = (x_out, y_out)\n",
    "        result = np.ones((c_out, *output_shape))\n",
    "        for i in range(x_out):\n",
    "            for j in range(y_out):\n",
    "                temp_x = X[:,:, i : i + ker_x, j : j + ker_y]\n",
    "                temp = temp_x * W\n",
    "                temp = np.sum(temp, axis = (2,3))\n",
    "                temp = np.sum(temp, axis = 1) \n",
    "                result[:,i,j] = temp + B\n",
    "        return result\n",
    "    \n",
    "    #! HELPERS\n",
    "\n",
    "    def flip180(self, arr, axes = (-2,-1)):\n",
    "        new_arr = np.rot90(arr,2, axes = axes)\n",
    "        return new_arr\n",
    "    def padded(self, arr, pad_x = 1, pad_y = 1):\n",
    "        return np.pad(arr, ((0,0),(0,0),(pad_x, pad_x),(pad_y, pad_y)), 'constant')\n",
    "    def swap_in_out_channels(self, arr, ax1 = 0, ax2 = 1):\n",
    "        return np.swapaxes(arr, ax1, ax2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Test conv2D\n",
    "\n",
    "cnn = Conv2d(kernel_size = (3,3), n_out_channels = 2)\n",
    "cnn.skip_init_weight = True\n",
    "cnn.W = W\n",
    "cnn.B = B\n",
    "\n",
    "X = np.array([[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]])\n",
    "cnn.W = np.array([[[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]]],\n",
    "              [[[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]]])\n",
    "cnn.B = np.array([0,0])\n",
    "forward = cnn.forward(X)\n",
    "\n",
    "# sample dZ\n",
    "dZ = np.array([[[ -4,  -4],\n",
    "                   [ 10,  11]],\n",
    "                  [[  1,  -7],\n",
    "                   [  1, -11]]])\n",
    "backward = cnn.backward(dZ)\n",
    "\n",
    "println('Forward (Z): ', forward.shape, forward)\n",
    "println('Backward: (dZ/dX) ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Forward (Z): \n",
      "(2, 2, 2)\n",
      "[[[-4. -4.]\n",
      "  [-4. -4.]]\n",
      "\n",
      " [[ 1.  1.]\n",
      "  [ 1.  1.]]]\n",
      "Backward: (dZ/dX) \n",
      "(1, 4, 4)\n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.  -5.   4.  -7.]\n",
      "  [  0.  13.  27. -11.]\n",
      "  [  0. -10. -11.   0.]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 3\n",
    "Output size after 2-dimensional convolution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def conv_dim(dim, ker, pad, stride):\n",
    "    dim, ker, pad, stride = np.array(dim), np.array(ker), np.array(pad), np.array(stride)\n",
    "    result = (dim + 2 * pad - ker)/stride + 1\n",
    "    return result.astype(np.int64)\n",
    "\n",
    "dim = (4,4)\n",
    "ker = (2,2)\n",
    "pad = (2,2)\n",
    "stride = (2,2)\n",
    "print('Conv dim: ', conv_dim(dim, ker, pad, stride)) \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conv dim:  [4 4]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 4\n",
    "Creation of max pooling layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from numpy import unravel_index\n",
    "class MaxPool2D():\n",
    "    def __init__(self, pool_size = (2,2), padding = (0,0), stride = (1,1)):\n",
    "        self.pool_size = pool_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        if X.ndim != 3: raise 'Invalid dimension, must be 3 (in, height, width)'\n",
    "\n",
    "        self.dim = X.shape[1:]\n",
    "        self.out_x, self.out_y = conv_dim(self.dim, self.pool_size, self.padding, self.stride)\n",
    "\n",
    "        output = []\n",
    "        self.max_indexes_array = []\n",
    "        for X_channel in X:\n",
    "            pool_result, max_indexes = self.pool_channel(X_channel)\n",
    "            output.append(pool_result)\n",
    "            self.max_indexes_array.append(max_indexes)\n",
    "        return np.stack(output, 0)\n",
    "    \n",
    "    def pool_channel(self, X):\n",
    "        output = np.zeros((self.out_x, self.out_y))\n",
    "        x_pool, y_pool = self.pool_size\n",
    "        max_indexes = []\n",
    "        for i in range(self.out_x):\n",
    "            for j in range(self.out_y):\n",
    "                temp_x = X[i : i + x_pool,j : j + y_pool]\n",
    "\n",
    "                kernel_max_index = unravel_index(temp_x.argmax(), temp_x.shape)\n",
    "                max_index = (kernel_max_index[0] + i, kernel_max_index[1] + j)\n",
    "                output[i,j] = X[max_index]\n",
    "                max_indexes.append(max_index)\n",
    "        return output, max_indexes\n",
    "\n",
    "    def backward(self, dZ): \n",
    "        dX = []\n",
    "        for channel, dz in enumerate(dZ):\n",
    "            dx = np.zeros(self.dim)\n",
    "            for i, gradient in enumerate(dz.flatten()):\n",
    "                max_idx = self.max_indexes_array[channel][i]\n",
    "                \n",
    "                dx[max_idx] = gradient\n",
    "            dX.append(dx)\n",
    "        return np.stack(dX, 0) \n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# test max pool\n",
    "X = np.arange(27)\n",
    "X = X.reshape((3,3,3))\n",
    "println('X', X)\n",
    "pool_layer = MaxPool2D()\n",
    "forward = pool_layer.forward(X)\n",
    "println('Pool forward: ', forward.shape, forward)\n",
    "\n",
    "dZ = np.arange(12)\n",
    "dZ = dZ.reshape((3,2,2))\n",
    "println('dZ', dZ)\n",
    "backward = pool_layer.backward(dZ)\n",
    "println('Pool backward: ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n",
      "Pool forward: \n",
      "(3, 2, 2)\n",
      "[[[ 4.  5.]\n",
      "  [ 7.  8.]]\n",
      "\n",
      " [[13. 14.]\n",
      "  [16. 17.]]\n",
      "\n",
      " [[22. 23.]\n",
      "  [25. 26.]]]\n",
      "dZ\n",
      "[[[ 0  1]\n",
      "  [ 2  3]]\n",
      "\n",
      " [[ 4  5]\n",
      "  [ 6  7]]\n",
      "\n",
      " [[ 8  9]\n",
      "  [10 11]]]\n",
      "Pool backward: \n",
      "(3, 3, 3)\n",
      "[[[ 0.  0.  0.]\n",
      "  [ 0.  0.  1.]\n",
      "  [ 0.  2.  3.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  4.  5.]\n",
      "  [ 0.  6.  7.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  8.  9.]\n",
      "  [ 0. 10. 11.]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 5 \n",
    "Average Pooling layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class AveragePool2D():\n",
    "    def __init__(self, pool_size = (2,2), padding = (0,0), stride = (1,1)):\n",
    "        self.pool_size = pool_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        if X.ndim != 3: raise 'Invalid dimension, must be 3 (in, height, width)'\n",
    "\n",
    "        self.dim = X.shape[1:]\n",
    "        out_dim = conv_dim(self.dim, self.pool_size, self.padding, self.stride)\n",
    "        \n",
    "        return self.pool_full(X, out_dim)\n",
    "        \n",
    "\n",
    "\n",
    "    def pool_full(self, X, out_dim):\n",
    "        output = []\n",
    "        for X_channel in X:\n",
    "            pool_result = self.pool_channel(X_channel, out_dim)\n",
    "            output.append(pool_result)\n",
    "        return np.stack(output, 0)\n",
    "    \n",
    "    def pool_channel(self, X, out_dim):\n",
    "        out_x, out_y = out_dim\n",
    "        output = np.zeros((out_x, out_y))\n",
    "        x_pool, y_pool = self.pool_size\n",
    "        for i in range(out_x):\n",
    "            for j in range(out_y):\n",
    "                temp_x = X[i : i + x_pool, j : j + y_pool]\n",
    "                output[i,j] = temp_x.mean()\n",
    "        return output\n",
    "\n",
    "    def backward(self, dZ): \n",
    "        pad_x, pad_y = np.array(self.pool_size) - 1\n",
    "        dZ = self.padded(dZ, pad_x, pad_y)\n",
    "\n",
    "        return self.pool_full(dZ, self.dim)\n",
    "    \n",
    "    def padded(self, arr, pad_x = 1, pad_y = 1):\n",
    "        return np.pad(arr, ((0,0),(pad_x, pad_x),(pad_y, pad_y)), 'constant')\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# test mean pool\n",
    "X = np.arange(27)\n",
    "X = X.reshape((3,3,3))\n",
    "println('X', X)\n",
    "pool_layer = AveragePool2D()\n",
    "forward = pool_layer.forward(X)\n",
    "# println('Pool forward: ', forward.shape, forward)\n",
    "\n",
    "dZ = np.arange(12)\n",
    "dZ = dZ.reshape((3,2,2))\n",
    "println('dZ', dZ)\n",
    "backward = pool_layer.backward(dZ)\n",
    "println('Pool backward: ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n",
      "dZ\n",
      "[[[ 0  1]\n",
      "  [ 2  3]]\n",
      "\n",
      " [[ 4  5]\n",
      "  [ 6  7]]\n",
      "\n",
      " [[ 8  9]\n",
      "  [10 11]]]\n",
      "Pool backward: \n",
      "(3, 3, 3)\n",
      "[[[0.   0.25 0.25]\n",
      "  [0.5  1.5  1.  ]\n",
      "  [0.5  1.25 0.75]]\n",
      "\n",
      " [[1.   2.25 1.25]\n",
      "  [2.5  5.5  3.  ]\n",
      "  [1.5  3.25 1.75]]\n",
      "\n",
      " [[2.   4.25 2.25]\n",
      "  [4.5  9.5  5.  ]\n",
      "  [2.5  5.25 2.75]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 6\n",
    "Smoothing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class Flatten():\n",
    "  def __init__(self): pass\n",
    "  def forward(self, X):\n",
    "    self.input_shape = X.shape\n",
    "    return X.flatten()\n",
    "  def backward(self,dZ):\n",
    "    return dZ.reshape(self.input_shape)\n",
    "\n",
    "# test mean pool\n",
    "X = np.arange(27)\n",
    "X = X.reshape((3,3,3))\n",
    "println('X', X)\n",
    "flatten_layer = Flatten()\n",
    "forward = flatten_layer.forward(X)\n",
    "# println('Pool forward: ', forward.shape, forward)\n",
    "\n",
    "dZ = X\n",
    "println('dZ', dZ)\n",
    "backward = flatten_layer.backward(dZ)\n",
    "println('Pool backward: ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n",
      "dZ\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n",
      "Pool backward: \n",
      "(3, 3, 3)\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 7\n",
    "Leaning and Estimation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supporting Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Import layers from previous tasks\n",
    "\n",
    "from deep_neural import SimpleInitializer\n",
    "from deep_neural import SGD\n",
    "from deep_neural import FC\n",
    "from deep_neural import GetMiniBatch\n",
    "from deep_neural import Sigmoid\n",
    "from deep_neural import SoftMax\n",
    "from deep_neural import Tanh\n",
    "from deep_neural import DeepNeuralNetworkClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#data set\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# #reshape\n",
    "# X_train = X_train.reshape(-1, 784)\n",
    "# X_test = X_test.reshape(-1, 784)\n",
    "#scaling\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "#one hot encode for multiclass labels!\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "#validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, train_size=0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# test run deep neural\n",
    "model = DeepNeuralNetworkClassifier(enc, debug = False, verbose = True, max_iter = 5)\n",
    "n_in_features = X_train.shape[1]\n",
    "batch_size  = 20\n",
    "print('train shape: ', X_train.shape)\n",
    "print('input features: ', n_in_features)\n",
    "print('input channels: ', batch_size)\n",
    "\n",
    "\n",
    "in_channel = X_train.shape[1]\n",
    "\n",
    "l1 = Conv1DBatch(filter_size = 3, n_input = in_channel, n_output = 1, optimizer = AdaGrad(0.1))\n",
    "model.add(l1,Tanh())\n",
    "lshape = FlattenLayer()\n",
    "model.add(lshape,TransparentFunction())\n",
    "l2 = FC(26,100, SimpleInitializer(),SGD()) #cause output of conv is 394\n",
    "model.add(l2,Tanh())\n",
    "l3 = FC(100,10, SimpleInitializer(),SGD())\n",
    "model.add(l3,SoftMax())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X shape:  (30000, 28, 28) type:  float64\n",
      "Batch count:  1500\n",
      "Layer 1:  28 400\n",
      "Activ: 1: Tanh\n",
      "Layer 2:  400 200\n",
      "Activ: 2: Tanh\n",
      "Layer 3:  200 10\n",
      "Activ: 3: SoftMax\n",
      "Epoch:  0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,28,10) (200,1) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d855aab05bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/DiveIntoCode/Week 18/deep_neural.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;31m# X_train /= 255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;31m# X_test /= 255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m \u001b[0;31m# #one hot encode for multiclass labels!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;31m# from sklearn.preprocessing import OneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;31m# enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DiveIntoCode/Week 18/deep_neural.py\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m#prepare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlenx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetMiniBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#for debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DiveIntoCode/Week 18/deep_neural.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_nodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nodes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_nodes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,28,10) (200,1) "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}