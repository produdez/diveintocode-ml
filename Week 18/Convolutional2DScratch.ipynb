{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\r\n",
    "\r\n",
    "We will build from scratch a class of convolutional neural networks (CNNs) for 2D, implementing the algorithms using only minimal libraries such as NumPy.\r\n",
    "\r\n",
    "\r\n",
    "We will also create a pooling layer and so on to complete the basic form of the CNN. The name of the class should be Scratch2dCNNClassifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "def println(*str):\r\n",
    "    for i in str:\r\n",
    "        print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Forward Propagation (problem 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTE\n",
    "- [Reference](http://d2l.ai/chapter_convolutional-neural-networks/channels.html)\n",
    "- Multiple output just means repeating different kernels many times, each gives one output.\n",
    "- The kernel shape should be ($n_{out},n_{in},k_{height},k_{width}$)\n",
    "- Summing all the input channels give one output result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "# creating data, weight and bias\r\n",
    "np.random.seed(0)\r\n",
    "n_in = 1\r\n",
    "n_out = 2\r\n",
    "dim = (4,4)\r\n",
    "kernel_size = (3,3)\r\n",
    "print('n_in', n_in)\r\n",
    "print('n_out', n_out)\r\n",
    "print('dim',dim)\r\n",
    "print('kernel_size', kernel_size)\r\n",
    "\r\n",
    "# X = np.random.randint(0,10,(n_in, *dim))\r\n",
    "X = np.array([[[ 1,  2,  3,  4],\r\n",
    "                [ 5,  6,  7,  8],\r\n",
    "                [ 9, 10, 11, 12],\r\n",
    "                [13, 14, 15, 16]]])\r\n",
    "println('X',X.shape,  X)\r\n",
    "\r\n",
    "dup_needed = True\r\n",
    "# W = np.random.randint(0,2,(n_out, n_in, *kernel_size)) #init kernel\r\n",
    "W = np.array([[[[ 0.,  0.,  0.],\r\n",
    "               [ 0.,  1.,  0.],\r\n",
    "               [ 0., -1.,  0.]]],\r\n",
    "              [[[ 0.,  0.,  0.],\r\n",
    "               [ 0., -1.,  1.],\r\n",
    "               [ 0.,  0.,  0.]]]])\r\n",
    "B = np.random.randint(0,1, n_out)\r\n",
    "println('W', W.shape, W)\r\n",
    "println('B', B)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_in 1\n",
      "n_out 2\n",
      "dim (4, 4)\n",
      "kernel_size (3, 3)\n",
      "X\n",
      "(1, 4, 4)\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]\n",
      "  [13 14 15 16]]]\n",
      "W\n",
      "(2, 1, 3, 3)\n",
      "[[[[ 0.  0.  0.]\n",
      "   [ 0.  1.  0.]\n",
      "   [ 0. -1.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.]\n",
      "   [ 0. -1.  1.]\n",
      "   [ 0.  0.  0.]]]]\n",
      "B\n",
      "[0 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "# duplicate X to match n_out\r\n",
    "if dup_needed:\r\n",
    "    X = np.vstack([[X]] * n_out)\r\n",
    "    println('X', X.shape, X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "(2, 1, 4, 4)\n",
      "[[[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]\n",
      "   [ 9 10 11 12]\n",
      "   [13 14 15 16]]]\n",
      "\n",
      "\n",
      " [[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]\n",
      "   [ 9 10 11 12]\n",
      "   [13 14 15 16]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "# forward\r\n",
    "\r\n",
    "in_x, in_y = dim\r\n",
    "ker_x, ker_y = kernel_size\r\n",
    "conv_size_x = in_x - ker_x + 1\r\n",
    "conv_size_y = in_y - ker_y + 1\r\n",
    "output_shape = (conv_size_x, conv_size_y)\r\n",
    "result = np.ones((n_out, *output_shape))\r\n",
    "print('expected output shape: ', output_shape)\r\n",
    "print('result shape: ', result.shape)\r\n",
    "print('Xshape: ', X.shape)\r\n",
    "for i in range(conv_size_x):\r\n",
    "    for j in range(conv_size_y):\r\n",
    "        print('convolving at: ', i,j)\r\n",
    "        temp_x = X[:,:, i : i + ker_x, j : j + ker_y]\r\n",
    "        # println('before: ', temp_x, temp_x.shape)\r\n",
    "        temp = temp_x * W\r\n",
    "        # print(1,temp)\r\n",
    "        temp = np.sum(temp, axis = (2,3))\r\n",
    "        # print(2, temp)\r\n",
    "        temp = np.sum(temp, axis = 1) \r\n",
    "        # print(3,temp)\r\n",
    "        result[:,i,j] = temp + B\r\n",
    "        print(4,result[:,i,j] )\r\n",
    "\r\n",
    "println('Forward', result.shape, result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expected output shape:  (2, 2)\n",
      "result shape:  (2, 2, 2)\n",
      "Xshape:  (2, 1, 4, 4)\n",
      "convolving at:  0 0\n",
      "4 [-4.  1.]\n",
      "convolving at:  0 1\n",
      "4 [-4.  1.]\n",
      "convolving at:  1 0\n",
      "4 [-4.  1.]\n",
      "convolving at:  1 1\n",
      "4 [-4.  1.]\n",
      "Forward\n",
      "(2, 2, 2)\n",
      "[[[-4. -4.]\n",
      "  [-4. -4.]]\n",
      "\n",
      " [[ 1.  1.]\n",
      "  [ 1.  1.]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test backward prop (problem 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTEs\n",
    "- Basic backward prop [Ref](https://www.youtube.com/watch?v=i94OvYb6noo)\n",
    "- Using this property of chain rule, plus some clever tricks, we can find a great way to back propagate on conv neural network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In ultra simplification form\n",
    "The formula for forward propagation is basically (super simpilifield):\n",
    "\n",
    "Given z is one cell of the output propagation, x are the selected cells from input X for one convolution, W is kernel and b is bias, the forward prop formula is as follows (for one output cell):\n",
    "$$ z = y + b $$\n",
    "$$ y = \\sum_ix_i*w_i $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So in backward propagation, given $\\frac{dL}{dZ}$, with dz is one cell of dZ\n",
    "$$\\frac{dz}{dy} = \\frac{dz}{db} = 1 \\Rightarrow \\frac{dL}{db} = \\frac{dL}{dz}\\frac{dz}{db} = \\frac{dL}{dz}$$\n",
    "\n",
    "Continuing, let's simply denote any sum $\\sum$  as all the values that has the contribution of (or related to) a cell xi, So:\n",
    "$$\\frac{dL}{dx_i} = \\sum \\frac{dL}{dz}\\frac{dz}{dy}\\frac{dy}{dx_i}$$\n",
    "$$ = \\sum (\\frac{dL}{dz} *1* w_i) $$ \n",
    "with wi being the weight that was use to multiply with xi to get corresponding y\n",
    "\n",
    "To put this in words, the gradient of some $x_i$ is gradient of each of the output cell that $x_i$ contributed to multiply with the corresponding weight of $x_i$ used in that output cell.\n",
    "\n",
    "So our formula is just basically some correct W multiply with some correct dZ. Just need to select the correct ones for each x. Here's an example of the gradient dX for (3,3) input with (2,2) kernel\n",
    "\n",
    "- **X:**\n",
    "$$\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$$\n",
    "\n",
    "- **dL/dx**, given the formula i described above, y here is actually dy or dz (i write y for simplicity)\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "y1w1 & y1w2 + y2w3 & y3w3 \\\\ \n",
    "y1w3 + y3w1 & y1w4 + y2w3 + y3w2 + y4w1 & y2w4 + y4w2 \\\\ \n",
    "y3w3 & y3w4 + y4w3 & y4w4 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The way to get this output is to use a clever trick: **convolution + padding + transformed kernel**\n",
    "- We rotate the kernel by 180 degrees\n",
    "- And convove it through the padded gradient dZ (or dY)\n",
    "- In the previoud example, padding is one, after convole gives expected result\n",
    "\n",
    "The result blew my mind"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### So how about gradient of weight?\n",
    "\n",
    "Remember the function?\n",
    "\n",
    "$$ y = \\sum_ix_i*w_i $$\n",
    "\n",
    "Given that, the formula is basically the same, just with different selection of x and z and different output shape\n",
    "\n",
    "We, similar to the above tric, will\n",
    "- Use the gradient of output Z as a kernel\n",
    "- Convolve it through the input X\n",
    "- Result is gradient of kernel W ( or K what ever you call it )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "# sample dZ\r\n",
    "dZ = np.array([[[ -4,  -4],\r\n",
    "                   [ 10,  11]],\r\n",
    "                  [[  1,  -7],\r\n",
    "                   [  1, -11]]])\r\n",
    "print('Grad dZ shape: ', dZ.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Grad dZ shape:  (2, 2, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "# let's quickly define a convolve method, copying from the forward method\r\n",
    "def convolve(X,K, B = None): #NOTE: INPUT FOR THIS METHOD MUST HAVE 4 dimensions (out, in, height, width)\r\n",
    "    in_x, in_y = X.shape[-2], X.shape[-1]\r\n",
    "    c_out, c_in, ker_x, ker_y = K.shape\r\n",
    "    B = B if not B is None else np.zeros(c_out)\r\n",
    "    x_out = in_x - ker_x + 1\r\n",
    "    y_out = in_y - ker_y + 1\r\n",
    "    output_shape = (x_out, y_out)\r\n",
    "    result = np.ones((c_out, *output_shape))\r\n",
    "    # print('result shape: ', result.shape)\r\n",
    "    # print('in,out: ', c_in, c_out)\r\n",
    "    # print('kshape: ', K.shape, 'Xshape: ', X.shape)\r\n",
    "    for i in range(x_out):\r\n",
    "        for j in range(y_out):\r\n",
    "            temp_x = X[:,:, i : i + ker_x, j : j + ker_y]\r\n",
    "            temp = temp_x * K\r\n",
    "            temp = np.sum(temp, axis = (2,3))\r\n",
    "            temp = np.sum(temp, axis = 1) \r\n",
    "            result[:,i,j] = temp + B\r\n",
    "\r\n",
    "    # println('Final convolve result: ', result, result.shape)\r\n",
    "    return result\r\n",
    "\r\n",
    "def flip180(arr, axes = (-2,-1)):\r\n",
    "    new_arr = np.rot90(arr,2, axes = axes)\r\n",
    "    return new_arr\r\n",
    "def padded(arr, pad_size = 1):\r\n",
    "    return np.pad(arr, ((0,0),(0,0),(pad_size, pad_size),(pad_size, pad_size)), 'constant')\r\n",
    "def swap_in_out_channels(arr, ax1 = 0, ax2 = 1):\r\n",
    "    return np.swapaxes(arr, ax1, ax2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "# back prop\r\n",
    "\r\n",
    "# remember to flip in/out channel of weights\r\n",
    "W = swap_in_out_channels(W)\r\n",
    "\r\n",
    "# bias gradient\r\n",
    "dZ_dB = dZ\r\n",
    "print('dZ_dB: ', dZ_dB, dZ_dB.shape)\r\n",
    "\r\n",
    "#z with added dimension\r\n",
    "dZ = np.stack([dZ]*n_in, axis = 0)\r\n",
    "# println('Dz: ', dZ, dZ.shape)\r\n",
    "# x gradient\r\n",
    "padded_dZ = padded(dZ, pad_size = 2)\r\n",
    "dZ_dX = convolve(padded_dZ, flip180(W))\r\n",
    "println('Dz_dX: ', dZ_dX, dZ_dX.shape)\r\n",
    "# w gradient\r\n",
    "dZ_dW = convolve(swap_in_out_channels(X), dZ)\r\n",
    "println('dZ_dW:', dZ_dW, dZ_dW.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dZ_dB:  [[[ -4  -4]\n",
      "  [ 10  11]]\n",
      "\n",
      " [[  1  -7]\n",
      "  [  1 -11]]] (2, 2, 2)\n",
      "Dz_dX: \n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.  -5.   4.  -7.]\n",
      "  [  0.  13.  27. -11.]\n",
      "  [  0. -10. -11.   0.]]]\n",
      "(1, 4, 4)\n",
      "dZ_dW:\n",
      "[[[30. 27. 24.]\n",
      "  [18. 15. 12.]\n",
      "  [ 6.  3.  0.]]]\n",
      "(1, 3, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Suppporting Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "from numpy.random import default_rng\r\n",
    "rng = default_rng()\r\n",
    "\r\n",
    "class SimpleInitializer:\r\n",
    "    def __init__(self, sigma = 0.1):\r\n",
    "        self.sigma = sigma\r\n",
    "    \r\n",
    "    def W(self, dimension = ()):\r\n",
    "        return rng.normal(0, self.sigma, dimension)\r\n",
    "    \r\n",
    "    def B(self,n=0):\r\n",
    "        return rng.normal(0, self.sigma, n)\r\n",
    "\r\n",
    "\r\n",
    "# Test\r\n",
    "# simp_init = SimpleInitializer()\r\n",
    "# w = simp_init.W(dimension = (3,2))\r\n",
    "# b = simp_init.B(2)\r\n",
    "\r\n",
    "# println('W', w)\r\n",
    "# println('B', b)\r\n",
    "\r\n",
    "# x = np.random.randint(5,size = (2,3))\r\n",
    "# println('X', x)\r\n",
    "\r\n",
    "# println('x*w + b', x@w + b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "class SGD():\r\n",
    "    def __init__(self, learning_rate = 0.01):\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "    def update(self, layer, dW, dB):\r\n",
    "        layer.W = layer.W - self.learning_rate * dW\r\n",
    "        layer.B = layer.B - self.learning_rate * dB\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "2D Convolutional Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONV2D"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "class Conv2D:\r\n",
    "    def __init__(self, \r\n",
    "        initializer = SimpleInitializer(), \r\n",
    "        optimizer = SGD(), \r\n",
    "        kernel_size = (3,3), \r\n",
    "        n_out_channels = 3, \r\n",
    "        padding = 0, \r\n",
    "        stride = 1):\r\n",
    "        #! let's not consider padding and stride at the moment\r\n",
    "        \r\n",
    "        self.initializer = initializer\r\n",
    "        self.optimizer = optimizer\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        self.padding = padding\r\n",
    "        self.stride = stride\r\n",
    "        self.n_out = n_out_channels\r\n",
    "        self.skip_init_weight = False\r\n",
    "\r\n",
    "    def forward(self,X):\r\n",
    "        result = []\r\n",
    "        for sample in X:\r\n",
    "            result.append(self._forward(sample))\r\n",
    "        return np.stack(result,0)\r\n",
    "\r\n",
    "    def backward(self,dZ):\r\n",
    "        result = []\r\n",
    "        for sample in dZ:\r\n",
    "            result.append(self._backward(sample))\r\n",
    "        return np.stack(result,0)\r\n",
    "\r\n",
    "    def _forward(self, X):\r\n",
    "        if X.ndim != 3:\r\n",
    "            raise 'Input must be 3-dimensional (input_channels, height, width)'\r\n",
    "        \r\n",
    "        # init size\r\n",
    "        self.n_in, self.n_row, self.n_col = X.shape\r\n",
    "\r\n",
    "        # init weight and biases\r\n",
    "        if not self.skip_init_weight:\r\n",
    "            self.W = self.initializer.W(dimension = (self.n_out, self.n_in, *self.kernel_size))\r\n",
    "            self.B = self.initializer.B(self.n_out)\r\n",
    "            self.skip_init_weight = True\r\n",
    "\r\n",
    "        # add one duplicated dimension to X\r\n",
    "        self.X = np.stack([X]*self.n_out)\r\n",
    "\r\n",
    "        return self.convolve(self.X, self.W, self.B)\r\n",
    "\r\n",
    "    def _backward(self, dZ):\r\n",
    "        if dZ.ndim != 3:\r\n",
    "            raise 'Input Gradient must be 3-dimensional (output_channels, height, width)'\r\n",
    "\r\n",
    "        # remember to flip in/out channel of weights and X\r\n",
    "        W = self.swap_in_out_channels(self.W)\r\n",
    "        X = self.swap_in_out_channels(self.X)\r\n",
    "        # bias gradient\r\n",
    "        dZ_dB = dZ.sum(axis = (1,2))\r\n",
    "        #z with added dimension\r\n",
    "        dZ = np.stack([dZ]*self.n_in, axis = 0)\r\n",
    "        # w gradient\r\n",
    "        dZ_dW = self.convolve(X, dZ)\r\n",
    "\r\n",
    "        self.optimizer.update(self, dZ_dW, dZ_dB)\r\n",
    "\r\n",
    "        \r\n",
    "        #z with pad\r\n",
    "        pad_x, pad_y = self.dz_padsize()\r\n",
    "        padded_dZ = self.padded(dZ, pad_x, pad_y)\r\n",
    "        # x gradient\r\n",
    "        dZ_dX = self.convolve(padded_dZ, self.flip180(W))\r\n",
    "        return dZ_dX\r\n",
    "\r\n",
    "\r\n",
    "    def convolve(self, X, W, B = None):\r\n",
    "    #NOTE: INPUT FOR THIS METHOD MUST HAVE 4 dimensions (out, in, height, width)\r\n",
    "        in_x, in_y = X.shape[-2], X.shape[-1]\r\n",
    "        c_out, c_in, ker_x, ker_y = W.shape\r\n",
    "        B = B if not B is None else np.zeros(c_out)\r\n",
    "        x_out = in_x - ker_x + 1\r\n",
    "        y_out = in_y - ker_y + 1\r\n",
    "        output_shape = (x_out, y_out)\r\n",
    "        result = np.ones((c_out, *output_shape))\r\n",
    "        for i in range(x_out):\r\n",
    "            for j in range(y_out):\r\n",
    "                temp_x = X[:,:, i : i + ker_x, j : j + ker_y]\r\n",
    "                temp = temp_x * W\r\n",
    "                temp = np.sum(temp, axis = (2,3))\r\n",
    "                temp = np.sum(temp, axis = 1) \r\n",
    "                result[:,i,j] = temp + B\r\n",
    "        return result\r\n",
    "    \r\n",
    "    #! HELPERS\r\n",
    "\r\n",
    "    def flip180(self, arr, axes = (-2,-1)):\r\n",
    "        new_arr = np.rot90(arr,2, axes = axes)\r\n",
    "        return new_arr\r\n",
    "    def padded(self, arr, pad_x = 1, pad_y = 1):\r\n",
    "        return np.pad(arr, ((0,0),(0,0),(pad_x, pad_x),(pad_y, pad_y)), 'constant')\r\n",
    "    def swap_in_out_channels(self, arr, ax1 = 0, ax2 = 1):\r\n",
    "        return np.swapaxes(arr, ax1, ax2)\r\n",
    "    def dz_padsize(self):\r\n",
    "        return self.kernel_size[0] - 1, self.kernel_size[1] - 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "# Test conv2D\r\n",
    "\r\n",
    "cnn = Conv2D(kernel_size = (3,3), n_out_channels = 2)\r\n",
    "cnn.skip_init_weight = True\r\n",
    "cnn.W = W\r\n",
    "cnn.B = B\r\n",
    "\r\n",
    "X = np.array([[[[ 1,  2,  3,  4],\r\n",
    "                [ 5,  6,  7,  8],\r\n",
    "                [ 9, 10, 11, 12],\r\n",
    "                [13, 14, 15, 16]]]])\r\n",
    "cnn.W = np.array([[[[ 0.,  0.,  0.],\r\n",
    "               [ 0.,  1.,  0.],\r\n",
    "               [ 0., -1.,  0.]]],\r\n",
    "              [[[ 0.,  0.,  0.],\r\n",
    "               [ 0., -1.,  1.],\r\n",
    "               [ 0.,  0.,  0.]]]])\r\n",
    "cnn.B = np.array([0,0])\r\n",
    "forward = cnn.forward(X)\r\n",
    "print(cnn.n_out, cnn.n_in)\r\n",
    "\r\n",
    "# sample dZ\r\n",
    "dZ = np.array([[[[ -4,  -4],\r\n",
    "                   [ 10,  11]],\r\n",
    "                  [[  1,  -7],\r\n",
    "                   [  1, -11]]]])\r\n",
    "backward = cnn.backward(dZ)\r\n",
    "\r\n",
    "println('Forward (Z): ', forward.shape, forward)\r\n",
    "println('Backward: (dZ/dX) ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 1\n",
      "Forward (Z): \n",
      "(1, 2, 2, 2)\n",
      "[[[[-4. -4.]\n",
      "   [-4. -4.]]\n",
      "\n",
      "  [[ 1.  1.]\n",
      "   [ 1.  1.]]]]\n",
      "Backward: (dZ/dX) \n",
      "(1, 1, 4, 4)\n",
      "[[[[  0.   0.   0.   0.]\n",
      "   [  0.  -5.   4.  -7.]\n",
      "   [  0.  13.  27. -11.]\n",
      "   [  0. -10. -11.   0.]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 3\n",
    "Output size after 2-dimensional convolution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "def conv_dim(dim, ker, pad, stride):\r\n",
    "    dim, ker, pad, stride = np.array(dim), np.array(ker), np.array(pad), np.array(stride)\r\n",
    "    result = (dim + 2 * pad - ker)/stride + 1\r\n",
    "    return result.astype(np.int64)\r\n",
    "\r\n",
    "dim = (4,4)\r\n",
    "ker = (2,2)\r\n",
    "pad = (2,2)\r\n",
    "stride = (2,2)\r\n",
    "print('Conv dim: ', conv_dim(dim, ker, pad, stride)) \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conv dim:  [4 4]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 4\n",
    "Creation of max pooling layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "from numpy import unravel_index\r\n",
    "class MaxPool2D():\r\n",
    "    def __init__(self, pool_size = (2,2), padding = (0,0), stride = (1,1)):\r\n",
    "        self.pool_size = pool_size\r\n",
    "        self.padding = padding\r\n",
    "        self.stride = stride\r\n",
    "\r\n",
    "    def forward(self,X):\r\n",
    "        result = []\r\n",
    "        for sample in X:\r\n",
    "            result.append(self._forward(sample))\r\n",
    "        return np.stack(result,0)\r\n",
    "\r\n",
    "    def backward(self,dZ):\r\n",
    "        result = []\r\n",
    "        for sample in dZ:\r\n",
    "            result.append(self._backward(sample))\r\n",
    "        return np.stack(result,0)\r\n",
    "\r\n",
    "    def _forward(self, X):\r\n",
    "        if X.ndim != 3: raise 'Invalid dimension, must be 3 (in, height, width)'\r\n",
    "\r\n",
    "        self.dim = X.shape[1:]\r\n",
    "        self.out_x, self.out_y = conv_dim(self.dim, self.pool_size, self.padding, self.stride)\r\n",
    "\r\n",
    "        output = []\r\n",
    "        self.max_indexes_array = []\r\n",
    "        for X_channel in X:\r\n",
    "            pool_result, max_indexes = self.pool_channel(X_channel)\r\n",
    "            output.append(pool_result)\r\n",
    "            self.max_indexes_array.append(max_indexes)\r\n",
    "        return np.stack(output, 0)\r\n",
    "    \r\n",
    "    def pool_channel(self, X):\r\n",
    "        output = np.zeros((self.out_x, self.out_y))\r\n",
    "        x_pool, y_pool = self.pool_size\r\n",
    "        max_indexes = []\r\n",
    "        for i in range(self.out_x):\r\n",
    "            for j in range(self.out_y):\r\n",
    "                temp_x = X[i : i + x_pool,j : j + y_pool]\r\n",
    "\r\n",
    "                kernel_max_index = unravel_index(temp_x.argmax(), temp_x.shape)\r\n",
    "                max_index = (kernel_max_index[0] + i, kernel_max_index[1] + j)\r\n",
    "                output[i,j] = X[max_index]\r\n",
    "                max_indexes.append(max_index)\r\n",
    "        return output, max_indexes\r\n",
    "\r\n",
    "    def _backward(self, dZ): \r\n",
    "        dX = []\r\n",
    "        for channel, dz in enumerate(dZ):\r\n",
    "            dx = np.zeros(self.dim)\r\n",
    "            for i, gradient in enumerate(dz.flatten()):\r\n",
    "                max_idx = self.max_indexes_array[channel][i]\r\n",
    "                \r\n",
    "                dx[max_idx] = gradient\r\n",
    "            dX.append(dx)\r\n",
    "        return np.stack(dX, 0) \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "# test max pool\r\n",
    "X = np.arange(27)\r\n",
    "X = X.reshape((1,3,3,3))\r\n",
    "println('X', X)\r\n",
    "pool_layer = MaxPool2D()\r\n",
    "forward = pool_layer.forward(X)\r\n",
    "println('Pool forward: ', forward.shape, forward)\r\n",
    "\r\n",
    "dZ = np.arange(12)\r\n",
    "dZ = dZ.reshape((1,3,2,2))\r\n",
    "println('dZ', dZ)\r\n",
    "backward = pool_layer.backward(dZ)\r\n",
    "println('Pool backward: ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]\n",
      "\n",
      "  [[18 19 20]\n",
      "   [21 22 23]\n",
      "   [24 25 26]]]]\n",
      "Pool forward: \n",
      "(1, 3, 2, 2)\n",
      "[[[[ 4.  5.]\n",
      "   [ 7.  8.]]\n",
      "\n",
      "  [[13. 14.]\n",
      "   [16. 17.]]\n",
      "\n",
      "  [[22. 23.]\n",
      "   [25. 26.]]]]\n",
      "dZ\n",
      "[[[[ 0  1]\n",
      "   [ 2  3]]\n",
      "\n",
      "  [[ 4  5]\n",
      "   [ 6  7]]\n",
      "\n",
      "  [[ 8  9]\n",
      "   [10 11]]]]\n",
      "Pool backward: \n",
      "(1, 3, 3, 3)\n",
      "[[[[ 0.  0.  0.]\n",
      "   [ 0.  0.  1.]\n",
      "   [ 0.  2.  3.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  4.  5.]\n",
      "   [ 0.  6.  7.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  8.  9.]\n",
      "   [ 0. 10. 11.]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 5 \n",
    "Average Pooling layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "class AveragePool2D():\r\n",
    "    def __init__(self, pool_size = (2,2), padding = (0,0), stride = (1,1)):\r\n",
    "        self.pool_size = pool_size\r\n",
    "        self.padding = padding\r\n",
    "        self.stride = stride\r\n",
    "\r\n",
    "    def _forward(self, X):\r\n",
    "        if X.ndim != 3: raise 'Invalid dimension, must be 3 (in, height, width)'\r\n",
    "\r\n",
    "        self.dim = X.shape[1:]\r\n",
    "        out_dim = conv_dim(self.dim, self.pool_size, self.padding, self.stride)\r\n",
    "        \r\n",
    "        return self.pool_full(X, out_dim)\r\n",
    "        \r\n",
    "    def forward(self,X):\r\n",
    "        result = []\r\n",
    "        for sample in X:\r\n",
    "            result.append(self._forward(sample))\r\n",
    "        return np.stack(result,0)\r\n",
    "\r\n",
    "    def backward(self,dZ):\r\n",
    "        result = []\r\n",
    "        for sample in dZ:\r\n",
    "            result.append(self._backward(sample))\r\n",
    "        return np.stack(result,0)\r\n",
    "\r\n",
    "    def pool_full(self, X, out_dim):\r\n",
    "        output = []\r\n",
    "        for X_channel in X:\r\n",
    "            pool_result = self.pool_channel(X_channel, out_dim)\r\n",
    "            output.append(pool_result)\r\n",
    "        return np.stack(output, 0)\r\n",
    "    \r\n",
    "    def pool_channel(self, X, out_dim):\r\n",
    "        out_x, out_y = out_dim\r\n",
    "        output = np.zeros((out_x, out_y))\r\n",
    "        x_pool, y_pool = self.pool_size\r\n",
    "        for i in range(out_x):\r\n",
    "            for j in range(out_y):\r\n",
    "                temp_x = X[i : i + x_pool, j : j + y_pool]\r\n",
    "                output[i,j] = temp_x.mean()\r\n",
    "        return output\r\n",
    "\r\n",
    "    def _backward(self, dZ): \r\n",
    "        pad_x, pad_y = np.array(self.pool_size) - 1\r\n",
    "        dZ = self.padded(dZ, pad_x, pad_y)\r\n",
    "\r\n",
    "        return self.pool_full(dZ, self.dim)\r\n",
    "    \r\n",
    "    def padded(self, arr, pad_x = 1, pad_y = 1):\r\n",
    "        return np.pad(arr, ((0,0),(pad_x, pad_x),(pad_y, pad_y)), 'constant')\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "# test mean pool\r\n",
    "X = np.arange(27)\r\n",
    "X = X.reshape((1,3,3,3))\r\n",
    "println('X', X)\r\n",
    "pool_layer = AveragePool2D()\r\n",
    "forward = pool_layer.forward(X)\r\n",
    "# println('Pool forward: ', forward.shape, forward)\r\n",
    "\r\n",
    "dZ = np.arange(12)\r\n",
    "dZ = dZ.reshape((1,3,2,2))\r\n",
    "println('dZ', dZ)\r\n",
    "backward = pool_layer.backward(dZ)\r\n",
    "println('Pool backward: ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n",
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]\n",
      "\n",
      "  [[18 19 20]\n",
      "   [21 22 23]\n",
      "   [24 25 26]]]]\n",
      "dZ\n",
      "[[[[ 0  1]\n",
      "   [ 2  3]]\n",
      "\n",
      "  [[ 4  5]\n",
      "   [ 6  7]]\n",
      "\n",
      "  [[ 8  9]\n",
      "   [10 11]]]]\n",
      "Pool backward: \n",
      "(1, 3, 3, 3)\n",
      "[[[[0.   0.25 0.25]\n",
      "   [0.5  1.5  1.  ]\n",
      "   [0.5  1.25 0.75]]\n",
      "\n",
      "  [[1.   2.25 1.25]\n",
      "   [2.5  5.5  3.  ]\n",
      "   [1.5  3.25 1.75]]\n",
      "\n",
      "  [[2.   4.25 2.25]\n",
      "   [4.5  9.5  5.  ]\n",
      "   [2.5  5.25 2.75]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 6\r\n",
    "Smoothing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "class FlattenLayer():\r\n",
    "  def __init__(self): pass\r\n",
    "  def forward(self, X):\r\n",
    "    self.input_shape = X.shape\r\n",
    "    return X.reshape(X.shape[0],-1) #flatten the input dimension, keep the channels\r\n",
    "\r\n",
    "  def backward(self,dZ):\r\n",
    "    return dZ.reshape(self.input_shape)\r\n",
    "\r\n",
    "# test mean pool\r\n",
    "X = np.arange(27)\r\n",
    "X = X.reshape((3,1,3,3))\r\n",
    "# println('X', X)\r\n",
    "flatten_layer = FlattenLayer()\r\n",
    "forward = flatten_layer.forward(X)\r\n",
    "println('Pool forward: ', forward.shape, forward)\r\n",
    "# println('Pool forward: ', forward.shape, forward)\r\n",
    "\r\n",
    "dZ = X\r\n",
    "# println('dZ', dZ)\r\n",
    "backward = flatten_layer.backward(dZ)\r\n",
    "println('Pool backward: ', backward.shape, backward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pool forward: \n",
      "(3, 9)\n",
      "[[ 0  1  2  3  4  5  6  7  8]\n",
      " [ 9 10 11 12 13 14 15 16 17]\n",
      " [18 19 20 21 22 23 24 25 26]]\n",
      "Pool backward: \n",
      "(3, 1, 3, 3)\n",
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]]\n",
      "\n",
      "\n",
      " [[[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]]\n",
      "\n",
      "\n",
      " [[[18 19 20]\n",
      "   [21 22 23]\n",
      "   [24 25 26]]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 7\n",
    "Leaning and Estimation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Redefining other needed classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "#! activations..............\r\n",
    "\r\n",
    "class ActivationFunction():\r\n",
    "  def forward(self,A):\r\n",
    "    pass\r\n",
    "  def backward(self,dZ):\r\n",
    "    pass\r\n",
    "class Tanh(ActivationFunction):\r\n",
    "  def forward(self,A):\r\n",
    "    self.A = A\r\n",
    "    Z = (np.exp(A) - np.exp(-A)) / (np.exp(A) + np.exp(-A))\r\n",
    "    return Z\r\n",
    "  def backward(self,dZ):\r\n",
    "    A = self.A\r\n",
    "    dA = dZ * (1 - np.tanh(A) ** 2)\r\n",
    "    return dA\r\n",
    "  \r\n",
    "class SoftMax(ActivationFunction):\r\n",
    "  def forward(self,A):\r\n",
    "    self.A = A\r\n",
    "    Z = np.exp(A) / np.sum(np.exp(A), axis = 1).reshape(-1,1)\r\n",
    "    return Z\r\n",
    "  def backward(self,Z,Y):\r\n",
    "    A = self.A\r\n",
    "    nb = Z.shape[0]\r\n",
    "    dA = 1/nb * (Z - Y)\r\n",
    "    return dA\r\n",
    "class ReLU(ActivationFunction):\r\n",
    "  def forward(self,A):\r\n",
    "    self.A = A\r\n",
    "    relu = A * np.where(A > 0, 1,0)\r\n",
    "    return relu\r\n",
    "  def backward(self,dZ):\r\n",
    "    df = np.where(self.A > 0, 1,0)\r\n",
    "    dA = dZ * df\r\n",
    "    return dA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "class FC:\r\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer = SimpleInitializer(), optimizer = SGD()):\r\n",
    "        # Initialize\r\n",
    "        self.optimizer = optimizer\r\n",
    "        self.n_nodes1, self.n_nodes2 = n_nodes1, n_nodes2\r\n",
    "        # Initialize self.W and self.B using the initializer method\r\n",
    "        self.W = initializer.W((n_nodes1, n_nodes2))\r\n",
    "        self.B = initializer.B(n_nodes2)\r\n",
    "        pass\r\n",
    "    def forward(self, X):   \r\n",
    "        self.X = X\r\n",
    "        A = X @ self.W + self.B\r\n",
    "        return A\r\n",
    "    def backward(self, dA):\r\n",
    "        # update\r\n",
    "        self.dA = dA\r\n",
    "        dW = self.calc_dW()\r\n",
    "        dB = self.calc_dB()\r\n",
    "        self.dZ = self.calc_dZ()\r\n",
    "\r\n",
    "        self.optimizer.update(self, dW, dB)\r\n",
    "        return self.dZ\r\n",
    "        # return self.dZ\r\n",
    "    def calc_dB(self):\r\n",
    "        dB = self.dA.sum(axis = 0).reshape(1,-1)  \r\n",
    "        return dB\r\n",
    "    def calc_dW(self):\r\n",
    "        dW = self.X.T @ self.dA\r\n",
    "        return dW\r\n",
    "    def calc_dZ(self):\r\n",
    "        dZ = self.dA @ self.W.T\r\n",
    "        return dZ\r\n",
    "\r\n",
    "class Conv1DBatch:\r\n",
    "  def __init__(self, filter_size = 3, n_input = 2, n_output = 3, optimizer = SGD(), initializer = SimpleInitializer(), padding = 0):\r\n",
    "    # Initialize\r\n",
    "    self.optimizer = optimizer\r\n",
    "    self.filter_size = filter_size\r\n",
    "    # Initialize self.W and self.B using the initializer method\r\n",
    "    self.filter_size = filter_size\r\n",
    "    self.n_input = n_input\r\n",
    "    self.n_output = n_output\r\n",
    "\r\n",
    "    if initializer == None:\r\n",
    "      self.W = np.ones((n_output, n_input,filter_size))\r\n",
    "      self.B = np.ones(n_output)\r\n",
    "    else:\r\n",
    "      self.W = initializer.W((n_output, n_input, filter_size))\r\n",
    "      self.B = initializer.B(n_output)\r\n",
    "    #padding and stride\r\n",
    "    self.padding = padding\r\n",
    "        \r\n",
    "  def forward(self,X): #! NOTE: X must be 3 dimensional (batch_size, channel_count, feature_count)\r\n",
    "    self.batch_size = len(X)\r\n",
    "    X = np.pad(X,[(0,0),(0,0),(self.padding, self.padding)], mode = 'constant')\r\n",
    "    self.X = X\r\n",
    "    result = []\r\n",
    "    for x in self.X:\r\n",
    "      result.append(self._forward_sample(x))\r\n",
    "    # print('forward: ', np.array(result).shape)\r\n",
    "    return np.array(result)\r\n",
    "  def _forward_sample(self,X):    \r\n",
    "    output = []\r\n",
    "    for i in range(self.n_output):\r\n",
    "      filt = self.W[i]\r\n",
    "      bias = self.B[i]\r\n",
    "      conv = self.convolve(filt, X)\r\n",
    "      output.append(conv.sum(axis = 0) + bias)\r\n",
    "    return np.array(output)\r\n",
    "\r\n",
    "  def backward(self, dA):\r\n",
    "    # update\r\n",
    "    self.dA = dA\r\n",
    "    self.dX = []\r\n",
    "\r\n",
    "    for i in range(self.batch_size):\r\n",
    "      dW = self.calc_dW(i)\r\n",
    "      dB = self.calc_dB(i)\r\n",
    "      self.dx = self.calc_dx(i)\r\n",
    "      self.optimizer.update(self, dW, dB)\r\n",
    "      self.dX.append(self.dx) #! keep updating while calculating error for prev layer\r\n",
    "      # print('dx: ', i, self.dx.shape,self.dx)\r\n",
    "    self.dX = np.array(self.dX)\r\n",
    "    return self.dX\r\n",
    "    \r\n",
    "\r\n",
    "  def calc_dW(self,sample_index):\r\n",
    "    dW = []\r\n",
    "    dupped_dA = np.repeat(self.dA[sample_index][:,np.newaxis, : ], self.n_input, axis=1)\r\n",
    "    for i in range(self.n_output): #convolve each output_channel through X\r\n",
    "      conv = self.convolve(dupped_dA[i], self.X[sample_index])\r\n",
    "      dW.append(conv) \r\n",
    "    return np.array(dW)\r\n",
    "  def calc_dB(self,sample_index):\r\n",
    "    return np.array(self.dA[sample_index].sum(axis = 1))\r\n",
    "\r\n",
    "  def calc_dx(self, sample_index): #! careful\r\n",
    "    pad_dA = self.pad_dA(sample_index) #match a with x\r\n",
    "    flipped_W = np.flip(self.W,axis = 2).reshape(self.n_input,self.n_output, -1) # flip each filter and the in_out dim also\r\n",
    "    output = []\r\n",
    "    for i in range(self.n_input):\r\n",
    "      filt = flipped_W[i]\r\n",
    "      conv = self.convolve(filt, pad_dA)\r\n",
    "      output.append(conv.sum(axis = 0))\r\n",
    "    return np.array(output)\r\n",
    "  def pad_dA(self,sample_index):\r\n",
    "    array = self.dA[sample_index]\r\n",
    "    n_features_in = array.shape[1]\r\n",
    "    n_features_out = self.X.shape[-1]\r\n",
    "    filter_size = self.filter_size\r\n",
    "    padding = (n_features_out - 1 + filter_size - n_features_in) // 2\r\n",
    "    return self.pad(array, padding)\r\n",
    "  def pad(self,array,padding):\r\n",
    "    return np.pad(array,[(0,0),(padding, padding)], mode = 'constant')\r\n",
    "  def convolve(self,F,X):\r\n",
    "    A = []\r\n",
    "    filter_size = F.shape[-1]\r\n",
    "    feature_count = X.shape[-1]\r\n",
    "    n_out_features = (feature_count - filter_size) + 1\r\n",
    "    for i in range(n_out_features):\r\n",
    "      A.append((X[...,i : i + filter_size] * F).sum(axis=-1))\r\n",
    "    return np.array(A).T\r\n",
    "\r\n",
    "# layer = Conv1DBatch(padding = 2)\r\n",
    "# X = np.array([[[1,2,3],[4,5,6]], [[2,3,4],[5,6,7]]])\r\n",
    "# layer.B = np.array([1,2,3])\r\n",
    "# fr = layer.forward(X)\r\n",
    "# bk = layer.backward(fr)\r\n",
    "# println(fr.shape, fr, bk.shape, bk)\r\n",
    "# X = X.reshape(X.shape[0], -1)\r\n",
    "# layer = FC(n_nodes1 = X.shape[1], n_nodes2 = 3)\r\n",
    "# fr = layer.forward(X)\r\n",
    "# bk = layer.backward(fr)\r\n",
    "# println(fr.shape, fr, bk.shape, bk)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Others"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "class AdaGrad():\r\n",
    "    \"\"\"\r\n",
    "    Stochastic gradient descent\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    lr : Learning rate\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, lr):\r\n",
    "        self.lr = lr\r\n",
    "        self.Hw = 0\r\n",
    "        self.Hb = 0\r\n",
    "    def update(self, layer, dW, dB):\r\n",
    "        \"\"\"\r\n",
    "        Update weights and biases for a layer\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        layer : Instance of the layer before update\r\n",
    "        \"\"\"\r\n",
    "        #ada weight\r\n",
    "\r\n",
    "        eps = 1e-6\r\n",
    "        self.Hw += dW ** 2\r\n",
    "        self.Hb += dB ** 2\r\n",
    "        layer.W += - self.lr * (1 / np.sqrt(self.Hw + eps)) * dW\r\n",
    "        layer.B += - self.lr * (1 / np.sqrt(self.Hb + eps)) * dB\r\n",
    "\r\n",
    "class GetMiniBatch:\r\n",
    "    \"\"\"\r\n",
    "Iterator to get a mini-batch\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\r\n",
    "      Training data\r\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\r\n",
    "      Correct answer value\r\n",
    "    batch_size : int\r\n",
    "      Batch size\r\n",
    "    seed : int\r\n",
    "      NumPy random number seed\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, X, y = None, batch_size = 20, seed=0):\r\n",
    "        self.batch_size = batch_size\r\n",
    "        np.random.seed(seed)\r\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
    "        self._X = X[shuffle_index]\r\n",
    "        self._y = y[shuffle_index] if y is not None else None\r\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
    "    def __len__(self):\r\n",
    "        return self._stop\r\n",
    "    def __getitem__(self,item):\r\n",
    "        p0 = item*self.batch_size\r\n",
    "        p1 = item*self.batch_size + self.batch_size\r\n",
    "        if self._y is not None:\r\n",
    "          return self._X[p0:p1], self._y[p0:p1] \r\n",
    "        else:\r\n",
    "          return self._X[p0:p1]       \r\n",
    "    def __iter__(self):\r\n",
    "        self._counter = 0\r\n",
    "        return self\r\n",
    "    def __next__(self):\r\n",
    "        if self._counter >= self._stop:\r\n",
    "            raise StopIteration()\r\n",
    "        p0 = self._counter*self.batch_size\r\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
    "        self._counter += 1\r\n",
    "        if self._y is not None:\r\n",
    "          return self._X[p0:p1], self._y[p0:p1] \r\n",
    "        else:\r\n",
    "          return self._X[p0:p1]\r\n",
    "\r\n",
    "class TransparentFunction():\r\n",
    "    def forward(self,X):\r\n",
    "        return X\r\n",
    "    def backward(self,dA):\r\n",
    "        return dA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "class DeepNeuralNetworkClassifier():\r\n",
    "    def __init__(self,encoder, max_iter = 5, lr = 0.1, batch_size = 20 ,\r\n",
    "               verbose = False, debug = False):\r\n",
    "        self.epoch = max_iter\r\n",
    "        self.verbose = verbose\r\n",
    "        self.debug = debug\r\n",
    "        self.lr = lr\r\n",
    "        #other non-parametric vars:\r\n",
    "        self.encoder = encoder\r\n",
    "        self.sigma = 0.01\r\n",
    "        self.batch_size = batch_size # batch size \r\n",
    "        \r\n",
    "        #layers and activations\r\n",
    "        self.layers = []\r\n",
    "        self.activations = []\r\n",
    "\r\n",
    "    def add(self,layer, activation):\r\n",
    "      self.layers.append(layer)\r\n",
    "      self.activations.append(activation)\r\n",
    "        \r\n",
    "    def enum_layer_act(self, rev = False):\r\n",
    "      zipped = zip(self.layers, self.activations)\r\n",
    "      if rev:\r\n",
    "        return enumerate(reversed(list(zipped)))\r\n",
    "      return enumerate(zipped)\r\n",
    "\r\n",
    "    def forward_prop(self,X):\r\n",
    "      Z = X\r\n",
    "      for i, (layer, activation) in self.enum_layer_act():\r\n",
    "        A = layer.forward(Z)\r\n",
    "        Z = activation.forward(A)\r\n",
    "        if self.debug:\r\n",
    "          print(f'Z{i+1}: ', Z.shape, A.shape)\r\n",
    "      return Z\r\n",
    "\r\n",
    "    def backward_prop(self,Z,y):\r\n",
    "      dA = self.activations[-1].backward(Z,y)\r\n",
    "      if self.debug:\r\n",
    "        print(f'Backward last Y: ', dA.shape)\r\n",
    "      for i, (layer, activation) in self.enum_layer_act(rev = True):\r\n",
    "        if i == 0: #last layer has different activation backward!\r\n",
    "          dZ = layer.backward(dA)\r\n",
    "          if self.debug:\r\n",
    "            print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
    "          continue\r\n",
    "        dA = activation.backward(dZ)\r\n",
    "        dZ = layer.backward(dA)\r\n",
    "        if self.debug:\r\n",
    "          print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
    "        \r\n",
    "    \r\n",
    "    def cross_entropy_error(self,Z,y):\r\n",
    "      return (np.log(Z) * y).sum() / (- len(Z))\r\n",
    "\r\n",
    "    def predict(self,X):\r\n",
    "      y = np.zeros(X.shape[0])\r\n",
    "      Z  = self.forward_prop(X)\r\n",
    "      return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))\r\n",
    "\r\n",
    "\r\n",
    "    def fit(self,X,y, X_val = None, y_val = None):\r\n",
    "      #prepare\r\n",
    "      self.n_features = X.shape[1]\r\n",
    "      self.lenx = len(X)\r\n",
    "      self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\r\n",
    "\r\n",
    "      if self.verbose:\r\n",
    "          print('X shape: ', X.shape, 'type: ', X.dtype)\r\n",
    "          print('Batch count: ', self.batch_count)\r\n",
    "          # for i, (layer, activation) in self.enum_layer_act():\r\n",
    "          #   print(f'Layer {i+1}: ', layer.n_nodes1, layer.n_nodes2)\r\n",
    "          #   print(f'Activ: {i+1}:', activation.__class__.__name__)\r\n",
    "\r\n",
    "      #train\r\n",
    "      self.loss = np.zeros(self.epoch)\r\n",
    "      self.accuracy = np.zeros(self.epoch)\r\n",
    "      for i in range(self.epoch): #one full data ilteration\r\n",
    "          if self.verbose: print('Epoch: ', i)\r\n",
    "          self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\r\n",
    "          for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\r\n",
    "              if self.debug: print('Current batch: ', idx, mini_X_train.shape, mini_y_train.shape)\r\n",
    "              #train mini_batch\r\n",
    "              Z = self.forward_prop(mini_X_train)\r\n",
    "              self.backward_prop(Z,mini_y_train)\r\n",
    "\r\n",
    "\r\n",
    "          #record loss data\r\n",
    "          if self.debug: print('Predicting and recording loss/acc')\r\n",
    "          Z = self.forward_prop(X)\r\n",
    "          self.loss[i] = self.cross_entropy_error(Z,y)\r\n",
    "          train_pred = self.predict(X)\r\n",
    "          self.accuracy[i]  = accuracy_score(train_pred,y)\r\n",
    "          if self.verbose:\r\n",
    "              print(f'Loss {i}:', self.loss[i])\r\n",
    "              print(f'Acc {i}:', self.accuracy[i])\r\n",
    "              \r\n",
    "      #verbose\r\n",
    "      if self.verbose:\r\n",
    "          print('Final train loss:',self.loss[-1])\r\n",
    "          print('Final train accuracy:',self.accuracy[-1])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "class UpdatedDNN():\r\n",
    "    def __init__(self,encoder, max_iter = 5, lr = 0.1, batch_size = 20 ,\r\n",
    "               verbose = False, debug = False):\r\n",
    "        self.epoch = max_iter\r\n",
    "        self.verbose = verbose\r\n",
    "        self.debug = debug\r\n",
    "        self.lr = lr\r\n",
    "        #other non-parametric vars:\r\n",
    "        self.encoder = encoder\r\n",
    "        self.sigma = 0.01\r\n",
    "        self.batch_size = batch_size # batch size \r\n",
    "        \r\n",
    "        #layers (including activations)\r\n",
    "        self.layers = []\r\n",
    "\r\n",
    "    def add(self,layer):\r\n",
    "      self.layers.append(layer)\r\n",
    "        \r\n",
    "    def enum_layers(self, rev = False):\r\n",
    "      if rev:\r\n",
    "        return enumerate(reversed(list(self.layers)))\r\n",
    "      return enumerate(self.layers)\r\n",
    "\r\n",
    "    def forward_prop(self,X):\r\n",
    "      try:\r\n",
    "        Z = X\r\n",
    "        for i, layer in self.enum_layers():\r\n",
    "          Z = layer.forward(Z)\r\n",
    "          if self.debug:\r\n",
    "            print(f'Z{i+1}: ', Z.shape)\r\n",
    "      except BaseException as e:\r\n",
    "        println('Error: ', e)\r\n",
    "        print('ERROR LAYER INFORMATION:', layer.__class__.__name__, 'index: ', i + 1)\r\n",
    "      return Z\r\n",
    "\r\n",
    "    def backward_prop(self,Z,y):\r\n",
    "      dZ = self.layers[-1].backward(Z,y)\r\n",
    "      if self.debug:\r\n",
    "        print(f'Backward last Y: ', dZ.shape)\r\n",
    "      for i, layer in self.enum_layers(rev = True):\r\n",
    "        if i == 0: continue #already calculated last backward\r\n",
    "        dZ = layer.backward(dZ)\r\n",
    "        if self.debug:\r\n",
    "          print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\r\n",
    "        \r\n",
    "    \r\n",
    "    def cross_entropy_error(self,Z,y):\r\n",
    "      return (np.log(Z) * y).sum() / (- len(Z))\r\n",
    "\r\n",
    "    def predict(self,X):\r\n",
    "      y = np.zeros(X.shape[0])\r\n",
    "      Z  = self.forward_prop(X)\r\n",
    "      return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))\r\n",
    "\r\n",
    "\r\n",
    "    def fit(self,X,y, X_val = None, y_val = None):\r\n",
    "      #prepare\r\n",
    "      self.n_features = X.shape[1]\r\n",
    "      self.lenx = len(X)\r\n",
    "      self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\r\n",
    "\r\n",
    "      if self.verbose:\r\n",
    "          print('X shape: ', X.shape, 'type: ', X.dtype)\r\n",
    "          print('Batch count: ', self.batch_count)\r\n",
    "          for i, layer in self.enum_layers():\r\n",
    "            print(f'Layer {i+1}: ', layer.__class__.__name__)\r\n",
    "\r\n",
    "      #train\r\n",
    "      self.loss = np.zeros(self.epoch)\r\n",
    "      self.accuracy = np.zeros(self.epoch)\r\n",
    "      for i in range(self.epoch): #one full data ilteration\r\n",
    "          if self.verbose: print('Epoch: ', i)\r\n",
    "          self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\r\n",
    "          for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\r\n",
    "              if self.debug: print('Current batch: ', idx, mini_X_train.shape, mini_y_train.shape)\r\n",
    "              #train mini_batch\r\n",
    "              Z = self.forward_prop(mini_X_train)\r\n",
    "              self.backward_prop(Z,mini_y_train)\r\n",
    "\r\n",
    "\r\n",
    "          #record loss data\r\n",
    "          if self.debug: print('Predicting and recording loss/acc')\r\n",
    "          Z = self.forward_prop(X)\r\n",
    "          self.loss[i] = self.cross_entropy_error(Z,y)\r\n",
    "          train_pred = self.predict(X)\r\n",
    "          self.accuracy[i]  = accuracy_score(train_pred,y)\r\n",
    "          if self.verbose:\r\n",
    "              print(f'Loss {i}:', self.loss[i])\r\n",
    "              print(f'Acc {i}:', self.accuracy[i])\r\n",
    "              \r\n",
    "      #verbose\r\n",
    "      if self.verbose:\r\n",
    "          print('Final train loss:',self.loss[-1])\r\n",
    "          print('Final train accuracy:',self.accuracy[-1])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "#data set\r\n",
    "from keras.datasets import mnist\r\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
    "\r\n",
    "#random subset since the data is too large\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=0.075)\r\n",
    "\r\n",
    "#reshape and add one channel to X\r\n",
    "X_train = np.expand_dims(X_train, axis=1)\r\n",
    "X_test = np.expand_dims(X_test, axis=1)\r\n",
    "\r\n",
    "#scaling\r\n",
    "X_train = X_train.astype(np.float)\r\n",
    "X_test = X_test.astype(np.float)\r\n",
    "X_train /= 255\r\n",
    "X_test /= 255\r\n",
    "#one hot encode for multiclass labels!\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\r\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\r\n",
    "\r\n",
    "#validation split\r\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, train_size=0.8)\r\n",
    "print([i.shape for i in [x_train, x_val, y_train, y_val]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(3600, 1, 28, 28), (900, 1, 28, 28), (3600, 10), (900, 10)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "# test run deep neural\r\n",
    "model = UpdatedDNN(enc, debug = False, verbose = True, max_iter = 5)\r\n",
    "n_in_features = x_train.shape[1]\r\n",
    "batch_size  = 20\r\n",
    "print('train shape: ', x_train.shape)\r\n",
    "print('input features: ', n_in_features)\r\n",
    "print('input channels: ', batch_size)\r\n",
    "\r\n",
    "\r\n",
    "in_channel = x_train.shape[1]\r\n",
    "\r\n",
    "# l1 = Conv1DBatch(filter_size = 3, n_input = in_channel, n_output = 1, optimizer = AdaGrad(0.1))\r\n",
    "# model.add(l1)\r\n",
    "# model.add(Tanh())\r\n",
    "l1 = Conv2D(n_out_channels = 2, kernel_size = (8,8))\r\n",
    "model.add(l1)\r\n",
    "model.add(MaxPool2D(pool_size = (5,5)))\r\n",
    "model.add(ReLU())\r\n",
    "\r\n",
    "# l11 = MaxPool()\r\n",
    "# model.add()\r\n",
    "lshape = FlattenLayer()\r\n",
    "model.add(lshape)\r\n",
    "l2 = FC(578,100, SimpleInitializer(),SGD()) #cause output of conv is 394\r\n",
    "model.add(l2)\r\n",
    "model.add(Tanh())\r\n",
    "l3 = FC(100,10, SimpleInitializer(),SGD())\r\n",
    "model.add(l3)\r\n",
    "model.add(SoftMax())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train shape:  (3600, 1, 28, 28)\n",
      "input features:  1\n",
      "input channels:  20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "print('xshape: ', x_train.shape, 'yshape: ', y_train.shape)\r\n",
    "model.fit(x_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "xshape:  (3600, 1, 28, 28) yshape:  (3600, 10)\n",
      "X shape:  (3600, 1, 28, 28) type:  float64\n",
      "Batch count:  180\n",
      "Layer 1:  Conv2D\n",
      "Layer 2:  MaxPool2D\n",
      "Layer 3:  ReLU\n",
      "Layer 4:  FlattenLayer\n",
      "Layer 5:  FC\n",
      "Layer 6:  Tanh\n",
      "Layer 7:  FC\n",
      "Layer 8:  SoftMax\n",
      "Epoch:  0\n",
      "Loss 0: 1.492013494944866\n",
      "Acc 0: 0.6308333333333334\n",
      "Epoch:  1\n",
      "Loss 1: 1.0687034757780534\n",
      "Acc 1: 0.7497222222222222\n",
      "Epoch:  2\n",
      "Loss 2: 0.8398523367735573\n",
      "Acc 2: 0.7947222222222222\n",
      "Epoch:  3\n",
      "Loss 3: 0.6944132532629612\n",
      "Acc 3: 0.8172222222222222\n",
      "Epoch:  4\n",
      "Loss 4: 0.6200881833937789\n",
      "Acc 4: 0.8305555555555556\n",
      "Final train loss: 0.6200881833937789\n",
      "Final train accuracy: 0.8305555555555556\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "\r\n",
    "# loss result\r\n",
    "plt.plot(model.loss, label = 'TrainLoss')\r\n",
    "plt.plot(model.accuracy, label = 'TrainAccuracy')\r\n",
    "plt.title('Training Metrics')\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "# test result\r\n",
    "pred = model.predict(x_val)\r\n",
    "print('test acc: ', accuracy_score(pred, y_val))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test acc:  0.8022222222222222\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 8\r\n",
    "Lenet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Subsampling is the modern equivalent of pooling. Let's make a modern version of it as follows. The activation function was also a sigmoid function back then, but we'll call it ReLU.**\r\n",
    "\r\n",
    "1. Convolutional layer Number of output channels 6, filter size 5 x 5, stride 1\r\n",
    "2. ReLU\r\n",
    "3. Maximum pooling\r\n",
    "4. Convolutional layer 16 output channels, filter size 5 x 5, stride 1\r\n",
    "5. ReLU\r\n",
    "6. Maximum pooling\r\n",
    "7. Smoothing\r\n",
    "8. All coupling layers 120 output nodes\r\n",
    "9. ReLU\r\n",
    "10. All coupling layers 84 output nodes\r\n",
    "11. ReLU\r\n",
    "12. All coupling layers Number of output nodes 10\r\n",
    "13. Softmax Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "lenet = UpdatedDNN(enc, debug = False, verbose = True, max_iter = 5)\r\n",
    "layers = [\r\n",
    "    Conv2D(n_out_channels=6, kernel_size = (5,5)),\r\n",
    "    ReLU(),\r\n",
    "    MaxPool2D(pool_size = (2,2)),\r\n",
    "    Conv2D(n_out_channels=16, kernel_size = (5,5)),\r\n",
    "    ReLU(),\r\n",
    "    MaxPool2D(pool_size = (2,2)),\r\n",
    "    FlattenLayer(),\r\n",
    "    FC(5184, 120, SimpleInitializer(),SGD()),\r\n",
    "    ReLU(),\r\n",
    "    FC(120, 84, SimpleInitializer(),SGD()),\r\n",
    "    ReLU(),\r\n",
    "    FC(84, 10, SimpleInitializer(),SGD()),\r\n",
    "    SoftMax()\r\n",
    "]\r\n",
    "\r\n",
    "[lenet.add(layer) for layer in layers];"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "print('xshape: ', x_train.shape, 'yshape: ', y_train.shape)\r\n",
    "lenet.fit(x_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "xshape:  (3600, 1, 28, 28) yshape:  (3600, 10)\n",
      "X shape:  (3600, 1, 28, 28) type:  float64\n",
      "Batch count:  180\n",
      "Layer 1:  Conv2D\n",
      "Layer 2:  ReLU\n",
      "Layer 3:  MaxPool2D\n",
      "Layer 4:  Conv2D\n",
      "Layer 5:  ReLU\n",
      "Layer 6:  MaxPool2D\n",
      "Layer 7:  FlattenLayer\n",
      "Layer 8:  FC\n",
      "Layer 9:  ReLU\n",
      "Layer 10:  FC\n",
      "Layer 11:  ReLU\n",
      "Layer 12:  FC\n",
      "Layer 13:  SoftMax\n",
      "Epoch:  0\n",
      "Loss 0: 0.5597113988623933\n",
      "Acc 0: 0.8305555555555556\n",
      "Epoch:  1\n",
      "Loss 1: 0.5022307156355194\n",
      "Acc 1: 0.8586111111111111\n",
      "Epoch:  2\n",
      "Loss 2: 0.48734969861941724\n",
      "Acc 2: 0.8708333333333333\n",
      "Epoch:  3\n",
      "Loss 3: 0.4879990131273452\n",
      "Acc 3: 0.8725\n",
      "Epoch:  4\n",
      "Loss 4: 0.3717217675472228\n",
      "Acc 4: 0.8988888888888888\n",
      "Final train loss: 0.3717217675472228\n",
      "Final train accuracy: 0.8988888888888888\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "# loss result\r\n",
    "plt.plot(lenet.loss, label = 'TrainLoss')\r\n",
    "plt.plot(lenet.accuracy, label = 'TrainAccuracy')\r\n",
    "plt.title('Training Metrics')\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "# test result\r\n",
    "pred = lenet.predict(x_val)\r\n",
    "print('test acc: ', accuracy_score(pred, y_val))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test acc:  0.8766666666666667\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 9\r\n",
    "Survey of famous image recognition models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AlexNet\r\n",
    "\r\n",
    "- 2012 ImageNet LSVRC-2012 winner, large impact on computer vision ML\r\n",
    "- ReLU activation after every convolutio or FC layer\r\n",
    "- This was use on multiple gpus\r\n",
    "- Pooling was overlapping not like previously non-overlapping methods.\r\n",
    "\r\n",
    "## VGG\r\n",
    "\r\n",
    "**Very Deep Convolutional Networks for Large-Scale Image Recognition**\r\n",
    "\r\n",
    "- A family (VGG16, 19)\r\n",
    "- 2014, very good even to this day, among top\r\n",
    "- Model is a bunch of sequential filters, using 3x3 kenels to reduce the params and use ReLU activation\r\n",
    "- Number of parameters is 138 bil, which makes it quite slower and larger model to train.\r\n",
    "- Simple and intuitive in it's idea\r\n",
    "- 1st Rank in the ImageNet ILSVRC-2014"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 10\r\n",
    "Calculation of output size and number of parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Calculate the output size and the number of parameters for the following three convolution layers. For the number of parameters, please also consider the bias term.**\r\n",
    "\r\n",
    "### In general\r\n",
    "\r\n",
    "- Weight: n_out_channel * n_in_channel * ker_height * ker_width\r\n",
    "- Bias: n_out_channel\r\n",
    "- Output size: n_out_channel * out_height * out_width\r\n",
    "- out_height, out_width with respect to kernel size, padding and stride\r\n",
    "\r\n",
    "### Question 1\r\n",
    "\r\n",
    "- Input size : 144 x 144, 3 channels\r\n",
    "- Filter size: 3 x 3, 6 channels\r\n",
    "- Stride : 1\r\n",
    "- Padding: none\r\n",
    "\r\n",
    "**Ans:**\r\n",
    "\r\n",
    "- W: 6 x 3 x 3 x 3\r\n",
    "- B: 6\r\n",
    "- (out_height, out_width): (142, 142)\r\n",
    "- output: 6 x 142 x 142\r\n",
    "\r\n",
    "### Question 2\r\n",
    "\r\n",
    "- Input size : 60x60, 24 channels\r\n",
    "- Filter size: 3 x 3, 48 channels\r\n",
    "- Stride : 1\r\n",
    "- Padding: none\r\n",
    "\r\n",
    "**Ans:**\r\n",
    "\r\n",
    "- W: 48 x 24 x 3 x 3\r\n",
    "- B: 48\r\n",
    "- (out_height, out_width): (58, 58)\r\n",
    "- output: 48 x 58 x 58\r\n",
    "\r\n",
    "### Question 3\r\n",
    "\r\n",
    "- Input size : 20x20, 10 channels\r\n",
    "- Filter size: 3 x 3, 20 channels\r\n",
    "- Stride: 2\r\n",
    "- Padding: none\r\n",
    "\r\n",
    "**Ans:**\r\n",
    "\r\n",
    "- W: 20 x 10 x 3 x 3\r\n",
    "- B: 20\r\n",
    "- (out_height, out_width): (9, 9)\r\n",
    "- output: 20 x 9 x 9\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 11\r\n",
    "Survey on filter size"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Smaller is prefered to capture the local connection among pixels and reveal repeating patterns\r\n",
    "- Odd size filter is prefered since we want the result to be centered, given that most of convolution implementation takes the center as the place for the convolved result. \r\n",
    "\r\n",
    "### Size 1x1\r\n",
    "- Mostly for channel reduction, squashing many channels into one\r\n",
    "\r\n",
    "### Size 3x3\r\n",
    "- The smallest odd size that works\r\n",
    "- Computational in-expensive\r\n",
    "\r\n",
    "[Very nice reference](https://icecreamlabs.com/2018/08/19/3x3-convolution-filters%E2%80%8A-%E2%80%8Aa-popular-choice/)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('ML': conda)"
  },
  "interpreter": {
   "hash": "141d363dc4fa48226168697c78b1c392551357d3383cf9fbfe3b78c47f1b89a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}