{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScratchDeepNeural.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZ1QZeonUkYaL8ERfbLAj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/produdez/diveintocode-ml/blob/main/Week%2016/ScratchDeepNeural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRSnDIbO6kLK"
      },
      "source": [
        "# Introduction\n",
        "Last time, we created a three-layer neural network, but this time we will rewrite it into one that can easily be expanded to an arbitrary number of layers. After that, we will be able to deal with advanced functions, activation functions, initial values, and optimization methods.\n",
        "\n",
        "\n",
        "By doing this from scratch, we aim to give you an idea of the inner workings of the various frameworks that we will be using.\n",
        "\n",
        "\n",
        "The name should be changed from Scratch Deep Neural Network Classifier class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMnIuiAl6PFU"
      },
      "source": [
        "#dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaiEVcL8hUPN"
      },
      "source": [
        "# Data Set Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYvxe-vn6llv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec57d7a8-64b1-4188-f254-cebf67938398"
      },
      "source": [
        "#data set\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#reshape\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "#scaling\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "#one hot encode for multiclass labels!\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "#validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4X-mPbdhXXl"
      },
      "source": [
        "# Mini Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JbQAtYB66z-"
      },
      "source": [
        "#mini batch\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tixzo3OhZ2j"
      },
      "source": [
        "# Old Prototype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcF58I5t7dYT"
      },
      "source": [
        "# 3 layers prototype\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Scratch3LayerNeural():\n",
        "    def __init__(self, max_iter = 50, lr = 0.1, encoder = enc, verbose = False, debug = False):\n",
        "        self.epoch = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.debug = debug\n",
        "        self.lr = lr\n",
        "        #other non-parametric vars:\n",
        "        self.encoder = encoder\n",
        "        self.activation = sigmoid\n",
        "        self.sigma = 0.01\n",
        "        self.batch_size = 20 # batch size \n",
        "        self.n_features = 784 # number of features \n",
        "        self.n_nodes1 = 400 # number of first layer nodes \n",
        "        self.n_nodes2 = 200 # number of second layer nodes \n",
        "        self.n_output = 10 # number of output classes (number of nodes in the 3rd layer)\n",
        "\n",
        "    def _init_params_and_output_container(self, prev_n_nodes, current_n_nodes):\n",
        "        weight = self.sigma * np.random.randn(prev_n_nodes,current_n_nodes)\n",
        "        bias = self.sigma * np.random.randn(1,current_n_nodes)\n",
        "        return weight,bias\n",
        "    def fit(self,X,y, X_val = None, y_val = None):\n",
        "        #prepare\n",
        "        self.n_features = X.shape[1]\n",
        "        self.lenx = len(X)\n",
        "        self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\n",
        "        #init weights,bias and z_container\n",
        "        self.W1,self.B1 = self._init_params_and_output_container(n_features,self.n_nodes1)\n",
        "        self.W2,self.B2 = self._init_params_and_output_container(self.n_nodes1,self.n_nodes2)\n",
        "        self.W3,self.B3 = self._init_params_and_output_container(self.n_nodes2,self.n_output)\n",
        "        if self.verbose:\n",
        "            print('X shape: ', X.shape, 'type: ', X.dtype)\n",
        "            print('Batch count: ', self.batch_count)\n",
        "            print('1st layer: ', self.W1.shape, self.B1.shape)\n",
        "            print('2nd layer: ', self.W2.shape, self.B2.shape)\n",
        "            print('3rd layer: ', self.W3.shape, self.B3.shape)\n",
        "\n",
        "        #train\n",
        "        self.loss = np.zeros(self.epoch)\n",
        "        self.accuracy = np.zeros(self.epoch)\n",
        "        for i in range(self.epoch): #one full data ilteration\n",
        "            if self.verbose: print('Epoch: ', i)\n",
        "            self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\n",
        "                if self.debug: print('Current batch: ', idx)\n",
        "                #train mini_batch\n",
        "                self.forward_prop(mini_X_train,mini_y_train)\n",
        "                self.backward_prop(mini_X_train,mini_y_train)\n",
        "\n",
        "\n",
        "            #record loss data\n",
        "            Z3 = self.forward_prop(X,y,update = False)\n",
        "            self.loss[i] = self.cross_entropy_error(Z3,y)\n",
        "            train_pred = self.predict(X)\n",
        "            self.accuracy[i]  = accuracy_score(train_pred,y)\n",
        "            if self.verbose:\n",
        "                print(f'Loss {i}:', self.loss[i])\n",
        "                print(f'Acc {i}:', self.accuracy[i])\n",
        "                \n",
        "        #verbose\n",
        "        if self.verbose:\n",
        "            print('Final train loss:',self.loss[-1])\n",
        "            print('Final train accuracy:',self.accuracy[-1])\n",
        "\n",
        "    def forward_prop(self,X,y, update = True):\n",
        "        activation = self.activation\n",
        "        W1,B1 = self.W1,self.B1\n",
        "        W2,B2 = self.W2,self.B2\n",
        "        W3,B3 = self.W3,self.B3\n",
        "        # first layer\n",
        "        A1 = X@W1 + B1\n",
        "        Z1= activation(A1)\n",
        "        # second layer\n",
        "        A2 = Z1@W2 + B2\n",
        "        Z2 = activation(A2)\n",
        "        #last (third) layer\n",
        "        A3 = Z2@W3 + B3\n",
        "        Z3 = softmax(A3)\n",
        "\n",
        "        if update: #for training\n",
        "            self.A1 = A1\n",
        "            self.Z1 = Z1\n",
        "            self.A2 = A2\n",
        "            self.Z2 = Z2\n",
        "            self.A3 = A3\n",
        "            self.Z3 = Z3\n",
        "        else: # for predicting\n",
        "            return Z3\n",
        "\n",
        "    def backward_prop(self,X,y):\n",
        "        A1 = self.A1 \n",
        "        Z1 = self.Z1 \n",
        "        A2 = self.A2 \n",
        "        Z2 = self.Z2 \n",
        "        A3 = self.A3 \n",
        "        Z3 = self.Z3 \n",
        "        #third\n",
        "        grad_L_A3 = 1/nb * (Z3 - y)\n",
        "        grad_L_B3 = grad_L_A3.sum(axis = 0).reshape(1,-1)\n",
        "        grad_L_W3 = Z2.T @ grad_L_A3\n",
        "        grad_L_Z2 = grad_L_A3 @ W3.T\n",
        "        #second\n",
        "        grad_L_A2 = grad_L_Z2 * (1 - np.tanh(A2)** 2)\n",
        "        grad_L_B2 = grad_L_A2.sum(axis = 0).reshape(1,-1)\n",
        "        grad_L_W2 = Z1.T @ grad_L_A2\n",
        "        grad_L_Z1 = grad_L_A2 @ W2.T\n",
        "        #first\n",
        "        grad_L_A1 = grad_L_Z1 * (1 - np.tanh(A1) ** 2)\n",
        "        grad_L_B1 = grad_L_A1.sum(axis = 0).reshape(1,-1)\n",
        "        grad_L_W1 = X.T @ grad_L_A1\n",
        "        \n",
        "        #new params\n",
        "        lr = self.lr\n",
        "        self.W1 += - lr * grad_L_W1\n",
        "        self.B1 += - lr * grad_L_B1\n",
        "        self.W2 += - lr * grad_L_W2\n",
        "        self.B2 += - lr * grad_L_B2\n",
        "        self.W3 += - lr * grad_L_W3\n",
        "        self.B3 += - lr * grad_L_B3\n",
        "        \n",
        "    \n",
        "    def cross_entropy_error(self,Z3,y):\n",
        "        return (np.log(Z3) * y).sum() / (- len(Z3))\n",
        "\n",
        "    def predict(self,X):\n",
        "        y = np.zeros(X.shape[0])\n",
        "        Z3  = self.forward_prop(X,y,update = False)\n",
        "        return self.encoder.transform(np.argmax(Z3, axis = 1).reshape(-1,1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R81UNdtEhceP"
      },
      "source": [
        "# Problem 1\n",
        "Classifying fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6H97BS0-ZeL"
      },
      "source": [
        "class FC:\n",
        "  \"\"\"\n",
        "  Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "  Parameters\n",
        "  ----------\n",
        "  n_nodes1 : int\n",
        "    Number of nodes in the previous layer\n",
        "  n_nodes2 : int\n",
        "    Number of nodes in the later layer\n",
        "  initializer: instance of initialization method\n",
        "  optimizer: instance of optimization method\n",
        "  \"\"\"\n",
        "  def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "      # Initialize\n",
        "      self.optimizer = optimizer\n",
        "      self.n_nodes1, self.n_nodes2 = n_nodes1, n_nodes2\n",
        "      # Initialize self.W and self.B using the initializer method\n",
        "      self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "      self.B = initializer.B(n_nodes2)\n",
        "      pass\n",
        "  def forward(self, X):\n",
        "      \"\"\"\n",
        "      forward\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "          入力\n",
        "      Returns\n",
        "      ----------\n",
        "      A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "          output\n",
        "      \"\"\"        \n",
        "      self.X = X\n",
        "      A = X @ self.W + self.B\n",
        "      return A\n",
        "  def backward(self, dA):\n",
        "      \"\"\"\n",
        "      Backward\n",
        "      Parameters\n",
        "      ----------\n",
        "      dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "          Gradient flowing from behind\n",
        "      Returns\n",
        "      ----------\n",
        "      dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "          Gradient to flow forward\n",
        "      \"\"\"\n",
        "      # update\n",
        "      self.dA = dA\n",
        "\n",
        "      self.dZ = None #update in optimizer!\n",
        "      self = self.optimizer.update(self)\n",
        "      return self.dZ"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deyYXwH9hjeb"
      },
      "source": [
        "# Problem 2\n",
        "Classifying initialization method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AkIjsn3Aqlp"
      },
      "source": [
        "class SimpleInitializer:\n",
        "  \"\"\"\n",
        "  Simple initialization with Gaussian distribution\n",
        "  Parameters\n",
        "  ----------\n",
        "  sigma : float\n",
        "    Standard deviation of Gaussian distribution\n",
        "  \"\"\"\n",
        "  def __init__(self, sigma):\n",
        "    self.sigma = sigma\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    \"\"\"\n",
        "    Weight initialization\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    Returns\n",
        "    ----------\n",
        "    W :\n",
        "    \"\"\"\n",
        "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "    return W\n",
        "  def B(self, n_nodes2):\n",
        "    \"\"\"\n",
        "    Bias initialization\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    Returns\n",
        "    ----------\n",
        "    B :\n",
        "    \"\"\"\n",
        "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "    return B"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT9yX5UrhnQ7"
      },
      "source": [
        "# Problem 3\n",
        "Classifying Optimization method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvjcuJMkB1_y"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        dA = layer.dA #this is flow from back\n",
        "        X = layer.X # input to the layer (X or Z)\n",
        "        W,B = layer.W, layer.B\n",
        "\n",
        "        dB = dA.sum(axis = 0).reshape(1,-1)  \n",
        "        dW = X.T @ dA\n",
        "        dZ = dA @ W.T #this will flow to the front\n",
        "        #update\n",
        "        layer.B += - self.lr * dB\n",
        "        layer.W += - self.lr * dW\n",
        "        layer.dZ = dZ\n",
        "        return layer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RrMxj2BhsKM"
      },
      "source": [
        "# Problem 4\n",
        "Classifying Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "912e8_nAIFRC"
      },
      "source": [
        "# Problem 4: Classifying activation function\n",
        "class ActivationFunction():\n",
        "  def forward(self,A):\n",
        "    pass\n",
        "  def backward(self,dZ):\n",
        "    pass\n",
        "class Sigmoid(ActivationFunction):\n",
        "  def func(self,A):\n",
        "    return 1/(1+np.exp(-A))\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    return self.func(A)\n",
        "  def backward(self,dZ):\n",
        "    A = self.A\n",
        "    dA = dZ * (1 - self.func(A))@self.func(A)\n",
        "class Tanh(ActivationFunction):\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    Z = (np.exp(A) - np.exp(-A)) / (np.exp(A) + np.exp(-A))\n",
        "    return Z\n",
        "  def backward(self,dZ):\n",
        "    A = self.A\n",
        "    dA = dZ * (1 - np.tanh(A) ** 2)\n",
        "    return dA\n",
        "  \n",
        "class SoftMax(ActivationFunction):\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    Z = np.exp(A) / np.sum(np.exp(A), axis = 1).reshape(-1,1)\n",
        "    return Z\n",
        "  def backward(self,Z,Y):\n",
        "    A = self.A\n",
        "    nb = Z.shape[0]\n",
        "    dA = 1/nb * (Z - Y)\n",
        "    return dA"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpQth5ppif8b"
      },
      "source": [
        "# Deep Neural Network Prototype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR-7rucDPTQU"
      },
      "source": [
        "# the deepneural net proto\n",
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "    def __init__(self, max_iter = 50, lr = 0.1, encoder = enc, verbose = False, debug = False):\n",
        "        self.epoch = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.debug = debug\n",
        "        self.lr = lr\n",
        "        #other non-parametric vars:\n",
        "        self.encoder = encoder\n",
        "        self.sigma = 0.01\n",
        "        self.batch_size = 20 # batch size \n",
        "        self.n_features = 784 # number of features \n",
        "        self.n_nodes1 = 400 # number of first layer nodes \n",
        "        self.n_nodes2 = 200 # number of second layer nodes \n",
        "        self.n_output = 10 # number of output classes (number of nodes in the 3rd layer)\n",
        "        #layers\n",
        "        optimizer = SGD(self.lr)\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation1 = Tanh()\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation2 = Tanh()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation3 = SoftMax()\n",
        "        #wrap up\n",
        "        self.layers = [self.FC1,self.FC2, self.FC3]\n",
        "        self.activations = [self.activation1, self.activation2, self.activation3]\n",
        "    def enum_layer_act(self, rev = False):\n",
        "        zipped = zip(self.layers, self.activations)\n",
        "        if rev:\n",
        "          return enumerate(reversed(list(zipped)))\n",
        "        return enumerate(zipped)\n",
        "    def fit(self,X,y, X_val = None, y_val = None):\n",
        "        #prepare\n",
        "        self.n_features = X.shape[1]\n",
        "        self.lenx = len(X)\n",
        "        self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\n",
        "\n",
        "        if self.verbose:\n",
        "            print('X shape: ', X.shape, 'type: ', X.dtype)\n",
        "            print('Batch count: ', self.batch_count)\n",
        "            for i, (layer, activation) in self.enum_layer_act():\n",
        "              print(f'Layer {i+1}: ', layer.n_nodes1, layer.n_nodes2)\n",
        "              print(f'Activ: {i+1}:', activation.__class__.__name__)\n",
        "\n",
        "        #train\n",
        "        self.loss = np.zeros(self.epoch)\n",
        "        self.accuracy = np.zeros(self.epoch)\n",
        "        for i in range(self.epoch): #one full data ilteration\n",
        "            if self.verbose: print('Epoch: ', i)\n",
        "            self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\n",
        "                if self.debug: print('Current batch: ', idx)\n",
        "                #train mini_batch\n",
        "                Z = self.forward_prop(mini_X_train)\n",
        "                self.backward_prop(Z,mini_y_train)\n",
        "\n",
        "\n",
        "            #record loss data\n",
        "            Z = self.forward_prop(X)\n",
        "            self.loss[i] = self.cross_entropy_error(Z,y)\n",
        "            train_pred = self.predict(X)\n",
        "            self.accuracy[i]  = accuracy_score(train_pred,y)\n",
        "            if self.verbose:\n",
        "                print(f'Loss {i}:', self.loss[i])\n",
        "                print(f'Acc {i}:', self.accuracy[i])\n",
        "                \n",
        "        #verbose\n",
        "        if self.verbose:\n",
        "            print('Final train loss:',self.loss[-1])\n",
        "            print('Final train accuracy:',self.accuracy[-1])\n",
        "\n",
        "    def forward_prop(self,X):\n",
        "        Z = X\n",
        "        for i, (layer, activation) in self.enum_layer_act():\n",
        "          A = layer.forward(Z)\n",
        "          Z = activation.forward(A)\n",
        "          if self.debug:\n",
        "            print(f'Z{i+1}: ', Z.shape, A.shape)\n",
        "        return Z\n",
        "\n",
        "    def backward_prop(self,Z,y):\n",
        "        dA = self.activations[-1].backward(Z,y)\n",
        "        for i, (layer, activation) in self.enum_layer_act(rev = True):\n",
        "          if i == 0: #last layer has different activation backward!\n",
        "            dZ = layer.backward(dA)\n",
        "            continue\n",
        "          dA = activation.backward(dZ)\n",
        "          dZ = layer.backward(dA)\n",
        "        \n",
        "    \n",
        "    def cross_entropy_error(self,Z,y):\n",
        "        return (np.log(Z) * y).sum() / (- len(Z))\n",
        "\n",
        "    def predict(self,X):\n",
        "        y = np.zeros(X.shape[0])\n",
        "        Z  = self.forward_prop(X)\n",
        "        return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yamoSBB9YIdy",
        "outputId": "60a63480-6031-4acd-d4bb-f2e43d85379c"
      },
      "source": [
        "model = ScratchDeepNeuralNetworkClassifier(max_iter = 5,verbose = True, debug = False)\n",
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape:  (48000, 784) type:  float64\n",
            "Batch count:  2400\n",
            "Layer 1:  784 400\n",
            "Activ: 1: Tanh\n",
            "Layer 2:  400 200\n",
            "Activ: 2: Tanh\n",
            "Layer 3:  200 10\n",
            "Activ: 3: SoftMax\n",
            "Epoch:  0\n",
            "Loss 0: 0.2657156442831827\n",
            "Acc 0: 0.9175625\n",
            "Epoch:  1\n",
            "Loss 1: 0.16605391961200766\n",
            "Acc 1: 0.94925\n",
            "Epoch:  2\n",
            "Loss 2: 0.11474138659146708\n",
            "Acc 2: 0.9640416666666667\n",
            "Epoch:  3\n",
            "Loss 3: 0.08668579866175892\n",
            "Acc 3: 0.9730833333333333\n",
            "Epoch:  4\n",
            "Loss 4: 0.06956532891616557\n",
            "Acc 4: 0.9781666666666666\n",
            "Final train loss: 0.06956532891616557\n",
            "Final train accuracy: 0.9781666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6K0XA7bilU3"
      },
      "source": [
        "# Problem 5\n",
        "ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18EJf11ilWXK"
      },
      "source": [
        "class ReLU(ActivationFunction):\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    relu = A * np.where(A > 0, 1,0)\n",
        "    return relu\n",
        "  def backward(self,dZ):\n",
        "    df = np.where(self.A > 0, 1,0)\n",
        "    dA = dZ * df\n",
        "    return dA"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuAbTweyivWD"
      },
      "source": [
        "# Problem 6\n",
        "Init value of Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxpfh6sqn5zI"
      },
      "source": [
        "#sample\n",
        "# W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "# B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "#..\n",
        "\n",
        "class XavierInitializer():\n",
        "  def __init__(self,sigma):\n",
        "    pass\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    self.sigma = 1 / np.sqrt(n_nodes1)\n",
        "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "    return W\n",
        "  def B(self, n_nodes2):\n",
        "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "    return B\n",
        "\n",
        "class HelnInitializer():\n",
        "  def __init__(self,sigma):\n",
        "    pass\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    self.sigma = np.sqrt(2/n_nodes1)\n",
        "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "    return W\n",
        "  def B(self, n_nodes2):\n",
        "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "    return B"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMrx2c43u0fg"
      },
      "source": [
        "# Problem 7\n",
        "Optimization Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izHlJYC1qagp"
      },
      "source": [
        "class AdaGrad():\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.Hw = 0\n",
        "        self.Hb = 0\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        dA = layer.dA #this is flow from back\n",
        "        X = layer.X # input to the layer (X or Z)\n",
        "        W,B = layer.W, layer.B\n",
        "\n",
        "        dB = dA.sum(axis = 0).reshape(1,-1)  \n",
        "        dW = X.T @ dA\n",
        "        dZ = dA @ W.T #this will flow to the front\n",
        "        #ada weight\n",
        "        eps = 1e-6\n",
        "\n",
        "        self.Hw += dW ** 2\n",
        "        self.Hb += dB ** 2\n",
        "        \n",
        "        layer.W += - self.lr * (1 / np.sqrt(self.Hw + eps)) * dW\n",
        "        layer.B += - self.lr * (1 / np.sqrt(self.Hb + eps)) * dB\n",
        "        layer.dZ = dZ\n",
        "        return layer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcZ83E8Fi6nu"
      },
      "source": [
        "# Problem 8\n",
        "Class Completion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI3x2GS2rhrY"
      },
      "source": [
        "# Just fixing up the class to be flexible!\n",
        "class FinishedDeepNeuralNetworkClassifier(ScratchDeepNeuralNetworkClassifier):\n",
        "  def __init__(self, max_iter = 50, lr = 0.1, \n",
        "               layers_n_nodes = [400,200,10],\n",
        "               encoder = enc, optimizer_class = SGD,\n",
        "               activation_class = Tanh, initializer_class = SimpleInitializer,\n",
        "               verbose = False, debug = False):\n",
        "        self.epoch = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.debug = debug\n",
        "        self.lr = lr\n",
        "        #other non-parametric vars:\n",
        "        self.encoder = encoder\n",
        "        self.sigma = 0.01\n",
        "        self.batch_size = 20 # batch size \n",
        " \n",
        "        #prep layers\n",
        "        self.layers_n_nodes = layers_n_nodes\n",
        "        self.initializer_class = initializer_class\n",
        "        self.activation_class = activation_class\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "        for i in range(len(layers_n_nodes)): \n",
        "          if i == 0: continue #specify first layer later when have X\n",
        "          n_nodes1 = layers_n_nodes[i-1]\n",
        "          n_nodes2 = layers_n_nodes[i]\n",
        "          layer = FC(n_nodes1, n_nodes2, initializer_class(self.sigma), optimizer_class(self.lr))\n",
        "          self.layers.append(layer)\n",
        "          if i != len(layers_n_nodes)- 1: #last activation is softmax!\n",
        "            self.activations.append(activation_class())\n",
        "          else:\n",
        "            self.activations.append(SoftMax())\n",
        "  def fit(self,X,y, X_val = None, y_val = None):\n",
        "        first_layer = FC(X.shape[1], self.layers_n_nodes[0], self.initializer_class(self.sigma), self.optimizer_class(self.lr))\n",
        "        self.layers.insert(0,first_layer)\n",
        "        self.activations.insert(0,self.activation_class())\n",
        "        super(FinishedDeepNeuralNetworkClassifier,self).fit(X,y,X_val,y_val)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQRNtw_05Mj"
      },
      "source": [
        "# Problem 9\n",
        "Learning and completion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAm9V2Fk06mq",
        "outputId": "21da85ed-f547-4c1b-8098-d83fa37f4af5"
      },
      "source": [
        "model = FinishedDeepNeuralNetworkClassifier(\n",
        "    max_iter = 5, lr = 0.1,\n",
        "    layers_n_nodes = [100,50,120,10],\n",
        "    optimizer_class = AdaGrad, \n",
        "    activation_class = ReLU,\n",
        "    initializer_class = XavierInitializer,\n",
        "    verbose = True, debug = False\n",
        "    \n",
        ")\n",
        "model.fit(X_train,y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape:  (48000, 784) type:  float64\n",
            "Batch count:  2400\n",
            "Layer 1:  784 100\n",
            "Activ: 1: ReLU\n",
            "Layer 2:  100 50\n",
            "Activ: 2: ReLU\n",
            "Layer 3:  50 120\n",
            "Activ: 3: ReLU\n",
            "Layer 4:  120 10\n",
            "Activ: 4: SoftMax\n",
            "Epoch:  0\n",
            "Loss 0: 0.17078098478946205\n",
            "Acc 0: 0.9493333333333334\n",
            "Epoch:  1\n",
            "Loss 1: 0.12454150036920335\n",
            "Acc 1: 0.962625\n",
            "Epoch:  2\n",
            "Loss 2: 0.10063399293243759\n",
            "Acc 2: 0.9700416666666667\n",
            "Epoch:  3\n",
            "Loss 3: 0.08715571196205905\n",
            "Acc 3: 0.9737083333333333\n",
            "Epoch:  4\n",
            "Loss 4: 0.07841140956401296\n",
            "Acc 4: 0.9766875\n",
            "Final train loss: 0.07841140956401296\n",
            "Final train accuracy: 0.9766875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEFCfS_njPUr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "e8fdfc36-676f-4f5a-b891-a3c79113ee5a"
      },
      "source": [
        "# loss result\n",
        "plt.plot(model.loss, label = 'TrainLoss')\n",
        "plt.plot(model.accuracy, label = 'TrainAccuracy')\n",
        "plt.title('Training Metrics')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}