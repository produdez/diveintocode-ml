{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Keras provides multiple Recurrent layers and related classes. In this Sprint, we aim to be able to explain each role after moving all of these.\n",
    "\n",
    "\n",
    "It is summarized in the following documents.\n",
    "\n",
    "\n",
    "Recurrent layer-Keras documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Execution of various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "- I've tested basic GRU, LSTM and SimpleRNN\n",
    "- On the same dataset\n",
    "- With the same basic model structure with the only difference in the middle layer being GRU, LSTM or SimpleRNN\n",
    "- The code is in other notebooks for simple execution, I only take note of their result here\n",
    "- The base code is from keras's sample lstm code [Link](https://github.com/awslabs/keras-apache-mxnet/blob/master/examples/imdb_lstm.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Notebooks\n",
    "- `gru.ipynb`\n",
    "- `sample_lstm.ipynb`\n",
    "- `simple_rnn.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model structure:\n",
    "mid layer is LSTM/GRU/SimpleRNN\n",
    "```\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, None, 128)         1280000   \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 128)               131584    \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test's setup\n",
    "- Model structure as above\n",
    "- Dataset is IMDB dataset with 10000 max_features\n",
    "- optimizer adam\n",
    "- mid layer (LSTM/GRU/SimpleRNN) have dropout 0.2 and recurrent_dropout 0.2\n",
    "- batch_size 32 and train for 2 epoch\n",
    "- Result is evaluated on error and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Comparison\n",
    "|                    | SimpleRNN |  LSTM |  GRU  |\n",
    "|:------------------:|:---------:|:-----:|:-----:|\n",
    "|    Test Accuracy   |   0.739   | 0.828 | 0.838 |\n",
    "|     Test Error     |   0.528   | 0.391 | 0.370 |\n",
    "|   Train Accuracy   |   0.643   | 0.867 | 0.866 |\n",
    "|     Train Error    |   0.619   | 0.322 | 0.320 |\n",
    "| Validation Accuray |   0.739   | 0.828 | 0.838 |\n",
    "|  Validation Error  |   0.528   | 0.391 | 0.370 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- SimpleRNN is quite behind compare to others\n",
    "- LSTM and GRU have similar performance, decent in all test, validation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "(SKIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Explanation of other classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes\n",
    "I will be reading about these and explaining them, maybe discussing some ideas if possible\n",
    "\n",
    "**RNN**\n",
    "- RNN\n",
    "- SimpleRNN\n",
    "- SimpleRNNCell\n",
    "\n",
    "\n",
    "**GRU**\n",
    "- GRU\n",
    "- GRUCell\n",
    "\n",
    "\n",
    "**LSTM**\n",
    "- LSTM\n",
    "- LSTMCell\n",
    "\n",
    "\n",
    "**Special?**\n",
    "- StackedRNNCells\n",
    "- CuDNNGRU\n",
    "- CuDNNLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/simple_rnn.png) RNN structure and it's visualization over time\n",
    "- This one is basic NN to learn sequence data\n",
    "- Basically it's a modded FC where each node takes two inputs:\n",
    "  - output of the previous time step `h`\n",
    "  - and input `x_i`\n",
    "- The node keeps feeding its output into itself in the next iteration until the `x` input is finished\n",
    "- This allow RNN to learn the sequential properties of the input\n",
    "  \n",
    "**Biggest Problem?**\n",
    "- Vanishing gradient\n",
    "- In this case, it leads to short term memory in RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the keras implementation of our mentioned RNN, also called fully connected RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Cell\" Variant\n",
    "Includes SimpleRNNCell, LSTMCell, GRUCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I think of these as utility unit, though i have not try to use them\n",
    "- Basically, they take a step **down** the ladder of abstraction on RNN implementation\n",
    "- RNN is a unit for processing a sequence by iterating through time, and the \"cell\" is one step in time\n",
    "- \"Cell\" processes only one iteration of the input wheres RNN processes the whole sequence\n",
    "- \"Cell\" allows user (us) to manipulate and build RNN the way we want in more detail and have control over what it does in each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU and LSTM\n",
    "- Gated Recurrent Unit\n",
    "- Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"0\">\n",
    " <tr>\n",
    "    <td>LSTM</td>\n",
    "    <td>GRU</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td><img src=\"./images/lstm.jpg\" width=\"500\"/></td>\n",
    "    <td><img src=\"./images/gru.jpeg\" width=\"400\"/></td>\n",
    " </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are RNN architecture that aims to fix the **\"Short term memory problem of FC RNN**\n",
    "\n",
    "**Basis ideas of both models includes**\n",
    "- The use of \"gates\" to control dataflow in the unit\n",
    "- Significant data will be remembered while other will be discarded\n",
    "- They incorporate a **\"Data highway\"** (cell state) into their structure where data of the previous time step can flow through pretty much `freely`, thus keeping the long term knowledge that has been learnt\n",
    "- Some of their used gates includes:\n",
    "  - input gate: take a part of current time step's knowledge to add to the `data highway`\n",
    "  - output gate: output what has been learnt from current time step\n",
    "  - forget gate: remove insignificance\n",
    "- They also make big use of sigmoid and tanh activation\n",
    "  - sigmoid to filter useless data\n",
    "  - tanh to normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other RNN related classes in Keras\n",
    "- StackedRNNCells\n",
    "- CuDNNGRU\n",
    "- CuDNNLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackedRNNCells\n",
    "A wrapper that allows an RNN unit to behave the same as a single cell (one time step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDNNGRU\n",
    "Fast GRU implementation backed by cuDNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDNNLSTM\n",
    "Same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE on CUDNN\n",
    "1. As of tensorflow 2.0, the CUDNN variant of GRU and LSTM is deprecated\n",
    "2. Also, the base classes of GRU, LSTM will use the CUDNN implementation and GPU **IF** the requirements are met\n",
    "\n",
    "Requirements are as follows:\n",
    "```\n",
    "The requirements to use the cuDNN implementation are:\n",
    "  1. `activation` == `tanh`\n",
    "  2. `recurrent_activation` == `sigmoid`\n",
    "  3. `recurrent_dropout` == 0\n",
    "  4. `unroll` is `False`\n",
    "  5. `use_bias` is `True`\n",
    "  6. Inputs are not masked or strictly right padded.\n",
    "```\n",
    "[Reference](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/recurrent_v2.py#L902)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbd5bea911370489c6458bc4dc1ddafddd410f0ca74aced470acd31061c2c4f6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('gpu-ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
