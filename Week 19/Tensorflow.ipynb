{"cells":[{"cell_type":"markdown","source":["# Intro to tensorflow"],"metadata":{}},{"cell_type":"code","execution_count":1068,"source":["import tensorflow.compat.v1 as tf\n","tf.disable_eager_execution()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1069,"source":["a = tf.constant(5)\n","b = tf.constant(7)\n","add = tf.add(a, b)\n","sess = tf.Session()\n","output = sess.run(add)\n","print(output) # 12\n","sess.close()"],"outputs":[{"output_type":"stream","name":"stdout","text":["12\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1070,"source":["c = tf.placeholder(tf.int32)\n","d = tf.placeholder(tf.int32)\n","add = tf.add(c, d)\n","sess = tf.Session()\n","output = sess.run(add, feed_dict={c:5, d:7})\n","print(output) # 12\n","output = sess.run(add, feed_dict={c:20, d:32})\n","print(output) # 52\n","sess.close()\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["12\n","52\n"]}],"metadata":{}},{"cell_type":"markdown","source":["### NOTE:\n","Following tensorflow 2 from now on."],"metadata":{}},{"cell_type":"markdown","source":["# Logistic regress on Tensorflow"],"metadata":{}},{"cell_type":"code","execution_count":1071,"source":["import numpy as np\n","x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y_train = np.array([[0],[0],[0],[1]])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1072,"source":["x = tf.placeholder(tf.float32, [None, 2])\n","t = tf.placeholder(tf.float32, [None, 1])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1073,"source":["W = tf.Variable(tf.zeros([2,1]))\n","b = tf.Variable(tf.zeros([1]))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1074,"source":["y = tf.sigmoid(tf.matmul(x, W) + b)\n","cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1075,"source":["train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1076,"source":["correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1077,"source":["sess = tf.Session()\n","sess.run(tf.global_variables_initializer())"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1078,"source":["for epoch in range(1000):\n","    sess.run(train_step, feed_dict={\n","        x:x_train,\n","        t:y_train\n","    })\n","# Display the correct answer rate every 100 times\n","    if epoch % 100 == 0:\n","        acc_val = sess.run(\n","            accuracy, feed_dict={\n","                x:x_train,\n","                t:y_train})\n","        print ('epoch: %d, Accuracy: %f'\n","               %(epoch, acc_val))"],"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, Accuracy: 0.750000\n","epoch: 100, Accuracy: 1.000000\n","epoch: 200, Accuracy: 1.000000\n","epoch: 300, Accuracy: 1.000000\n","epoch: 400, Accuracy: 1.000000\n","epoch: 500, Accuracy: 1.000000\n","epoch: 600, Accuracy: 1.000000\n","epoch: 700, Accuracy: 1.000000\n","epoch: 800, Accuracy: 1.000000\n","epoch: 900, Accuracy: 1.000000\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1079,"source":["#Check if the learning result is correct\n","classified = sess.run(correct_prediction, feed_dict={\n","    x:x_train,\n","    t:y_train\n","})\n","#Check output y\n","prob = sess.run(y, feed_dict={\n","    x:x_train,\n","    t:y_train\n","})\n","print(classified)\n","# [[ True]\n","# [ True]\n","# [ True]\n","# [ True]]\n","\n","print(prob)\n","# [[  1.96514215e-04]\n","# [  4.90498319e-02]\n","# [  4.90498319e-02]\n","# [  9.31203783e-01]]"],"outputs":[{"output_type":"stream","name":"stdout","text":["[[ True]\n"," [ True]\n"," [ True]\n"," [ True]]\n","[[1.9654632e-04]\n"," [4.9049824e-02]\n"," [4.9049824e-02]\n"," [9.3120384e-01]]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1080,"source":["print('W:', sess.run(W))\n","print('b:', sess.run(b))\n","# W: [[ 5.5699544]\n","# [ 5.5699544]]\n","# b: [-8.53457928]"],"outputs":[{"output_type":"stream","name":"stdout","text":["W: [[5.5699544]\n"," [5.5699544]]\n","b: [-8.534579]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1081,"source":["sess.close()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Tensorflow (v1)"],"metadata":{}},{"cell_type":"markdown","source":["# Problem 1\n","Looking back on the scratch"],"metadata":{}},{"cell_type":"markdown","source":["### Steps to build a ML model\n","1. Load\n","2. Define model (class) along with needed stuffs (params, input shapes,..., procedure, batch ...)\n","3. Init stuff and use the model"],"metadata":{}},{"cell_type":"markdown","source":["# Problem 2\n","Correspond between scratch and tensorflow"],"metadata":{}},{"cell_type":"code","execution_count":1082,"source":["\"\"\"\n","Binary classification of Iris dataset using neural network implemented in TensorFlow\n","\"\"\"\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","#Load dataset\n","df = pd.read_csv(\"Iris.csv\")\n","\n","#Condition extraction from data frame\n","df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","# NumPy 配列に変換\n","X = np.array(X)\n","y = np.array(y)\n","# Convert label to number\n","y[y == \"Iris-versicolor\"] = 0\n","y[y == \"Iris-virginica\"] = 1\n","y = y.astype(np.int64)[:, np.newaxis]\n","#Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","class GetMiniBatch:\n","    \"\"\"\n","    Iterator to get a mini-batch\n","    Parameters\n","    ----------\n","    X : The following forms of ndarray, shape (n_samples, n_features)\n","      Training data\n","    y : The following form of ndarray, shape (n_samples, 1)\n","      Correct answer value\n","    batch_size : int\n","      Batch size\n","    seed : int\n","      NumPy random number seed\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# Hyperparameter settings\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 10\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","#Determine the shape of the argument to be passed to the calculation graph\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# train mini batch iterator\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    Simple 3-layer neural network\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # Declaration of weight and bias\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n","    return layer_output\n","#Read network structure                              \n","logits = example_net(X)\n","# Objective function\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# Optimization method\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# Estimated result\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","#Indicator value calculation\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","#Initialization of variable\n","init = tf.global_variables_initializer()\n","\n","#Run calculation graph\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        #Loop for each epoch\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # Loop for each mini-batch\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})            \n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, loss : 1.7062, val_loss : 4.3175, acc : 0.562\n","Epoch 1, loss : 0.9475, val_loss : 3.3044, acc : 0.562\n","Epoch 2, loss : 0.6002, val_loss : 7.3488, acc : 0.375\n","Epoch 3, loss : 0.3332, val_loss : 2.4291, acc : 0.625\n","Epoch 4, loss : 0.3623, val_loss : 4.0366, acc : 0.500\n","Epoch 5, loss : 0.1949, val_loss : 1.1861, acc : 0.688\n","Epoch 6, loss : 0.1861, val_loss : 2.2722, acc : 0.562\n","Epoch 7, loss : 0.1023, val_loss : 0.8318, acc : 0.812\n","Epoch 8, loss : 0.1148, val_loss : 1.6800, acc : 0.562\n","Epoch 9, loss : 0.0733, val_loss : 0.6209, acc : 0.812\n","test_acc : 0.800\n"]}],"metadata":{}},{"cell_type":"markdown","source":["### Steps to build in tensorflow\n","1. Load dataset\n","2. Define model and it's flow\n","   1. Define parameters (variable) and inputs (placeholders)\n","   2. Define model's prediction (or forward flow)\n","   3. Define models' loss function\n","   4. Get an optimizer\n","   5. Define the traing flow\n","      - Optimize\n","      - Training scores\n","      - Loss\n","      - Log,...\n","3. Prepare epoch and batch\n","4. Initialize paremeters\n","5.  Start a session\n","  - Train in batches\n","  - Use the model and stuss defined in tf syntax\n","6.  Close session"],"metadata":{}},{"cell_type":"markdown","source":["**Tensorflow 1.0 has a lazy execution approach to it.**"],"metadata":{}},{"cell_type":"markdown","source":["# Problem 3\n","Create a model of Iris using all three types of objective variables"],"metadata":{}},{"cell_type":"markdown","source":["## NOTE\n","**Let's go through the process one by one and test them all!**"],"metadata":{}},{"cell_type":"markdown","source":["## Data Prep"],"metadata":{}},{"cell_type":"code","execution_count":1083,"source":["#load dataset\n","df = pd.read_csv('Iris.csv')\n","\n","print('shape: ', df.shape)\n","print(df.head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["shape:  (150, 6)\n","   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n","0   1            5.1           3.5            1.4           0.2  Iris-setosa\n","1   2            4.9           3.0            1.4           0.2  Iris-setosa\n","2   3            4.7           3.2            1.3           0.2  Iris-setosa\n","3   4            4.6           3.1            1.5           0.2  Iris-setosa\n","4   5            5.0           3.6            1.4           0.2  Iris-setosa\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1084,"source":["# predictor\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","print('X shape:', X.shape)\n","print(X.head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["X shape: (150, 4)\n","   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n","0            5.1           3.5            1.4           0.2\n","1            4.9           3.0            1.4           0.2\n","2            4.7           3.2            1.3           0.2\n","3            4.6           3.1            1.5           0.2\n","4            5.0           3.6            1.4           0.2\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1085,"source":["# label\n","y = df['Species']\n","print('y shape: ', y.shape)\n","print(y.head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["y shape:  (150,)\n","0    Iris-setosa\n","1    Iris-setosa\n","2    Iris-setosa\n","3    Iris-setosa\n","4    Iris-setosa\n","Name: Species, dtype: object\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1086,"source":["# Convert to numpy\n","X = np.array(X)\n","y = np.array(y)\n","print(X.shape, y.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4) (150,)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1087,"source":["# Encode y from text to number label\n","print('init shape y: ', y.shape)\n","#reshape for onehot use\n","y = y.reshape(-1,1)\n","#label encode\n","from sklearn.preprocessing import OneHotEncoder\n","enc = OneHotEncoder()\n","y = enc.fit_transform(y).toarray()\n","print('shape: ', y.shape)\n","print(y[:5])"],"outputs":[{"output_type":"stream","name":"stdout","text":["init shape y:  (150,)\n","shape:  (150, 3)\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1088,"source":["from sklearn.model_selection import train_test_split\n","# train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.9)\n","print('Train/Test size: ', [data.shape for data in [X_train, X_test, y_train, y_test]])\n","# train validation split\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size = 0.8)\n","print('Train/Test size: ', [data.shape for data in [X_train, X_val, y_train, y_val]])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Test size:  [(135, 4), (15, 4), (135, 3), (15, 3)]\n","Train/Test size:  [(108, 4), (27, 4), (108, 3), (27, 3)]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Model Prep"],"metadata":{}},{"cell_type":"code","execution_count":1089,"source":["# some variables\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = y_train.shape[1]\n","\n","print('input nodes: ', n_input)\n","print('sample size: ', n_samples)\n","print('prediction classes: ', n_classes)"],"outputs":[{"output_type":"stream","name":"stdout","text":["input nodes:  4\n","sample size:  108\n","prediction classes:  3\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1090,"source":["# train mini batch iterator\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","\n","print('minibatch prepared!')"],"outputs":[{"output_type":"stream","name":"stdout","text":["minibatch prepared!\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1091,"source":["# Hyperparameter settings\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","n_hidden1 = 50\n","n_hidden2 = 100\n","\n","print('HYPER PARAMS')\n","print('alpha learning rate: ', learning_rate)\n","print('batch size: ', batch_size)\n","print('epoch num', num_epochs)\n","print('n_hidden1: ', n_hidden1)\n","print('n_hidden2: ', n_hidden2)"],"outputs":[{"output_type":"stream","name":"stdout","text":["HYPER PARAMS\n","alpha learning rate:  0.001\n","batch size:  10\n","epoch num 100\n","n_hidden1:  50\n","n_hidden2:  100\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1092,"source":["#Determine the shape of the argument to be passed to the calculation graph\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","print('X holder: ',X)\n","print('Y holder: ', Y)"],"outputs":[{"output_type":"stream","name":"stdout","text":["X holder:  Tensor(\"Placeholder_276:0\", shape=(None, 4), dtype=float32)\n","Y holder:  Tensor(\"Placeholder_277:0\", shape=(None, 3), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# Model"],"metadata":{}},{"cell_type":"markdown","source":["### Test layer"],"metadata":{}},{"cell_type":"code","execution_count":1093,"source":["# weight\n","w1 = tf.Variable(tf.random_normal([n_input, n_classes]))\n","b1 = tf.Variable(tf.random_normal([n_classes]))\n","print('w:', w1)\n","print('b:', b1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["w: <tf.Variable 'Variable_425:0' shape=(4, 3) dtype=float32>\n","b: <tf.Variable 'Variable_426:0' shape=(3,) dtype=float32>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1094,"source":["# layer\n","print('X: ', X)\n","layer1 = tf.matmul(X, w1)\n","print('after matrix multiplication:', layer1)\n","layer1 = tf.add(layer1, b1)\n","print('after adding bias:', layer1)\n","layer1 = tf.nn.relu(layer1)\n","print('after activation: ', layer1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["X:  Tensor(\"Placeholder_276:0\", shape=(None, 4), dtype=float32)\n","after matrix multiplication: Tensor(\"MatMul_217:0\", shape=(None, 3), dtype=float32)\n","after adding bias: Tensor(\"Add_306:0\", shape=(None, 3), dtype=float32)\n","after activation:  Tensor(\"Relu_122:0\", shape=(None, 3), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1095,"source":["def do(stuff, session, input):\n","  return session.run(stuff, feed_dict = input)\n","def prep_session():\n","  session = tf.Session()\n","  session.run(tf.global_variables_initializer())\n","  return session\n","\n","\n","with prep_session() as s:\n","  forward = do(layer1, s, {X: X_train, Y: y_train})\n","  print('forward shape: ', forward.shape)\n","  print('forward: ', forward[0])\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["forward shape:  (108, 3)\n","forward:  [0.5547868 0.        0.       ]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Objective Function"],"metadata":{}},{"cell_type":"code","execution_count":1096,"source":["logits = layer1 # assuming 1 layer model\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n","print('loss:', loss_op)\n","\n","\n","# the equivalent or the tf.softmax's implementation!\n","softmax = tf.nn.softmax(logits)\n","loss_op2 = -tf.reduce_sum(Y * tf.log(softmax), 1)\n","loss_op2 = tf.reduce_mean(loss_op2)"],"outputs":[{"output_type":"stream","name":"stdout","text":["loss: Tensor(\"Mean_169:0\", shape=(), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1097,"source":["# test loss op\n","tf.set_random_seed(0)\n","with prep_session() as s:\n","  x_in, y_in = X_train[:5], y_train[:5]\n","  run = lambda x: do(x, s, {X: x_in, Y: y_in})\n","  print('y_true: ', y_in)\n","\n","  loss = run(loss_op)\n","  print('loss: ', loss)\n","  loss2 = run(loss_op2)\n","  print('loss2: ', loss2.mean())"],"outputs":[{"output_type":"stream","name":"stdout","text":["y_true:  [[0. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n","loss:  1.2593744\n","loss2:  1.2593744\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Prediction"],"metadata":{}},{"cell_type":"code","execution_count":1099,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Optimizer"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Training"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}}],"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.6.8 64-bit ('3.6.8': pyenv)"},"interpreter":{"hash":"26e78bd553b95be0b58407d4e230199be8d6925e41b449e18e4d85b81bf49eae"}},"nbformat":4,"nbformat_minor":2}