{"cells":[{"cell_type":"markdown","source":["# Intro to tensorflow"],"metadata":{}},{"cell_type":"code","execution_count":1379,"source":["import tensorflow.compat.v1 as tf\n","tf.disable_eager_execution()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1380,"source":["a = tf.constant(5)\n","b = tf.constant(7)\n","add = tf.add(a, b)\n","sess = tf.Session()\n","output = sess.run(add)\n","print(output) # 12\n","sess.close()"],"outputs":[{"output_type":"stream","name":"stdout","text":["12\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1381,"source":["c = tf.placeholder(tf.int32)\n","d = tf.placeholder(tf.int32)\n","add = tf.add(c, d)\n","sess = tf.Session()\n","output = sess.run(add, feed_dict={c:5, d:7})\n","print(output) # 12\n","output = sess.run(add, feed_dict={c:20, d:32})\n","print(output) # 52\n","sess.close()\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["12\n","52\n"]}],"metadata":{}},{"cell_type":"markdown","source":["### NOTE:\n","Following tensorflow 2 from now on."],"metadata":{}},{"cell_type":"markdown","source":["# Logistic regress on Tensorflow"],"metadata":{}},{"cell_type":"code","execution_count":1382,"source":["import numpy as np\n","x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y_train = np.array([[0],[0],[0],[1]])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1383,"source":["x = tf.placeholder(tf.float32, [None, 2])\n","t = tf.placeholder(tf.float32, [None, 1])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1384,"source":["W = tf.Variable(tf.zeros([2,1]))\n","b = tf.Variable(tf.zeros([1]))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1385,"source":["y = tf.sigmoid(tf.matmul(x, W) + b)\n","cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1386,"source":["train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1387,"source":["correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1388,"source":["sess = tf.Session()\n","sess.run(tf.global_variables_initializer())"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1389,"source":["for epoch in range(1000):\n","    sess.run(train_step, feed_dict={\n","        x:x_train,\n","        t:y_train\n","    })\n","# Display the correct answer rate every 100 times\n","    if epoch % 100 == 0:\n","        acc_val = sess.run(\n","            accuracy, feed_dict={\n","                x:x_train,\n","                t:y_train})\n","        print ('epoch: %d, Accuracy: %f'\n","               %(epoch, acc_val))"],"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, Accuracy: 0.750000\n","epoch: 100, Accuracy: 1.000000\n","epoch: 200, Accuracy: 1.000000\n","epoch: 300, Accuracy: 1.000000\n","epoch: 400, Accuracy: 1.000000\n","epoch: 500, Accuracy: 1.000000\n","epoch: 600, Accuracy: 1.000000\n","epoch: 700, Accuracy: 1.000000\n","epoch: 800, Accuracy: 1.000000\n","epoch: 900, Accuracy: 1.000000\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1390,"source":["#Check if the learning result is correct\n","classified = sess.run(correct_prediction, feed_dict={\n","    x:x_train,\n","    t:y_train\n","})\n","#Check output y\n","prob = sess.run(y, feed_dict={\n","    x:x_train,\n","    t:y_train\n","})\n","print(classified)\n","# [[ True]\n","# [ True]\n","# [ True]\n","# [ True]]\n","\n","print(prob)\n","# [[  1.96514215e-04]\n","# [  4.90498319e-02]\n","# [  4.90498319e-02]\n","# [  9.31203783e-01]]"],"outputs":[{"output_type":"stream","name":"stdout","text":["[[ True]\n"," [ True]\n"," [ True]\n"," [ True]]\n","[[1.9654632e-04]\n"," [4.9049824e-02]\n"," [4.9049824e-02]\n"," [9.3120384e-01]]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1391,"source":["print('W:', sess.run(W))\n","print('b:', sess.run(b))\n","# W: [[ 5.5699544]\n","# [ 5.5699544]]\n","# b: [-8.53457928]"],"outputs":[{"output_type":"stream","name":"stdout","text":["W: [[5.5699544]\n"," [5.5699544]]\n","b: [-8.534579]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1392,"source":["sess.close()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Tensorflow (v1)"],"metadata":{}},{"cell_type":"markdown","source":["# Problem 1\n","Looking back on the scratch"],"metadata":{}},{"cell_type":"markdown","source":["### Steps to build a ML model\n","1. Load\n","2. Define model (class) along with needed stuffs (params, input shapes,..., procedure, batch ...)\n","3. Init stuff and use the model"],"metadata":{}},{"cell_type":"markdown","source":["# Problem 2\n","Correspond between scratch and tensorflow"],"metadata":{}},{"cell_type":"code","execution_count":1393,"source":["\"\"\"\n","Binary classification of Iris dataset using neural network implemented in TensorFlow\n","\"\"\"\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","#Load dataset\n","df = pd.read_csv(\"Iris.csv\")\n","\n","#Condition extraction from data frame\n","df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","# NumPy 配列に変換\n","X = np.array(X)\n","y = np.array(y)\n","# Convert label to number\n","y[y == \"Iris-versicolor\"] = 0\n","y[y == \"Iris-virginica\"] = 1\n","y = y.astype(np.int64)[:, np.newaxis]\n","#Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","class GetMiniBatch:\n","    \"\"\"\n","    Iterator to get a mini-batch\n","    Parameters\n","    ----------\n","    X : The following forms of ndarray, shape (n_samples, n_features)\n","      Training data\n","    y : The following form of ndarray, shape (n_samples, 1)\n","      Correct answer value\n","    batch_size : int\n","      Batch size\n","    seed : int\n","      NumPy random number seed\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# Hyperparameter settings\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 10\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","#Determine the shape of the argument to be passed to the calculation graph\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# train mini batch iterator\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    Simple 3-layer neural network\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # Declaration of weight and bias\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n","    return layer_output\n","#Read network structure                              \n","logits = example_net(X)\n","# Objective function\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# Optimization method\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# Estimated result\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","#Indicator value calculation\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","#Initialization of variable\n","init = tf.global_variables_initializer()\n","\n","#Run calculation graph\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        #Loop for each epoch\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # Loop for each mini-batch\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})            \n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, loss : 8.7645, val_loss : 94.4552, acc : 0.375\n","Epoch 1, loss : 6.1416, val_loss : 62.6028, acc : 0.375\n","Epoch 2, loss : 3.5133, val_loss : 30.6852, acc : 0.375\n","Epoch 3, loss : 1.1685, val_loss : 2.5341, acc : 0.625\n","Epoch 4, loss : 0.9549, val_loss : 3.6983, acc : 0.688\n","Epoch 5, loss : 0.3575, val_loss : 5.3783, acc : 0.375\n","Epoch 6, loss : 0.4797, val_loss : 4.2005, acc : 0.375\n","Epoch 7, loss : 0.3030, val_loss : 0.5185, acc : 0.812\n","Epoch 8, loss : 0.2119, val_loss : 0.7033, acc : 0.812\n","Epoch 9, loss : 0.1795, val_loss : 1.6276, acc : 0.625\n","test_acc : 0.750\n"]}],"metadata":{}},{"cell_type":"markdown","source":["### Steps to build in tensorflow\n","1. Load dataset\n","2. Define model and it's flow\n","   1. Define parameters (variable) and inputs (placeholders)\n","   2. Define model's prediction (or forward flow)\n","   3. Define models' loss function\n","   4. Get an optimizer\n","   5. Define the traing flow\n","      - Optimize\n","      - Training scores\n","      - Loss\n","      - Log,...\n","3. Prepare epoch and batch\n","4. Initialize paremeters\n","5.  Start a session\n","  - Train in batches\n","  - Use the model and stuss defined in tf syntax\n","6.  Close session"],"metadata":{}},{"cell_type":"markdown","source":["**Tensorflow 1.0 has a lazy execution approach to it.**"],"metadata":{}},{"cell_type":"markdown","source":["# Problem 3\n","Create a model of Iris using all three types of objective variables"],"metadata":{}},{"cell_type":"markdown","source":["## NOTE\n","**Let's go through the process one by one and test them all!**"],"metadata":{}},{"cell_type":"markdown","source":["## Data Prep"],"metadata":{}},{"cell_type":"code","execution_count":1394,"source":["#load dataset\n","df = pd.read_csv('Iris.csv')\n","\n","print('shape: ', df.shape)\n","print(df.head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["shape:  (150, 6)\n","   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n","0   1            5.1           3.5            1.4           0.2  Iris-setosa\n","1   2            4.9           3.0            1.4           0.2  Iris-setosa\n","2   3            4.7           3.2            1.3           0.2  Iris-setosa\n","3   4            4.6           3.1            1.5           0.2  Iris-setosa\n","4   5            5.0           3.6            1.4           0.2  Iris-setosa\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1395,"source":["# predictor\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","print('X shape:', X.shape)\n","print(X.head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["X shape: (150, 4)\n","   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n","0            5.1           3.5            1.4           0.2\n","1            4.9           3.0            1.4           0.2\n","2            4.7           3.2            1.3           0.2\n","3            4.6           3.1            1.5           0.2\n","4            5.0           3.6            1.4           0.2\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1396,"source":["# label\n","y = df['Species']\n","print('y shape: ', y.shape)\n","print(y.head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["y shape:  (150,)\n","0    Iris-setosa\n","1    Iris-setosa\n","2    Iris-setosa\n","3    Iris-setosa\n","4    Iris-setosa\n","Name: Species, dtype: object\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1397,"source":["# Convert to numpy\n","X = np.array(X)\n","y = np.array(y)\n","print(X.shape, y.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4) (150,)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1398,"source":["# Encode y from text to number label\n","print('init shape y: ', y.shape)\n","#label encode\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","y = LabelEncoder().fit_transform(y)\n","#reshape for onehot use\n","y = y.reshape(-1,1)\n","enc = OneHotEncoder()\n","y = enc.fit_transform(y).toarray()\n","print('shape: ', y.shape)\n","print(y[:5])"],"outputs":[{"output_type":"stream","name":"stdout","text":["init shape y:  (150,)\n","shape:  (150, 3)\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1399,"source":["from sklearn.model_selection import train_test_split\n","# train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.9)\n","print('Train/Test size: ', [data.shape for data in [X_train, X_test, y_train, y_test]])\n","# train validation split\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size = 0.8)\n","print('Train/Test size: ', [data.shape for data in [X_train, X_val, y_train, y_val]])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Test size:  [(135, 4), (15, 4), (135, 3), (15, 3)]\n","Train/Test size:  [(108, 4), (27, 4), (108, 3), (27, 3)]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Model Prep"],"metadata":{}},{"cell_type":"code","execution_count":1400,"source":["# some variables\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = y_train.shape[1]\n","\n","print('input nodes: ', n_input)\n","print('sample size: ', n_samples)\n","print('prediction classes: ', n_classes)"],"outputs":[{"output_type":"stream","name":"stdout","text":["input nodes:  4\n","sample size:  108\n","prediction classes:  3\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1401,"source":["# train mini batch iterator\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","\n","print('minibatch prepared!')"],"outputs":[{"output_type":"stream","name":"stdout","text":["minibatch prepared!\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1402,"source":["# Hyperparameter settings\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","n_hidden1 = 50\n","n_hidden2 = 100\n","\n","print('HYPER PARAMS')\n","print('alpha learning rate: ', learning_rate)\n","print('batch size: ', batch_size)\n","print('epoch num', num_epochs)\n","print('n_hidden1: ', n_hidden1)\n","print('n_hidden2: ', n_hidden2)"],"outputs":[{"output_type":"stream","name":"stdout","text":["HYPER PARAMS\n","alpha learning rate:  0.001\n","batch size:  10\n","epoch num 100\n","n_hidden1:  50\n","n_hidden2:  100\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1403,"source":["#Determine the shape of the argument to be passed to the calculation graph\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","print('X holder: ',X)\n","print('Y holder: ', Y)"],"outputs":[{"output_type":"stream","name":"stdout","text":["X holder:  Tensor(\"Placeholder_330:0\", shape=(None, 4), dtype=float32)\n","Y holder:  Tensor(\"Placeholder_331:0\", shape=(None, 3), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# Model"],"metadata":{}},{"cell_type":"markdown","source":["### Test layer"],"metadata":{}},{"cell_type":"code","execution_count":1404,"source":["# weight\n","w1 = tf.Variable(tf.random_normal([n_input, n_classes]))\n","b1 = tf.Variable(tf.random_normal([n_classes]))\n","print('w:', w1)\n","print('b:', b1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["w: <tf.Variable 'Variable_501:0' shape=(4, 3) dtype=float32>\n","b: <tf.Variable 'Variable_502:0' shape=(3,) dtype=float32>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1405,"source":["# layer\n","print('X: ', X)\n","layer1 = tf.matmul(X, w1)\n","print('after matrix multiplication:', layer1)\n","layer1 = tf.add(layer1, b1)\n","print('after adding bias:', layer1)\n","layer1 = tf.nn.relu(layer1)\n","print('after activation: ', layer1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["X:  Tensor(\"Placeholder_330:0\", shape=(None, 4), dtype=float32)\n","after matrix multiplication: Tensor(\"MatMul_255:0\", shape=(None, 3), dtype=float32)\n","after adding bias: Tensor(\"Add_357:0\", shape=(None, 3), dtype=float32)\n","after activation:  Tensor(\"Relu_142:0\", shape=(None, 3), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1406,"source":["session = tf.Session()\n","def do(stuff, input):\n","  return session.run(stuff, feed_dict = input)\n","def prep_session():\n","  session.run(tf.global_variables_initializer())\n","\n","prep_session()\n","forward = do(layer1, {X: X_train, Y: y_train})\n","print('forward shape: ', forward.shape)\n","print('forward: ', forward[0])\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["forward shape:  (108, 3)\n","forward:  [ 0.        13.25295    1.8706096]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Objective Function"],"metadata":{}},{"cell_type":"code","execution_count":1407,"source":["logits = layer1 # assuming 1 layer model\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n","print('loss:', loss_op)\n","\n","\n","# the equivalent or the tf.softmax's implementation!\n","softmax = tf.nn.softmax(logits)\n","loss_op2 = -tf.reduce_sum(Y * tf.log(softmax), 1)\n","loss_op2 = tf.reduce_mean(loss_op2)"],"outputs":[{"output_type":"stream","name":"stdout","text":["loss: Tensor(\"Mean_205:0\", shape=(), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1408,"source":["# test loss op\n","tf.set_random_seed(0)\n","\n","x_in, y_in = X_train[:5], y_train[:5]\n","run = lambda x: do(x, {X: x_in, Y: y_in})\n","print('y_true: ', y_in)\n","\n","loss = run(loss_op)\n","print('loss: ', loss)\n","loss2 = run(loss_op2)\n","print('loss2: ', loss2.mean())"],"outputs":[{"output_type":"stream","name":"stdout","text":["y_true:  [[0. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n","loss:  4.027909\n","loss2:  4.027909\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Prediction"],"metadata":{}},{"cell_type":"code","execution_count":1409,"source":["def prediction_function(logits):\n","  pred = np.argmax(logits, axis = 1).reshape(-1,1)\n","  return enc.transform(pred).toarray()\n","logits_ = run(logits)\n","print('logits: ', logits_)\n","print('pred: ', prediction_function(logits_))\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["logits:  [[ 0.        13.25295    1.8706096]\n"," [ 0.        13.412225   1.9192337]\n"," [ 0.        12.519266   1.6441416]\n"," [ 0.        13.583306   1.5877434]\n"," [ 0.         8.7562685  1.5236439]]\n","pred:  [[0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Optimizer"],"metadata":{}},{"cell_type":"code","execution_count":1410,"source":["optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","print(optimizer)"],"outputs":[{"output_type":"stream","name":"stdout","text":["<tensorflow.python.training.adam.AdamOptimizer object at 0x188bc2588>\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Some Metrics"],"metadata":{}},{"cell_type":"code","execution_count":1411,"source":["# accuracy\n","def accuracy_function(pred, real):\n","  correct_pred = np.argmax(pred,axis = 1) == np.argmax(real,axis = 1)\n","  return correct_pred.mean()\n","pred = prediction(logits_)\n","real = y_in\n","print('accuracy_function: ', accuracy_function(pred,real))"],"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_function:  0.6\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Training"],"metadata":{}},{"cell_type":"code","execution_count":1412,"source":["train_optimizer = optimizer.minimize(loss_op)\n","\n","def accuracy(logits, y):\n","  prediction = prediction_function(logits)\n","  acc = accuracy_function(prediction, y)\n","  return acc\n","\n","def train_batch(mini_batch_x, mini_batch_y):\n","  input_xy = {X: mini_batch_x, Y: mini_batch_y}\n","  run = lambda x: do(x, input_xy)\n","  # train\n","  run(train_optimizer)\n","  # loss\n","  batch_loss= run(loss_op)\n","  # accuracy\n","  batch_acc = accuracy(run(logits), mini_batch_y)\n","  return batch_loss, batch_acc\n","\n","def train_epoch(X_val, y_val, iterator, verbose= False):\n","  #Loop for each epoch\n","  total_loss = 0\n","  total_acc = 0\n","  # train\n","  for i, (mini_batch_x, mini_batch_y) in enumerate(iterator):\n","      batch_loss, batch_acc = train_batch(mini_batch_x, mini_batch_y)\n","      total_loss += batch_loss\n","      total_acc += batch_acc * batch_size\n","      if verbose:\n","        print(\"Batch: {}, b_loss: {:.3f}, b_acc: {:.3f}\".format(i, batch_loss, batch_acc))\n","  total_loss /= n_samples\n","  total_acc /= n_samples\n","\n","  input_xy = {X: X_val, Y: y_val}\n","  run = lambda x: do(x, input_xy)\n","  val_loss = run(loss_op)\n","  val_acc = accuracy(run(logits), y_val)\n","  return total_loss, total_acc, val_loss, val_acc\n","\n","def train(X_train, y_train, X_val, y_val, epoch = 10, verbose_interval = 10):\n","  iterator = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","  for i in range(epoch):\n","    # print('Starting epoch: ', i)\n","    total_loss, total_acc, val_loss, val_acc = train_epoch(X_val, y_val, iterator)\n","    if (i+1)%verbose_interval == 0 or i == 0:\n","      print(\"Epoch {} Result -- tot_loss : {:.2f}, tot_acc : {:.2f}, val_loss : {:.2f}, val_acc : {:.2f}\".format(i, total_loss, total_acc, val_loss, val_acc))\n","\n","    # print('______')\n","\n","session.close()\n","session = tf.Session()\n","prep_session()\n","train(X_train, y_train, X_val, y_val, epoch = 100, verbose_interval = 10)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 Result -- tot_loss : 0.74, tot_acc : 0.34, val_loss : 7.05, val_acc : 0.33\n","Epoch 9 Result -- tot_loss : 0.58, tot_acc : 0.34, val_loss : 5.58, val_acc : 0.33\n","Epoch 19 Result -- tot_loss : 0.40, tot_acc : 0.34, val_loss : 3.99, val_acc : 0.33\n","Epoch 29 Result -- tot_loss : 0.26, tot_acc : 0.34, val_loss : 2.80, val_acc : 0.33\n","Epoch 39 Result -- tot_loss : 0.22, tot_acc : 0.38, val_loss : 2.45, val_acc : 0.48\n","Epoch 49 Result -- tot_loss : 0.21, tot_acc : 0.47, val_loss : 2.27, val_acc : 0.44\n","Epoch 59 Result -- tot_loss : 0.19, tot_acc : 0.46, val_loss : 2.08, val_acc : 0.44\n","Epoch 69 Result -- tot_loss : 0.17, tot_acc : 0.47, val_loss : 1.88, val_acc : 0.44\n","Epoch 79 Result -- tot_loss : 0.16, tot_acc : 0.47, val_loss : 1.68, val_acc : 0.37\n","Epoch 89 Result -- tot_loss : 0.14, tot_acc : 0.48, val_loss : 1.48, val_acc : 0.37\n","Epoch 99 Result -- tot_loss : 0.12, tot_acc : 0.49, val_loss : 1.28, val_acc : 0.37\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# Problem 4\n","Creating a model of house prices"],"metadata":{}},{"cell_type":"code","execution_count":1413,"source":["# data set\n","house_price_pd = pd.read_csv('../Data/Normal/houseprice_train.csv')\n","house_price_pd.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>Street</th>\n","      <th>Alley</th>\n","      <th>LotShape</th>\n","      <th>LandContour</th>\n","      <th>Utilities</th>\n","      <th>...</th>\n","      <th>PoolArea</th>\n","      <th>PoolQC</th>\n","      <th>Fence</th>\n","      <th>MiscFeature</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","      <th>SalePrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>65.0</td>\n","      <td>8450</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>208500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>80.0</td>\n","      <td>9600</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2007</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>181500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>68.0</td>\n","      <td>11250</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>223500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>60.0</td>\n","      <td>9550</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>140000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>84.0</td>\n","      <td>14260</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>250000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 81 columns</p>\n","</div>"],"text/plain":["   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n","0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n","1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n","2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n","3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n","4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n","\n","  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n","0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n","1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n","2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n","3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n","4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n","\n","  YrSold  SaleType  SaleCondition  SalePrice  \n","0   2008        WD         Normal     208500  \n","1   2007        WD         Normal     181500  \n","2   2008        WD         Normal     223500  \n","3   2006        WD        Abnorml     140000  \n","4   2008        WD         Normal     250000  \n","\n","[5 rows x 81 columns]"]},"metadata":{},"execution_count":1413}],"metadata":{}},{"cell_type":"code","execution_count":1414,"source":["# selection\n","selected = house_price_pd[['GrLivArea', 'YearBuilt','SalePrice']]\n","print('selected: ', selected.shape, selected.columns)\n","print(selected.head())"],"outputs":[{"output_type":"stream","name":"stdout","text":["selected:  (1460, 3) Index(['GrLivArea', 'YearBuilt', 'SalePrice'], dtype='object')\n","   GrLivArea  YearBuilt  SalePrice\n","0       1710       2003     208500\n","1       1262       1976     181500\n","2       1786       2001     223500\n","3       1717       1915     140000\n","4       2198       2000     250000\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1415,"source":["# check nan\n","print(selected.isna().sum(axis = 0))"],"outputs":[{"output_type":"stream","name":"stdout","text":["GrLivArea    0\n","YearBuilt    0\n","SalePrice    0\n","dtype: int64\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1416,"source":["# normalize\n","print(selected.describe().transpose()[['mean', 'std']])\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","scaled = scaler.fit_transform(selected)\n","print(pd.DataFrame(scaled).describe().transpose()[['mean', 'std']])\n","\n","explainatory = scaled[:,:-1]\n","objective = scaled[:,-1].reshape(-1,1)\n","print(explainatory.shape, explainatory[:3])\n","print(objective.shape, objective[:3])"],"outputs":[{"output_type":"stream","name":"stdout","text":["                    mean           std\n","GrLivArea    1515.463699    525.480383\n","YearBuilt    1971.267808     30.202904\n","SalePrice  180921.195890  79442.502883\n","           mean       std\n","0 -1.277517e-16  1.000343\n","1  1.046347e-15  1.000343\n","2  1.362685e-16  1.000343\n","(1460, 2) [[ 0.37033344  1.05099379]\n"," [-0.48251191  0.15673371]\n"," [ 0.51501256  0.9847523 ]]\n","(1460, 1) [[0.34727322]\n"," [0.00728832]\n"," [0.53615372]]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1417,"source":["# train test split\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val = train_test_split(explainatory, objective, train_size = 0.8)\n","print('train shape: ', [i.shape for i in [X_train, X_val, y_train, y_val]])"],"outputs":[{"output_type":"stream","name":"stdout","text":["train shape:  [(1168, 2), (292, 2), (1168, 1), (292, 1)]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1418,"source":["# prep vars\n","input_dimention = X_train.shape[-1]\n","batch_size = 20\n","lr = 0.1\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1419,"source":["# placeholder\n","X = tf.placeholder(\"float\", [None, input_dimention])\n","Y = tf.placeholder(\"float\", [None, 1])\n","print(X,Y)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"Placeholder_332:0\", shape=(None, 2), dtype=float32) Tensor(\"Placeholder_333:0\", shape=(None, 1), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1427,"source":["# model\n","w = tf.Variable(tf.random_normal([input_dimention, 1]))\n","b = tf.Variable(tf.random_normal([1]))\n","linear = tf.matmul(X, w)\n","linear = tf.add(linear, b)\n","\n","model = linear"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1428,"source":["# loss\n","loss = Y - model\n","loss = tf.math.square(loss)\n","loss = tf.math.reduce_mean(loss) / 2\n","\n","# optimizer\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n","optimize_operation = optimizer.minimize(loss)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1434,"source":["# train\n","def _train(X_train, y_train, X_val, y_val):\n","  input_xy = {X: X_train, Y: y_train}\n","  run = lambda x: do(x, input_xy)\n","  # train\n","  run(optimize_operation)\n","  # predict\n","  train_loss =  run(loss)\n","  input_xy = {X: X_val, Y: y_val}\n","  val_loss = run(loss)\n","  return train_loss, val_loss\n","\n","def train(X_train, y_train, X_val, y_val, epoch = 100, print_interval = 10):\n","  print(X_train.shape)\n","  for i in range(epoch): \n","    train_loss, val_loss = _train(X_train, y_train, X_val, y_val)\n","    if (i + 1) % print_interval == 0 or i == 0:\n","      print(f'Ilteration: {i}, SSE_train: {train_loss}, SSE_val: {val_loss}')\n","\n","session.close()\n","session = tf.Session()\n","prep_session()\n","train(X_train, y_train, X_val, y_val)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(1168, 2)\n","Ilteration: 0, SSE_train: 0.5249675512313843, SSE_val: 0.5466243028640747\n","Ilteration: 9, SSE_train: 0.2246502935886383, SSE_val: 0.24539430439472198\n","Ilteration: 19, SSE_train: 0.1761990338563919, SSE_val: 0.19716167449951172\n","Ilteration: 29, SSE_train: 0.16992352902889252, SSE_val: 0.19106000661849976\n","Ilteration: 39, SSE_train: 0.16908639669418335, SSE_val: 0.1902848184108734\n","Ilteration: 49, SSE_train: 0.16897033154964447, SSE_val: 0.1901824176311493\n","Ilteration: 59, SSE_train: 0.16895349323749542, SSE_val: 0.19016504287719727\n","Ilteration: 69, SSE_train: 0.1689509004354477, SSE_val: 0.19015952944755554\n","Ilteration: 79, SSE_train: 0.16895049810409546, SSE_val: 0.19015681743621826\n","Ilteration: 89, SSE_train: 0.16895043849945068, SSE_val: 0.19015537202358246\n","Ilteration: 99, SSE_train: 0.1689504086971283, SSE_val: 0.19015459716320038\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# Problem 5\n","Creating a MNIST model"],"metadata":{}},{"cell_type":"markdown","source":["# TODO!"],"metadata":{}}],"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.6.8 64-bit ('3.6.8': pyenv)"},"interpreter":{"hash":"26e78bd553b95be0b58407d4e230199be8d6925e41b449e18e4d85b81bf49eae"}},"nbformat":4,"nbformat_minor":2}